Article
Discovering faster matrix multiplication
algorithms with reinforcement learning
https://doi.org/10.1038/s41586-022-05172-4
Received: 2 October 2021
Accepted: 2 August 2022
Alhussein Fawzi1,2 ✉, Matej Balog1,2, Aja Huang1,2, Thomas Hubert1,2,
Bernardino Romera-Paredes1,2, Mohammadamin Barekatain1, Alexander Novikov1,
Francisco J. R. Ruiz1, Julian Schrittwieser1, Grzegorz Swirszcz1, David Silver1, Demis Hassabis1
& Pushmeet Kohli1
Published online: 5 October 2022
Open access
Check for updates
Improving the efficiency of algorithms for fundamental computations can have a
widespread impact, as it can affect the overall speed of a large amount of computations.
Matrix multiplication is one such primitive task, occurring in many systems—from
neural networks to scientific computing routines. The automatic discovery of
algorithms using machine learning offers the prospect of reaching beyond human
intuition and outperforming the current best human-designed algorithms. However,
automating the algorithm discovery procedure is intricate, as the space of possible
algorithms is enormous. Here we report a deep reinforcement learning approach
based on AlphaZero1 for discovering efficient and provably correct algorithms for the
multiplication of arbitrary matrices. Our agent, AlphaTensor, is trained to play a
single-player game where the objective is finding tensor decompositions within a
finite factor space. AlphaTensor discovered algorithms that outperform the state-
of-the-art complexity for many matrix sizes. Particularly relevant is the case of 4 × 4
matrices in a finite field, where AlphaTensor’s algorithm improves on Strassen’s two-
level algorithm for the first time, to our knowledge, since its discovery 50 years ago2.
We further showcase the flexibility of AlphaTensor through different use-cases:
algorithms with state-of-the-art complexity for structured matrix multiplication and
improved practical efficiency by optimizing matrix multiplication for runtime on
specific hardware. Our results highlight AlphaTensor’s ability to accelerate the
process of algorithmic discovery on a range of problems, and to optimize for different
criteria.
We focus on the fundamental task of matrix multiplication, and use
deep reinforcement learning (DRL) to search for provably correct and
efficient matrix multiplication algorithms. This algorithm discovery
process is particularly amenable to automation because a rich space of
matrix multiplication algorithms can be formalized as low-rank decom-
positions of a specific three-dimensional (3D) tensor2, called the matrix
multiplication tensor3–7. This space of algorithms contains the stand-
ard matrix multiplication algorithm and recursive algorithms such as
Strassen’s2, as well as the (unknown) asymptotically optimal algorithm.
Although an important body of work aims at characterizing the com-
plexity of the asymptotically optimal algorithm8–12, this does not yield
practical algorithms5. We focus here on practical matrix multiplication
algorithms, which correspond to explicit low-rank decompositions of
the matrix multiplication tensor. In contrast to two-dimensional matri-
ces, for which efficient polynomial-time algorithms computing the rank
have existed for over two centuries13, finding low-rank decompositions
of 3D tensors (and beyond) is NP-hard14 and is also hard in practice.
In fact, the search space is so large that even the optimal algorithm
for multiplying two 3 × 3 matrices is still unknown. Nevertheless, in a
longstanding research effort, matrix multiplication algorithms have
been discovered by attacking this tensor decomposition problem using
human search2,15,16, continuous optimization17–19 and combinatorial
search20. These approaches often rely on human-designed heuristics,
which are probably suboptimal. We instead use DRL to learn to recog-
nize and generalize over patterns in tensors, and use the learned agent
to predict efficient decompositions.
We formulate the matrix multiplication algorithm discovery pro-
cedure (that is, the tensor decomposition problem) as a single-player
game, called TensorGame. At each step of TensorGame, the player
selects how to combine different entries of the matrices to multiply.
A score is assigned based on the number of selected operations required
to reach the correct multiplication result. This is a challenging game
with an enormous action space (more than 1012 actions for most inter-
esting cases) that is much larger than that of traditional board games
such as chess and Go (hundreds of actions). To solve TensorGame and
find efficient matrix multiplication algorithms, we develop a DRL agent,
AlphaTensor. AlphaTensor is built on AlphaZero1,21, where a neural net-
work is trained to guide a planning procedure searching for efficient
matrix multiplication algorithms. Our framework uses a single agent
to decompose matrix multiplication tensors of various sizes, yielding
DeepMind, London, UK. 2These authors contributed equally: Alhussein Fawzi, Matej Balog, Aja Huang, Thomas Hubert and Bernardino Romera-Paredes. ✉e-mail: afawzi@deepmind.com
1
Nature | Vol 610 | 6 October 2022 | 47Article
a
b
c
m1 = ( a1 + a4 )( b1 + b4 )
c1c2
c3c4
=
a1a2
a3a4
·
b1b2m2 = ( a3 + a4 ) b1
b3b4m3 = a1 ( b2 – b4 )
c1
b1
b2
c2
c3
b3
c4
b4
a2
a3
a4
10101–1000010
0
1
0100010
m4 = a4 ( b3 – b1 )110100–1
m5 = ( a1 + a2 ) b4110010
001–1
0010
0001001
m7 = ( a2 – a4 )( b3 + b4 )10–10101
c1 = m1 + m4 – m5 + m7100101
0010–1
100
01
–10
11
0000
010
m6 = ( a3 – a1 )( b1 + b2 )
c2 = m3 + m5
a1
U =
c3 = m2 + m4
V =
W =
1
c4 = m1 – m2 + m3 + m6
Fig. 1 | Matrix multiplication tensor and algorithms. a, Tensor T 2 representing
the multiplication of two 2 × 2 matrices. Tensor entries equal to 1 are depicted
in purple, and 0 entries are semi-transparent. The tensor specifies which entries
from the input matrices to read, and where to write the result. For example,
as c1 = a1 b1 + a2 b3, tensor entries located at (a1, b1, c1) and (a2, b3, c1) are set to 1.b, Strassen's algorithm2 for multiplying 2 × 2 matrices using 7 multiplications.
c, Strassen's algorithm in tensor factor representation. The stacked factors
U, V and W (green, purple and yellow, respectively) provide a rank-7
decomposition of T 2 (equation (1)). The correspondence between arithmetic
operations (b) and factors (c) is shown by using the aforementioned colours.
transfer of learned decomposition techniques across various tensors.
To address the challenging nature of the game, AlphaTensor uses a
specialized neural network architecture, exploits symmetries of the
problem and makes use of synthetic training games.
AlphaTensor scales to a substantially larger algorithm space than
what is within reach for either human or combinatorial search. In fact,
AlphaTensor discovers from scratch many provably correct matrix
multiplication algorithms that improve over existing algorithms in
terms of number of scalar multiplications. We also adapt the algo-
rithm discovery procedure to finite fields, and improve over Strassen’s
two-level algorithm for multiplying 4 × 4 matrices for the first time, to
our knowledge, since its inception in 1969. AlphaTensor also discovers a
diverse set of algorithms—up to thousands for each size—showing that
the space of matrix multiplication algorithms is richer than previously
thought. We also exploit the diversity of discovered factorizations to
improve state-of-the-art results for large matrix multiplication sizes.
Through different use-cases, we highlight AlphaTensor’s flexibility
and wide applicability: AlphaTensor discovers efficient algorithms
for structured matrix multiplication improving over known results,
and finds efficient matrix multiplication algorithms tailored to spe-
cific hardware, by optimizing for actual runtime. These algorithms
multiply large matrices faster than human-designed algorithms on
the same hardware.A decomposition of T n into R rank-one terms provides an algorithm
for multiplying arbitrary n × n matrices using R scalar multiplications
(see Algorithm 1). We refer to Fig. 1b,c for an example algorithm mul-
tiplying 2 × 2 matrices with R = 7 (Strassen’s algorithm).
Crucially, Algorithm 1 can be used to multiply block matrices. By using
this algorithm recursively, one can multiply matrices of arbitrary size, with
the rank R controlling the asymptotic complexity of the algorithm. In par-
ticular, N × N matrices can be multiplied with asymptotic complexity
O(Nlogn (R)); see ref. 5 for more details.
Algorithms as tensor decomposition
As matrix multiplication (A, B) ↦ AB is bilinear (that is, linear in both
arguments), it can be fully represented by a 3D tensor: see Fig. 1a for
how to represent the 2 × 2 matrix multiplication operation as a 3D ten-
sor of size 4 × 4 × 4, and refs. 3,5,7 for more details. We write T n for the
tensor describing n × n matrix multiplication. The tensor T n is fixed
(that is, it is independent of the matrices to be multiplied), has entries
in {0, 1}, and is of size n2 × n2 × n2. More generally, we use T n, m, p to
describe the rectangular matrix multiplication operation of size n × m
with m × p (note that T n = T n, n, n ). By a decomposition of T n into R
rank-one terms, we mean
R
T n = ∑ u(r ) ⊗ v (r ) ⊗ w (r ),
(1)
r =1
where ⊗ denotes the outer (tensor) product, and u(r), v(r) and w(r) are all
vectors. If a tensor T can be decomposed into R rank-one terms, we say
the rank of T is at most R, or Rank (T ) ≤ R. This is a natural extension
R
from the matrix rank, where a matrix is decomposed into ∑r =1 u(r ) ⊗ v (r ).
48 | Nature | Vol 610 | 6 October 2022
DRL for algorithm discovery
We cast the problem of finding efficient matrix multiplication algo-
rithms as a reinforcement learning problem, modelling the environ-
ment as a single-player game, TensorGame. The game state after step
t is described by a tensor S t , which is initially set to the target tensor
we wish to decompose: S 0 = T n. In each step t of the game, the player
selects a triplet (u(t), v(t), w(t)), and the tensor S t is updated by subtract-
ing the resulting rank-one tensor: S t ← S t −1 − u(t ) ⊗ v (t ) ⊗ w (t ). The goal
of the player is to reach the zero tensor S t = 0 by applying the smallest
number of moves. When the player reaches the zero tensor, the
R
sequence of selected factors satisfies T n = ∑t =1 u(t ) ⊗ v (t ) ⊗ w (t ) (where
R denotes the number of moves), which guarantees the correctness of
the resulting matrix multiplication algorithm. To avoid playing unnec-
essarily long games, we limit the number of steps to a maximum value,
Rlimit.
For every step taken, we provide a reward of −1 to encourage finding
the shortest path to the zero tensor. If the game terminates with a
non-zero tensor (after Rlimit steps), the agent receives an additional
terminal reward equal to − γ(S R limit), where γ(S R limit) is an upper bound
on the rank of the terminal tensor. Although this reward optimizes for
rank (and hence for the complexity of the resulting algorithm), other
reward schemes can be used to optimize other properties, such as
practical runtime (see ‘Algorithm discovery results’). Besides, as our
aim is to find exact matrix multiplication algorithms, we constrain
{u(t), v(t), w(t)} to have entries in a user-specified discrete set of coeffi-
cients F (for example, F = {−2, −1, 0, 1, 2}). Such discretization is com-
mon practice to avoid issues with the finite precision of floating
points15,18,20.
To play TensorGame, we propose AlphaTensor (Fig. 2), an agent based
on AlphaZero1, which achieved tabula rasa superhuman performance
in the classical board games of Go, chess and shogi, and on its extension
to handle large action spaces Sampled AlphaZero21. Similarly to
AlphaZero, AlphaTensor uses a deep neural network to guide a Monteimprove the overall performance over a plain AlphaZero agent (see
Methods and Supplementary Information for details).
Algorithm 1
R
A meta-algorithm parameterized by {u(r ), v (r ), w (r )}r = 1 for computing
the matrix product C = AB. It is noted that R controls the number of
multiplications between input matrix entries.
R
Parameters: {u(r ), v (r ), w (r )}r = 1: length-n2 vectors such that
R
Tn = ∑r = 1 u(r ) ⊗ v (r ) ⊗ w (r )
Input: A, B: matrices of size n × n
Output: C = AB
(1) for r = 1, …, R do
(2) mr ← (u(1r )a1 +  + u(nr2)an 2) (v (1r )b1 +  + v (nr2)bn 2)
(3) for i = 1, …, n2 do
(R )
(4) ci ← w (1)
i m1 +  + w i mR
return C
Carlo tree search (MCTS) planning procedure. The network takes as
input a state (that is, a tensor S t to decompose), and outputs a policy
and a value. The policy provides a distribution over potential actions.
As the set of potential actions (u(t), v(t), w(t)) in each step is enormous,
we rely on sampling actions rather than enumerating them21,22. The
value provides an estimate of the distribution z of returns (cumulative
reward) starting from the current state S t . With the above reward
scheme, the distribution z models the agent’s belief about the rank of
the tensor S t. To play a game, AlphaTensor starts from the target tensor
(T n) and uses the MCTS planner at each step to choose the next action.
Finished games are used as feedback to the network to improve the
network parameters.
Overcoming the challenges posed by TensorGame—namely, an enor-
mous action space, and game states described by large 3D tensors
representing an abstract mathematical operation—requires multiple
advances. All these components, described briefly below, substantially
Neural network architecture
We propose a transformer-based23 architecture that incorporates
inductive biases for tensor inputs. We first project the S × S × S input
tensor into three S × S grids of feature vectors by using linear layers
applied to the three cyclic transpositions of the tensor. The main part of
the model comprises a sequence of attention operations, each applied
to a set of features belonging to a pair of grids (Extended Data Figs. 3
and 4). This generalizes axial attention24 to multiple grids, and is both
more efficient and yields better results than naive self-attention. The
proposed architecture, which disregards the order of rows and columns
in the grids, is inspired by the invariance of the tensor rank to slice
reordering. The final feature representation of the three matrices is
passed both to the policy head (an autoregressive model) and the value
head (a multilayer perceptron).
Synthetic demonstrations
Although tensor decomposition is NP-hard, the inverse task of con-
structing the tensor from its rank-one factors is elementary. Hence,
we generate a large dataset of tensor-factorization pairs (synthetic
R
demonstrations) by first sampling factors {(u(r ), v (r ), w (r ))}r =1 at random,
R
(r )
(r )
(r )
and then constructing the tensor D = ∑r =1 u ⊗ v ⊗ w . We train the
network on a mixture of supervised loss (that is, to imitate synthetic
demonstrations) and standard reinforcement learning loss (that is,
learning to decompose a target tensor T n) (Fig. 2). This mixed training
strategy—training on the target tensor and random tensors— substan-
tially outperforms each training strategy separately. This is despite
randomly generated tensors having different properties from the tar-
get tensors.
Change of basis
T n (Fig. 1a) is the tensor representing the matrix multiplication bilinear
operation in the canonical basis. The same bilinear operation can be
expressed in other bases, resulting in other tensors. These different
Acting
Change of basis
...
Played
game
(u(1), v(1), w(1))
Updated
model
(u(2), v(2), w(2))
(u(3), v(3), w(3))
Learning
Policy head
Played games
buffer
(u, v, w)
Sample
random state
Value head
Training labels
Neural network
Fig. 2 | Overview of AlphaTensor. The neural network (bottom box) takes
as input a tensor S t , and outputs samples (u, v, w) from a distribution
over potential next actions to play, and an estimate of the future returns
(for example, of −Rank (S t )). The network is trained on two data sources:
Network input
Pre-generated
synthetic
demonstrations
previously played games and synthetic demonstrations. The updated network
is sent to the actors (top box), where it is used by the MCTS planner to generate
new games.
Nature | Vol 610 | 6 October 2022 | 49Article
Best method
known
(2, 2, 2)
(3, 3, 3)(Strassen, 1969)2
(Laderman, 1976)15
(Strassen, 1969)2
(2, 2, 2) ^ (2, 2, 2)
(3, 5, 5) + (2, 5, 5)
7
237
237
23
494749
989698
(2, 2, 3)
11
(2, 2, 2) + (2, 2, 1)
(2, 2, 4)
14
(2, 2, 2) + (2, 2, 2)
(2, 2, 5)
18
(2, 2, 2) + (2, 2, 3)
(2, 3, 3) (Hopcroft and Kerr, 1971)16 15
(2, 3, 4) (Hopcroft and Kerr, 1971)16 20
(2, 3, 5) (Hopcroft and Kerr, 1971)16 25
(2, 4, 4) (Hopcroft and Kerr, 1971)16 26
(2, 4, 5) (Hopcroft and Kerr, 1971)16 33
(2, 5, 5) (Hopcroft and Kerr, 1971)16 40
(3, 3, 4)
(Smirnov, 2013)18
29
(3, 3, 5)
36
(Smirnov, 2013)18
(Smirnov, 2013)18
(3, 4, 4)
38
(Smirnov, 2013)18
(3, 4, 5)
48
(3, 5, 5) (Sedoglavic and Smirnov, 2021)19 58
(4, 4, 5)
64
(4, 4, 2) + (4, 4, 3)
(4, 5, 5)
80
(2, 5, 5) ^ (2, 1, 1)11
14
18
15
20
25
26
33
40
29
36
38
47
58
63
7611
14
18
15
20
25
26
33
40
29
36
38
47
58
63
76
(4, 4, 4)
(5, 5, 5)
(11, 12, 12)
Best rank AlphaTensor rank
known Modular Standard
(9, 11, 11)
30
Improvement in rank
Size
(n, m, p)25(9, 9, 11)
20(10, 11, 12)
(9, 10, 10)
15
(11, 11, 11)
(9, 9, 9)
10
(10, 12, 12)
(10, 10, 10)
5
0
200
400 600 800
Best rank known
1,000
Fig. 3 | Comparison between the complexity of previously known matrix
multiplication algorithms and the ones discovered by AlphaTensor. Left:
column (n, m, p) refers to the problem of multiplying n × m with m × p matrices.
The complexity is measured by the number of scalar multiplications (or
equivalently, the number of terms in the decomposition of the tensor). ‘Best
rank known’ refers to the best known upper bound on the tensor rank (before
this paper), whereas ‘AlphaTensor rank’ reports the rank upper bounds
obtained with our method, in modular arithmetic (Z 2) and standard arithmetic.In all cases, AlphaTensor discovers algorithms that match or improve over
known state of the art (improvements are shown in red). See Extended Data
Figs. 1 and 2 for examples of algorithms found with AlphaTensor. Right: results
(for arithmetic in R) of applying AlphaTensor-discovered algorithms on larger
tensors. Each red dot represents a tensor size, with a subset of them labelled.
See Extended Data Table 1 for the results in table form. State-of-the-art results
are obtained from the list in ref. 64.
tensors are equivalent: they have the same rank, and decompositions
obtained in a custom basis can be mapped to the canonical basis, hence
obtaining a practical algorithm of the form in Algorithm 1. We leverage
this observation by sampling a random change of basis at the beginning
of every game, applying it to T n, and letting AlphaTensor play the game
in that basis (Fig. 2). This crucial step injects diversity into the games
played by the agent.Strassen’s2 and Laderman’s15 algorithms). More importantly, AlphaT-
ensor improves over the best algorithms known for several matrix
sizes. In particular, AlphaTensor finds an algorithm for multiplying
4 × 4 matrices using 47 multiplications in Z2, thereby outperforming
Strassen’s two-level algorithm2, which involves 72 = 49 multiplica-
tions. By applying this algorithm recursively, one obtains a practical
matrix multiplication algorithm in Z2 with complexity O(N 2.778) .
Moreover, AlphaTensor discovers efficient algorithms for multiply-
ing matrices in standard arithmetic; for example, AlphaTensor finds
a rank-76 decomposition of T 4,5,5 , improving over the previous
state-of-the-art complexity of 80 multiplications. See Extended
Data Figs. 1 and 2 for examples.
AlphaTensor generates a large database of matrix multiplication
algorithms—up to thousands of algorithms for each size. We exploit
this rich space of algorithms by combining them recursively, with the
aim of decomposing larger matrix multiplication tensors. We refer
to refs. 25,26 and Appendix H in Supplementary Information for more
details. Using this approach, we improve over the state-of-the-art
results for more than 70 matrix multiplication tensors (with
n, m, p ≤ 12). See Fig. 3 (right) and Extended Data Table 1 for the results.
A crucial aspect of AlphaTensor is its ability to learn to transfer knowl-
edge between targets (despite providing no prior knowledge on their
relationship). By training one agent to decompose various tensors,
AlphaTensor shares learned strategies among these, thereby improv-
ing the overall performance (see Supplementary Information for
analysis). Finally, it is noted that AlphaTensor scales beyond current
computational approaches for decomposing tensors. For example, to
our knowledge, no previous approach was able to handle T 4, which has
an action space 1010 times larger than T 3. Our agent goes beyond this
limit, discovering decompositions matching or surpassing
state-of-the-art for large tensors such as T 5.
Data augmentation
From every played game, we can extract additional tensor-factorization
pairs for training the network. Specifically, as factorizations are
order invariant (owing to summation), we build an additional
tensor-factorization training pair by swapping a random action with
the last action from each finished game.
Algorithm discovery results
Discovery of matrix multiplication algorithms
We train a single AlphaTensor agent to find matrix multiplication algo-
rithms for matrix sizes n × m with m × p, where n, m, p ≤ 5. At the begin-
ning of each game, we sample uniformly a triplet (n, m, p) and train
AlphaTensor to decompose the tensor T n, m, p. Although we consider
tensors of fixed size (T n, m, p has size nm × mp × pn), the discovered algo-
rithms can be applied recursively to multiply matrices of arbitrary size.
We use AlphaTensor to find matrix multiplication algorithms over
different arithmetics—namely, modular arithmetic (that is, multiplying
matrices in the quotient ring Z2), and standard arithmetic (that is, mul-
tiplying matrices in R).
Figure 3 (left) shows the complexity (that is, rank) of the algo-
rithms discovered by AlphaTensor. AlphaTensor re-discovers the
best algorithms known for multiplying matrices (for example,
50 | Nature | Vol 610 | 6 October 2022a
n=4
n=5
n=6
Extrapolation
n = 10
W
V
U
n=3
b
Input: n × n skew-symmetric matrix A, vector b.
( n – 1 )( n + 2 )
multiplications.
Output: The resulting vector c = Ab computed in
2
(1) for i = 1, . . . , n − 2 do
(2)
for j = i + 1, . . . , n do
(3)
wij = aij ( bj – bi )
Computing the first ( n – 2 )( n + 1 )/2 intermediate products
(4) for i = 1, . . . , n do
n
(5)
q i = bi Σ
a
j=1 ji
(6) for i = 1, . . . , n − 2 do
n
i–1
(7)
ci = Σ
w – qi
w +
j=1 ji Σ j=i+1 ij
n–2 n–2
n
n–2
(8) cn–1 = – Σ
w –
w +
q
i=1 Σ j=i+1 ij Σ j=1 jn Σ i=1,i≠n–1 i
n–1 n
n–1
(9) cn = – Σ
w +
q
i=1 Σ j=i+1 ij Σ i=1 i
Fig. 4 | Algorithm discovery beyond standard matrix multiplication.
n(n − 1)
a, Decompositions found by AlphaTensor for the tensors of size 2 × n × n
(with n = 3, 4, 5, 6) representing the skew-symmetric matrix-vector multiplication.
The red pixels denote 1, the blue pixels denote −1 and the white pixels denote 0.
Analysing the symmetries of matrix multiplication algorithms
From a mathematical standpoint, the diverse algorithms discovered
by AlphaTensor show that the space is richer than previously known.
For example, while the only known rank-49 factorization decomposing
T 4 = T 2 ⊗ T 2 before this paper conforms to the product structure (that
is, it uses the factorization of T 2 twice, which we refer to as Strassen-
square2), AlphaTensor finds more than 14,000 non-equivalent fac-
torizations (with standard arithmetic) that depart from this scheme,
and have different properties (such as matrix ranks and sparsity—see
Supplementary Information). By non-equivalent, we mean that it is
not possible to obtain one from another by applying a symmetry trans-
formation (such as permuting the factors). Such properties of matrix
multiplication tensors are of great interest, as these tensors represent
fundamental objects in algebraic complexity theory3,5,7. The study of
matrix multiplication symmetries can also provide insight into the
asymptotic complexity of matrix multiplication5. By exploring this rich
space of algorithms, we believe that AlphaTensor will be useful for
generating results and guiding mathematical research. See Supple-
mentary Information for proofs and details on the symmetries of
factorizations.
Beyond standard matrix multiplication
Tensors can represent any bilinear operation, such as structured matrix
multiplication, polynomial multiplication or more custom bilinear
operations used in machine learning27,28. We demonstrate here a
use-case where AlphaTensor finds a state-of-the-art algorithm for
multiplying an n x n skew-symmetric matrix with a vector of length n.
Figure 4a shows the obtained decompositions for small instance sizes n.
We observe a pattern that we generalize to arbitrary n, and prove that
this yields a general algorithm for the skew-symmetric matrix-vector
product (Fig. 4b). This algorithm, which uses (n − 1)(n + 2)/2 ~ 21 n 2
multiplications (where ∼ indicates asymptotic similarity), outperforms
the previously known algorithms using asymptotically n2 multiplica-
tions29, and is asymptotically optimal. See Supplementary Information
Computing the final n intermediate products
Extrapolation to n = 10 is shown in the rightmost figure. b, Skew-symmetric
matrix-by-vector multiplication algorithm, obtained from the examples solved
by AlphaTensor. The wij and qi terms in steps 3 and 5 correspond to the m r terms
in Algorithm 1. It is noted that steps 6–9 do not involve any multiplications.
for a proof, and for another use-case showing AlphaTensor’s ability
to re-discover the Fourier basis (see also Extended Data Table 2). This
shows that AlphaTensor can be applied to custom bilinear operations,
and yield efficient algorithms leveraging the problem structure.
Rapid tailored algorithm discovery
We show a use-case where AlphaTensor finds practically efficient matrix
multiplication algorithms, tailored to specific hardware, with zero
prior hardware knowledge. To do so, we modify the reward of AlphaT-
ensor: we provide an additional reward at the terminal state (after the
agent found a correct algorithm) equal to the negative of the runtime
of the algorithm when benchmarked on the target hardware. That is,
we set r t′ = rt + λbt , where rt is the reward scheme described in ‘DRL for
algorithm discovery’, bt is the benchmarking reward (non-zero only at
the terminal state) and λ is a user-specified coefficient. Aside from the
different reward, the exact same formulation of TensorGame is used.
We train AlphaTensor to search for efficient algorithms to multiply
4 × 4 block matrices, and focus on square matrix multiplication of size
8,192 (each block is hence of size 2,048) to define the benchmarking
reward. AlphaTensor searches for the optimal way of combining the
16 square blocks of the input matrices on the considered hardware. We
do not apply the 4 × 4 algorithm recursively, to leverage the efficient
implementation of matrix multiplication on moderate-size matrices
(2,048 × 2,048 in this case). We study two hardware devices commonly
used in machine learning and scientific computing: an Nvidia V100
graphics processing unit (GPU) and a Google tensor processing unit
(TPU) v2. The factorization obtained by AlphaTensor is transformed
into JAX30 code, which is compiled ( just in time) before benchmarking.
Figure 5a,b shows the efficiency of the AlphaTensor-discovered
algorithms on the GPU and the TPU, respectively. AlphaTensor dis-
covers algorithms that outperform the Strassen-square algorithm,
which is a fast algorithm for large square matrices31,32. Although
the discovered algorithm has the same theoretical complexity as
Strassen-square, it outperforms it in practice, as it is optimized for
the considered hardware. Interestingly, AlphaTensor finds algorithms
Nature | Vol 610 | 6 October 2022 | 51Article
23.9%
21.3%
20,480
13.3%
10.7%
8,192
8.5%
4.3%
Strassen-square
AlphaTensor
9.2%
13.9%
8.9%
Fig. 5 | Speed-ups of the AlphaTensor-discovered algorithm. a,b, Speed-ups
(%) of the AlphaTensor-discovered algorithms tailored for a GPU (a) and a TPU
(b), optimized for a matrix multiplication of size 8,192 × 8,192. Speed-ups are
measured relative to standard (for example, cuBLAS for the GPU) matrix
multiplication on the same hardware. Speed-ups are reported for various
with a larger number of additions compared with Strassen-square (or
equivalently, denser decompositions), but the discovered algorithms
generate individual operations that can be efficiently fused by the
specific XLA33 grouping procedure and thus are more tailored towards
the compiler stack we use. The algorithms found by AlphaTensor also
provide gains on matrix sizes larger than what they were optimized
for. Finally, Fig. 5c shows the importance of tailoring to particular
hardware, as algorithms optimized for one hardware do not perform
as well on other hardware.
Discussion
Trained from scratch, AlphaTensor discovers matrix multiplication
algorithms that are more efficient than existing human and
computer-designed algorithms. Despite improving over known
algorithms, we note that a limitation of AlphaTensor is the need
to pre-define a set of potential factor entries F, which discretizes
the search space but can possibly lead to missing out on efficient
algorithms. An interesting direction for future research is to adapt
AlphaTensor to search for F. One important strength of AlphaTensor
is its flexibility to support complex stochastic and non-differentiable
rewards (from the tensor rank to practical efficiency on specific hard-
ware), in addition to finding algorithms for custom operations in a
wide variety of spaces (such as finite fields). We believe this will spur
applications of AlphaTensor towards designing algorithms that opti-
mize metrics that we did not consider here, such as numerical stability
or energy usage.
The discovery of matrix multiplication algorithms has far-reaching
implications, as matrix multiplication sits at the core of many com-
putational tasks, such as matrix inversion, computing the determi-
nant and solving linear systems, to name a few7. We also note that our
methodology can be extended to tackle related primitive mathemati-
cal problems, such as computing other notions of rank (for example,
border rank—see Supplementary Information), and NP-hard matrix
factorization problems (for example, non-negative factorization). By
tackling a core NP-hard computational problem in mathematics using
DRL—the computation of tensor ranks—AlphaTensor demonstrates the
viability of DRL in addressing difficult mathematical problems, and
potentially assisting mathematicians in discoveries.
52 | Nature | Vol 610 | 6 October 2022
Speed-up on TPU v2
Optimized for GPU
TPU
10.3%
6.6%
Optimized for TPU
2.8%
12.4%
9.0%
8,192
Speed-up on Nvidia V100 GPU
4.4%
13.4%
14,336
10,240
6.8%
8.5%
GPU
11.2%
7.2%
12,288
10.1%
10,240
Matrix size
16.1%
12,288
12.3%
6.9%
16,384
19.6%
14,336
8.4%
18,432
16.6%
13.8%
16,384
13.9%
20,480
17.9%
15.3%
18,432
Matrix size
c
b
Benchmark device
a
10.3%
Strassen-square
AlphaTensor
Speed-up of tailored agorithms
on both devices
matrix sizes (despite optimizing the algorithm only on one matrix size). We also
report the speed-up of the Strassen-square algorithm. The median speed-up is
reported over 200 runs. The standard deviation over runs is <0.4 percentage
points (see Supplementary Information for more details). c, Speed-up of both
algorithms (tailored to a GPU and a TPU) benchmarked on both devices.
Online content
Any methods, additional references, Nature Research reporting summa-
ries, source data, extended data, supplementary information, acknowl-
edgements, peer review information; details of author contributions
and competing interests; and statements of data and code availability
are available at https://doi.org/10.1038/s41586-022-05172-4.
1.
Silver, D. et al. A general reinforcement learning algorithm that masters chess, shogi, and
Go through self-play. Science 362, 1140–1144 (2018).
2.
Strassen, V. Gaussian elimination is not optimal. Numer. Math. 13, 354–356 (1969).
3.
Bürgisser, P., Clausen, M. & Shokrollahi, A. Algebraic Complexity Theory Vol. 315 (Springer
Science & Business Media, 2013).
4. Bläser, M. Fast matrix multiplication. Theory Comput. 5, 1–60 (2013).
5.
Landsberg, J. M. Geometry and Complexity Theory 169 (Cambridge Univ. Press, 2017).
6.
Pan, V. Y. Fast feasible and unfeasible matrix multiplication. Preprint at https://arxiv.org/
abs/1804.04102 (2018).
7.
Lim, L.-H. Tensors in computations. Acta Numer. 30, 555–764 (2021).
8.
Schönhage, A. Partial and total matrix multiplication. SIAM J. Comput. 10, 434–455 (1981).
9.
Coppersmith, D. & Winograd, S. Matrix multiplication via arithmetic progressions. In ACM
Symposium on Theory of Computing 1–6 (ACM, 1987).
10. Strassen, V. The asymptotic spectrum of tensors and the exponent of matrix
multiplication. In 27th Annual Symposium on Foundations of Computer Science 49–54
(IEEE, 1986).
11. Le Gall, F. Powers of tensors and fast matrix multiplication. In International Symposium on
Symbolic and Algebraic Computation 296–303 (ACM, 2014).
12. Alman, J. & Williams, V. V. A refined laser method and faster matrix multiplication. In
ACM-SIAM Symposium on Discrete Algorithms 522–539 (SIAM, 2021).
13. Gauss, C. F. Theoria Motus Corporum Coelestium in Sectionibus Conicis Solum
Ambientium (Perthes and Besser, 1809).
14. Hillar, C. J. & Lim, L.-H. Most tensor problems are NP-hard. J. ACM 60, 1–39 (2013).
15. Laderman, J. D. A noncommutative algorithm for multiplying 3 × 3 matrices using 23
multiplications. Bull. Am. Math. Soc. 82, 126–128 (1976).
16. Hopcroft, J. E. & Kerr, L. R. On minimizing the number of multiplications necessary for
matrix multiplication. SIAM J. Appl. Math. 20, 30–36 (1971).
17. Vervliet, N., Debals, O., Sorber, L., Van Barel, M. & De Lathauwer, L. Tensorlab 3.0 (2016);
https://www.tensorlab.net/
18. Smirnov, A. V. The bilinear complexity and practical algorithms for matrix multiplication.
Comput. Math. Math. Phys. 53, 1781–1795 (2013).
19. Sedoglavic, A. & Smirnov, A. V. The tensor rank of 5x5 matrices multiplication is bounded
by 98 and its border rank by 89. In Proc. 2021 on International Symposium on Symbolic
and Algebraic Computation 345–351 (ACM, 2021).
20. Heule, M. J., Kauers, M. & Seidl, M. New ways to multiply 3 × 3-matrices. J. Symb. Comput.
104, 899–916 (2021).
21. Hubert, T. et al. Learning and planning in complex action spaces. In International
Conference on Machine Learning 4476–4486 (PMLR, 2021).
22. Zhang, W. & Dietterich, T. G. A reinforcement learning approach to job-shop scheduling.
In International Joint Conferences on Artificial Intelligence Vol. 95, 1114–1120
(Morgan Kaufmann Publishers, 1995).23. Vaswani, A. Attention is all you need. In International Conference on Neural Information
Processing Systems Vol 30, 5998–6008 (Curran Associates, 2017).
24. Ho, J., Kalchbrenner, N., Weissenborn, D. & Salimans, T. Axial attention in
multidimensional transformers. Preprint at https://arxiv.org/abs/1912.12180 (2019).
25. Drevet, C.-É., Islam, M. N. & Schost, É. Optimization techniques for small matrix
multiplication. Theor. Comput. Sci. 412, 2219–2236 (2011).
26. Sedoglavic, A. A non-commutative algorithm for multiplying (7 × 7) matrices using 250
multiplications. Preprint at https://arxiv.org/abs/1712.07935 (2017).
27. Battaglia, P. W. et al. Relational inductive biases, deep learning, and graph networks.
Preprint at https://arxiv.org/abs/1806.01261 (2018).
28. Balog, M., van Merriënboer, B., Moitra, S., Li, Y. & Tarlow, D. Fast training of sparse graph
neural networks on dense hardware. Preprint at https://arxiv.org/abs/1906.11786 (2019).
29. Ye, K. & Lim, L.-H. Fast structured matrix computations: tensor rank and Cohn–Umans
method. Found. Comput. Math. 18, 45–95 (2018).
30. Bradbury, J. et al. JAX: composable transformations of Python+NumPy programs. GitHub
http://github.com/google/jax (2018).
31. Benson, A. R. & Ballard, G. A framework for practical parallel fast matrix multiplication.
ACM SIGPLAN Not. 50, 42–53 (2015).
32. Huang, J., Smith, T. M., Henry, G. M. & Van De Geijn, R. A. Strassen’s algorithm reloaded.
In International Conference for High Performance Computing, Networking, Storage and
Analysis 690–701 (IEEE, 2016).
33. Abadi, M. et al. Tensorflow: a system for large-scale machine learning. In USENIX
Symposium On Operating Systems Design And Implementation 265–283 (USENIX,
2016).
Publisher’s note Springer Nature remains neutral with regard to jurisdictional claims in
published maps and institutional affiliations.
Open Access This article is licensed under a Creative Commons Attribution
4.0 International License, which permits use, sharing, adaptation, distribution
and reproduction in any medium or format, as long as you give appropriate
credit to the original author(s) and the source, provide a link to the Creative Commons license,
and indicate if changes were made. The images or other third party material in this article are
included in the article’s Creative Commons license, unless indicated otherwise in a credit line
to the material. If material is not included in the article’s Creative Commons license and your
intended use is not permitted by statutory regulation or exceeds the permitted use, you will
need to obtain permission directly from the copyright holder. To view a copy of this license,
visit http://creativecommons.org/licenses/by/4.0/.
© The Author(s) 2022
Nature | Vol 610 | 6 October 2022 | 53Article
Methods
TensorGame
TensorGame is played as follows. The start position S 0 of the game
corresponds to the tensor T representing the bilinear operation of
interest, expressed in some basis. In each step t of the game, the player
writes down three vectors (u(t), v(t), w(t)), which specify the rank-1 tensor
u(t) ⊗ v(t) ⊗ w(t), and the state of the game is updated by subtracting the
newly written down factor:
S t ← S t −1 − u(t ) ⊗ v (t ) ⊗ w (t ).
(2)
The game ends when the state reaches the zero tensor, SR = 0. This
means that the factors written down throughout the game form a
R
factorization of the start tensor S 0, that is, S 0 = ∑t =1 u(t ) ⊗ v (t ) ⊗ w (t ) .
This factorization is then scored. For example, when optimizing for
asymptotic time complexity the score is −R, and when optimizing
for practical runtime the algorithm corresponding to the factorization
R
{(u(t ), v (t ), w (t ))}t =1 is constructed (see Algorithm 1) and then bench-
marked on the fly (see Supplementary Information).
In practice, we also impose a limit Rlimit on the maximum number of
moves in the game, so that a weak player is not stuck in unnecessarily
(or even infinitely) long games. When a game ends because it has run
out of moves, a penalty score is given so that it is never advantageous
to deliberately exhaust the move limit. For example, when optimizing
for asymptotic time complexity, this penalty is derived from an upper
bound on the tensor rank of the final residual tensor S R limit. This upper
bound on the tensor rank is obtained by summing the matrix ranks of
the slices of the tensor.
TensorGame over rings. We say that the decomposition of T n in equa-
tion (1) is in a ring E (defining the arithmetic operations) if each of the
factors u(t), v(t) and w(t) has entries belonging to the set E , and additions
and multiplications are interpreted according to E . The tensor rank
depends, in general, on the ring. At each step of TensorGame, the ad-
ditions and multiplications in equation (2) are interpreted in E . For
example, when working in Z2, (in this case, the factors u(t), v(t) and w(t)
live in F = {0, 1}), a modulo 2 operation is applied after each state update
(equation (2)).
We note that integer-valued decompositions u(t), v(t) and w(t) lead to
decompositions in arbitrary rings E . Hence, provided F only contains
integers, algorithms we find in standard arithmetic apply more gener-
ally to any ring.
AlphaTensor
AlphaTensor builds on AlphaZero 1 and its extension Sampled
AlphaZero21, combining a deep neural network with a sample-based
MCTS search algorithm.
The deep neural network, fθ(s) = (π, z) parameterized by θ, takes as
input the current state s of the game and outputs a probability distribu-
tion π(⋅∣s) over actions and z(⋅∣s) over returns (sum of future rewards) G.
The parameters θ of the deep neural network are trained by reinforce-
ment learning from self-play games and synthetic demonstrations.
Self-play games are played by actors, running a sample-based MCTS
search at every state st encountered in the game. The MCTS search
returns an improved probability distribution over moves from which
an action at is selected and applied to the environment. The sub-tree
under at is reused for the subsequent search at st+1. At the end of the
game, a return G is obtained and the trajectory is sent to the learner to
update the neural network parameters θ. The distribution over returns
z(⋅∣st) is learned through distributional reinforcement learning using
the quantile regression distributional loss34, and the network policy
π(⋅∣st) is updated using a Kullback–Leibler divergence loss, to maximize
its similarity to the search policy for self-play games or to the next
action for synthetic demonstrations. We use the Adam optimizer35
with decoupled weight decay36 to optimize the parameters θ of the
neural network.
Sample-based MCTS search. The sample-based MCTS search is very
similar to the one described in Sampled AlphaZero. Specifically, the
search consists of a series of simulated trajectories of TensorGame that
are aggregated in a tree. The search tree therefore consists of nodes
representing states and edges representing actions. Each state-action
pair (s, a) stores a set of statistics N (s , a), Q(s , a), πˆ(s , a), where N(s, a)
is the visit count, Q(s, a) is the action value and πˆ(s , a) is the empirical
policy probability. Each simulation traverses the tree from the root
state s0 until a leaf state sL is reached by recursively selecting in each
state s an action a that has not been frequently explored, has high em-
pirical policy probability and high value. Concretely, actions within
the tree are selected by maximizing over the probabilistic upper con-
fidence tree bound21,37
argmaxQ(s , a) + c(s) ⋅ πˆ(s , a)
a
∑ b N ( s , b)
1 + N (s , a)
,
where c(s) is an exploration factor controlling the influence of the
empirical policy πˆ(s , a)relative to the values Q(s, a) as nodes are visited
more often. In addition, a transposition table is used to recombine
different action sequences if they reach the exact same tensor. This
can happen particularly often in TensorGame as actions are commuta-
tive. Finally, when a leaf state sL is reached, it is evaluated by the neural
network, which returns K actions {ai} sampled from π(a∣sL), alongside
1
the empirical distribution πˆ(a sL) = K ∑i δa, ai and a value v(sL) con-
structed from z(⋅∣sL). Differently from AlphaZero and Sampled Alp-
haZero, we chose v not to be the mean of the distribution of returns
z(⋅∣sL) as is usual in most reinforcement learning agents, but instead to
be a risk-seeking value, leveraging the facts that TensorGame is a deter-
ministic environment and that we are primarily interested in finding
the best trajectory possible. The visit counts and values on the simu-
lated trajectory are then updated in a backward pass as in Sampled
AlphaZero.
Policy improvement. After simulating N(s) trajectories from state s
using MCTS, the normalized visit counts of the actions at the root of
the search tree N(s, a)/N(s) form a sample-based improved policy. Dif-
ferently from AlphaZero and Sampled AlphaZero, we use an adaptive
temperature scheme to smooth the normalized visit counts distribution
as some states can accumulate an order of magnitude more visits than
others because of sub-tree reuse and transposition table. Concretely,
we define the improved policy as Iπˆ(s , a) = N 1/τ (s)(s , a)/ ∑b N 1/τ (s)(s , b)
where τ(s) = logN (s)/logN if N > N and 1 otherwise, with N being a
hyperparameter. For training, we use Iπ̂ directly as a target for the
network policy π. For acting, we additionally discard all actions that
have a value lower than the value of the most visited action, and sample
proportionally to Iπ̂ among those remaining high-value actions.
Learning one agent for multiple target tensors. We train a single
agent to decompose the different tensors T n, m, p in a given arithmetic
(standard or modular). As the network works with fixed-size inputs, we
pad all tensors (with zeros) to the size of the largest tensor we consider
(T 5, of size 25 × 25 × 25). At the beginning of each game, we sample uni-
formly at random a target T n, m, p, and play TensorGame. Training a sin-
gle agent on different targets leads to better results thanks to the trans-
fer between targets. All our results reported in Fig. 3 are obtained using
multiple runs of this multi-target setting. We also train a single agent
to decompose tensors in both arithmetics. Owing to learned transfer
between the two arithmetics, this agent discovers a different distribu-
tion of algorithms (of the same ranks) in standard arithmetic than the
agent trained on standard arithmetic only, thereby increasing the over-
all diversity of discovered algorithms.Synthetic demonstrations. The synthetic demonstrations buffer
contains tensor-factorization pairs, where the factorizations
R
{(u(r ), v (r ), w (r ))}r =1 are first generated at random, after which the tensor
R
(r )
D = ∑r =1 u ⊗ v (r ) ⊗ w (r ) is formed. We create a dataset containing
5 million such tensor-factorization pairs. Each element in the factors
is sampled independently and identically distributed (i.i.d.) from a
given categorical distribution over F (all possible values that can be
taken). We discarded instances whose decompositions were clearly
suboptimal (contained a factor with u = 0, v = 0, or w = 0).
In addition to these synthetic demonstrations, we further add to
the demonstration buffer previous games that have achieved large
scores to reinforce the good moves made by the agent in these games.
Change of basis. The rank of a bilinear operation does not depend on
the basis in which the tensor representing it is expressed, and for any
invertible matrices A, B and C we have Rank (T) = Rank (T (A, B, C)), where
T (A, B, C) is the tensor after change of basis given by
S
S
S
(A, B, C)
T ijk
= ∑ ∑ ∑ AiaBjbCkc T abc .
(3)
a =1 b =1 c =1
Hence, exhibiting a rank-R decomposition of the matrix multiplica-
tion tensor T n expressed in any basis proves that the product of two
n × n matrices can be computed using R scalar multiplications. Moreo-
ver, it is straightforward to convert such a rank-R decomposition into
a rank-R decomposition in the canonical basis, thus yielding a practical
algorithm of the form shown in Algorithm 1. We leverage this observa-
tion by expressing the matrix multiplication tensor T n in a large number
of randomly generated bases (typically 100,000) in addition to the
canonical basis, and letting AlphaTensor play games in all bases in
parallel.
This approach has three appealing properties: (1) it provides a natural
exploration mechanism as playing games in different bases automati-
cally injects diversity into the games played by the agent; (2) it exploits
properties of the problem as the agent need not succeed in all bases—it
is sufficient to find a low-rank decomposition in any of the bases; (3)
it enlarges coverage of the algorithm space because a decomposition
with entries in a finite set F = {−2, −1, 0, 1, 2} found in a different basis
need not have entries in the same set when converted back into the
canonical basis.
In full generality, a basis change for a 3D tensor of size S × S × S is
specified by three invertible S × S matrices A, B and C. However, in our
procedure, we sample bases at random and impose two restrictions:
(1) A = B = C, as this performed better in early experiments, and (2)
unimodularity (detA ∈ {−1, + 1}), which ensures that after converting
an integral factorization into the canonical basis it still contains integer
entries only (this is for representational convenience and numerical
stability of the resulting algorithm). See Supplementary Information
for the exact algorithm.
Signed permutations. In addition to playing (and training on) games in
different bases, we also utilize a data augmentation mechanism when-
ever the neural network is queried in a new MCTS node. At acting time,
when the network is queried, we transform the input tensor by applying
a change of basis—where the change of basis matrix is set to a random
signed permutation. We then query the network on this transformed
input tensor, and finally invert the transformation in the network’s
policy predictions. Although this data augmentation procedure can
be applied with any generic change of basis matrix (that is, it is not
restricted to signed permutation matrices), we use signed permuta-
tions mainly for computational efficiency. At training time, whenever
the neural network is trained on an (input, policy targets, value target)
triplet (Fig. 2), we apply a randomly chosen signed permutation to
both the input and the policy targets, and train the network on this
transformed triplet. In practice, we sample 100 signed permutations
at the beginning of an experiment, and use them thereafter.
Action canonicalization. For any λ1, λ2, λ3 ∈ {−1, +1} such that λ1λ2λ3 = 1,
the actions (λ1u, λ2v, λ3w) and (u, v, w) are equivalent because they lead
to the same rank-one tensor (λ1u) ⊗ (λ2v) ⊗ (λ3w) = u ⊗ v ⊗ w. To pre-
vent the network from wasting capacity on predicting multiple equiva-
lent actions, during training we always present targets (u, v, w) for the
policy head in a canonical form, defined as having the first non-zero
element of u and the first non-zero element of v strictly positive. This is
well defined because u or v cannot be all zeros (if they are to be part of
a minimal rank decomposition), and for any (u, v, w) there are unique
λ1, λ2, λ3 ∈ {−1, +1} (with λ1λ2λ3 = 1) that transform it into canonical form.
In case the network predicts multiple equivalent actions anyway, we
merge them together (summing their empirical policy probabilities)
before inserting them into the MCTS tree.
Training regime. We train AlphaTensor on a TPU v3, with a total batch
size of 2,048. We use 64 TPU cores, and train for 600,000 iterations.
On the actor side, the games are played on standalone TPU v4, and we
use 1,600 actors. In practice, the procedure takes a week to converge.
Neural network
The architecture is composed of a torso, followed by a policy head that
predicts a distribution over actions, and a value head that predicts a dis-
tribution of the returns from the current state (see Extended Data Fig. 3).
Input. The input to the network contains all the relevant information
of the current state and is composed of a list of tensors and a list of
scalars. The most important piece of information is the current 3D
tensor S t of size S × S × S. (For simplicity, in the description here we
assume that all the three dimensions of the tensor are equal in size. The
generalization to different sizes is straightforward.) In addition, the
model is given access to the last h actions (h being a hyperparameter
usually set to 7), represented as h rank-1 tensors that are concatenated
to the input. The list of scalars includes the time index t of the current
action (where 0 ≤ t < Rlimit).
Torso. The torso of the network is in charge of mapping both scalars and
tensors from the input to a representation that is useful to both policy
and value heads. Its architecture is based on a modification of transform-
ers23, and its main signature is that it operates over three S × S grids pro-
jected from the S × S × S input tensors. Each grid represents two out of
the three modes of the tensor. Defining the modes of the tensor as
U, V, W , the rows and columns of the first grid are associated to U and V ,
respectively, the rows and columns of the second grid are associated to
W and U , and the rows and columns of the third grid are associated to V
and W . Each element of each grid is a feature vector, and its initial value
is given by the elements of the input tensors along the grid’s missing
mode. These feature vectors are enriched by concatenating an S × S × 1
linear projection from the scalars. This is followed by a linear layer pro-
jecting these feature vectors into a 512-dimensional space.
The rest of the torso is a sequence of attention-based blocks with the
objective of propagating information between the three grids. Each of
those blocks has three stages, one for every pair of grids. In each stage,
the grids involved are concatenated, and axial attention24 is performed
over the columns. It is noted that in each stage we perform in parallel S
self-attention operations of 2S elements in each. The representation
sent to the policy head corresponds to the 3S2 512-dimensional feature
vectors produced by the last layer of the torso. A detailed description
of the structure of the torso is specified in Extended Data Fig. 4 (top)
and Appendix A.1.1 in Supplementary Information.
Policy head. The policy head uses the transformer architecture23
to model an autoregressive policy. Factors are decomposed into kArticle
tokens of dimensionality d such that k × d = 3S. The transformer con-
ditions on the tokens already generated and cross-attends to the fea-
tures produced by the torso. At training time, we use teacher-forcing,
that is, the ground truth actions are decomposed into tokens and
taken as inputs into the causal transformer in such a way that the
prediction of a token depends only on the previous tokens. At infer-
ence time, K actions are sampled from the head. The feature repre-
sentation before the last linear layer of the initial step (that is, the
only step that is not conditioned on the ground truth) is used as an
input to the value head, described below. Details of the architecture
are presented in Extended Data Fig. 4 (centre) and Appendix A.1.2 in
Supplementary Information.
Value head. The value head is composed of a four-layer multilayer
perceptron whose last layer produces q outputs corresponding to the
2q − 1
1
3
, , … 2q quantiles. In this way, the value head predicts the distri-
2q 2q
bution of returns from this state in the form of values predicted for the
aforementioned quantiles34. At inference time, we encourage the agent
to be risk-seeking by using the average of the predicted values for quan-
tiles over 75%. A detailed description of the value head is presented in
Extended Data Fig. 4 (bottom) and Appendix A.1.3 in Supplementary
Information.
Related work
The quest for efficient matrix multiplication algorithms started with
Strassen’s breakthrough in ref. 2, which showed that one can multiply
2 × 2 matrices using 7 scalar multiplications, leading to an algorithm
of complexity O(n 2.81). This led to the development of a very active field
of mathematics attracting worldwide interest, which studies the asymp-
totic complexity of matrix multiplication (see refs. 3–6). So far, the best
known complexity for matrix multiplication is O(n 2.37286)(ref. 12), which
improves over ref. 11, and builds on top of fundamental results in the
field8–10. However, this does not yield practical algorithms, as such
approaches become advantageous only for astronomical matrix sizes.
Hence, a significant body of work aims at exhibiting explicit factoriza-
tions of matrix multiplication tensors, as these factorizations provide
practical algorithms. After Strassen’s breakthrough showing that
rank (T 2) ≤ 7 , efficient algorithms for larger matrix sizes were
found15,16,18,26,38. Most notably, Laderman showed in ref. 15 that 3 × 3
matrix multiplications can be performed with 23 scalar multiplications.
In addition to providing individual low-rank factorizations, an impor-
tant research direction aims at understanding the space of matrix
multiplication algorithms—as opposed to exhibiting individual
low-rank factorizations—by studying the symmetry groups and diver-
sity of factorizations (see ref. 5 and references therein). For example,
the symmetries of 2 × 2 matrix multiplication were studied in refs. 39–42,
where Strassen’s algorithm was shown to be essentially unique. The
case of 3 × 3 was studied in ref. 43, whereas a symmetric factorization
for all n is provided in ref. 44.
On the computational front, continuous optimization has been the
main workhorse for decomposing tensors17,45,46, and in particular matrix
multiplication tensors. Such continuous optimization procedures (for
example, alternating least squares), however, yield approximate solu-
tions, which correspond to inexact matrix multiplication algorithms
with floating point operations. To circumvent this issue, regularization
procedures have been proposed, such as ref. 18, to extract exact decom-
positions. Unfortunately, such approaches often require substantial
human intervention and expertise to decompose large tensors. A dif-
ferent line of attack was explored in refs. 47,48, based on learning the
continuous weights of a two-layer network that mimics the structure
of the matrix multiplication operation. This method, which is trained
through supervised learning of matrix multiplication examples, finds
approximate solutions to 2 × 2 and 3 × 3 matrix multiplications.
In ref. 48, a quantization procedure is further used to obtain an exact
decomposition for 2 × 2. Unlike continuous optimization-based
approaches, AlphaTensor directly produces algorithms from the
desired set of valid algorithms, and is flexible in that it allows us to
optimize a wide range of (even non-differentiable) objectives. This
unlocks tackling broader settings (for example, optimization in finite
fields, optimization of runtime), as well as larger problems (for exam-
ple, T 4 and T 5) than those previously considered. Different from con-
tinuous optimization, a boolean satisfiability (SAT) based formulation
of the problem of decomposing 3 × 3 matrix multiplication was
recently proposed in ref. 20, which adds thousands of new decompo-
sitions of rank 23 to the list of known 3 × 3 factorizations. The approach
relies on a state-of-the-art SAT solving procedure, where several
assumptions and simplifications are made on the factorizations to
reduce the search space. As is, this approach is, however, unlikely to
scale to larger tensors, as the search space grows very quickly with
the size.
On the practical implementation front, ref. 31 proposed several ideas
to speed up implementation of fast matrix multiplication algorithms
on central processing units (CPUs). Different fast algorithms are then
compared and benchmarked, and the potential speed-up of such algo-
rithms is shown against standard multiplication. Other works focused
on getting the maximal performance out of a particular fast matrix
multiplication algorithm (Strassen’s algorithm with one or two levels
of recursion) on a CPU32 or a GPU49. These works show that, despite
popular belief, such algorithms are of practical value. We see writing
a custom low-level implementation of a given algorithm to be distinct
from the focus of this paper—developing new efficient algorithms—and
we believe that the algorithms we discovered can further benefit from
a more efficient implementation by experts.
Beyond matrix multiplication and bilinear operations, a growing
amount of research studies the use of optimization and machine learn-
ing to improve the efficiency of computational operations. There
are three levels of abstractions at which this can be done: (1) in the
hardware design, for example, chip floor planning50, (2) at the hard-
ware–software interface, for example, program super-optimization
of a reference implementation for specific hardware51, and (3) on the
algorithmic level, for example, program induction52, algorithm selec-
tion53 or meta-learning54. Our work focuses on the algorithmic level of
abstraction, although AlphaTensor is also flexible to discover efficient
algorithms for specific hardware. Different from previous works, we
focus on discovering matrix multiplication algorithms that are prov-
ably correct, without requiring initial reference implementations. We
conclude by relating our work broadly to existing reinforcement learn-
ing methods for scientific discovery. Within mathematics, reinforce-
ment learning was applied, for example, to theorem proving55–58, and
to finding counterexamples refuting conjectures in combinatorics and
graph theory59. Reinforcement learning was further shown to be useful
in many areas in science, such as molecular design60,61 and synthesis62
and optimizing quantum dynamics63.
Data availability
The data used to train the system were generated synthetically accord-
ing to the procedures explained in the paper. The algorithms discov-
ered by AlphaTensor are available for download at https://github.com/
deepmind/alphatensor.
Code availability
An interactive notebook with code to check the non-equivalence of algo-
rithms is provided. Moreover, the fast algorithms from the ‘Algorithm
discovery results’ section on a GPU and a TPU are provided. These are
available for download at https://github.com/deepmind/alphatensor.
A full description of the AlphaZero algorithm that this work is based
on is available in ref. 1, and the specific neural network architecture we
use is described using pseudocode in the Supplementary Information.34. Dabney, W., Rowland, M., Bellemare, M. & Munos, R. Distributional reinforcement learning
with quantile regression. In AAAI Conference on Artificial Intelligence Vol. 32, 2892–2901
(AAAI Press, 2018).
35. Kingma, D. P., & Ba, J. Adam: a method for stochastic optimization. In International
Conference on Learning Representations (ICLR) (2015).
36. Loshchilov, I. & Hutter, F. Decoupled weight decay regularization. In International
Conference on Learning Representations (ICLR) (2019).
37. Silver, D. et al. Mastering the game of Go with deep neural networks and tree search.
Nature 529, 484–489 (2016).
38. Sedoglavic, A. A non-commutative algorithm for multiplying 5x5 matrices using 99
multiplications. Preprint at https://arxiv.org/abs/1707.06860 (2017).
39. de Groote, H. F. On varieties of optimal algorithms for the computation of bilinear
mappings II. optimal algorithms for 2 × 2-matrix multiplication. Theor. Comput. Sci. 7,
127–148 (1978).
40. Burichenko, V. P. On symmetries of the Strassen algorithm. Preprint at https://arxiv.org/
abs/1408.6273 (2014).
41. Chiantini, L., Ikenmeyer, C., Landsberg, J. M. & Ottaviani, G. The geometry of rank
decompositions of matrix multiplication I: 2 × 2 matrices. Exp. Math. 28, 322–327 (2019).
42. Grochow, J. A. & Moore, C. Designing Strassen’s algorithm. Preprint at https://arxiv.org/
abs/1708.09398 (2017).
43. Ballard, G., Ikenmeyer, C., Landsberg, J. M. & Ryder, N. The geometry of rank
decompositions of matrix multiplication II: 3 × 3 matrices. J. Pure Appl. Algebra 223,
3205–3224 (2019).
44. Grochow, J. A. & Moore, C. Matrix multiplication algorithms from group orbits. Preprint at
https://arxiv.org/abs/1612.01527 (2016).
45. Kolda, T. G. & Bader, B. W. Tensor decompositions and applications. SIAM Rev. 51,
455–500 (2009).
46. Bernardi, A., Brachat, J., Comon, P. & Mourrain, B. General tensor decomposition,
moment matrices and applications. J. Symb. Comput. 52, 51–71 (2013).
47. Elser, V. A network that learns Strassen multiplication. J. Mach. Learn. Res. 17, 3964–3976
(2016).
48. Tschannen, M., Khanna, A. & Anandkumar, A, StrassenNets: deep learning with a
multiplication budget. In International Conference on Machine Learning 4985–4994
(PMLR, 2018).
49. Huang, J., Yu, C. D. & Geijn, R. A. V. D. Strassen’s algorithm reloaded on GPUs. ACM Trans.
Math. Softw. 46, 1–22 (2020).
50. Mirhoseini, A. et al. A graph placement methodology for fast chip design. Nature 594,
207–212 (2021).
51. Bunel, R., Desmaison, A., Kohli, P., Torr, P. H. & Kumar, M. P. Learning to superoptimize
programs. In International Conference on Learning Representations (ICLR) (2017).
52. Li, Y., Gimeno, F., Kohli, P. & Vinyals, O. Strong generalization and efficiency in neural
programs. Preprint at https://arxiv.org/abs/2007.03629 (2020).
53. Lagoudakis, M. G. et al. Algorithm selection using reinforcement learning. In International
Conference on Machine Learning 511–518 (Morgan Kaufmann Publishers, 2000).
54. Schmidhuber, J. Evolutionary Principles in Self-Referential Learning. On Learning now to
Learn: The Meta-Meta-Meta...-Hook. Diploma thesis, Technische Univ. Munchen (1987).
55. Kaliszyk, C., Urban, J., Michalewski, H. & Olšák, M. Reinforcement learning of theorem
proving. In International Conference on Neural Information Processing Systems
8836–8847 (Curran Associates, 2018).
56. Piotrowski, B. & Urban, J. ATPboost: learning premise selection in binary setting with ATP
feedback. In International Joint Conference on Automated Reasoning 566–574 (Springer,
2018).
57.
58.
59.
60.
61.
62.
63.
64.
Bansal, K., Loos, S., Rabe, M., Szegedy, C. & Wilcox, S. HOList: an environment for
machine learning of higher order logic theorem proving. In International Conference on
Machine Learning 454–463 (PMLR, 2019).
Zombori, Z., Urban, J. & Brown, C. E. Prolog technology reinforcement learning prover.
In International Joint Conference on Automated Reasoning 489–507 (Springer, 2020).
Wagner, A. Z. Constructions in combinatorics via neural networks. Preprint at https://
arxiv.org/abs/2104.14516 (2021).
Popova, M., Isayev, O. & Tropsha, A. Deep reinforcement learning for de novo drug
design. Sci. Adv. 4, eaap7885 (2018).
Zhou, Z., Kearnes, S., Li, L., Zare, R. N. & Riley, P. Optimization of molecules via deep
reinforcement learning. Sci. Rep. 9, 10752 (2019).
Segler, M. H., Preuss, M. & Waller, M. P. Planning chemical syntheses with deep neural
networks and symbolic AI. Nature 555, 604–610 (2018).
Dalgaard, M., Motzoi, F., Sørensen, J. J. & Sherson, J. Global optimization of quantum
dynamics with AlphaZero deep exploration. npj Quantum Inf. 6, 6 (2020).
Fast matrix multiplication algorithms catalogue. Université de Lille https://fmm.univ-
lille.fr/ (2021).
Acknowledgements We thank O. Fawzi, H. Fawzi, C. Ikenmeyer, J. Ellenberg, C. Umans and
A. Wigderson for the inspiring discussions on the use of machine learning for maths; A. Davies,
A. Gaunt, P. Mudigonda, R. Bunel and O. Ronneberger for their advice on early drafts of the
paper; A. Ruderman, M. Bauer, R. Leblond, R. Kabra and B. Winckler for participating in a
hackathon at the early stages of the project; D. Visentin, R. Tanburn and S. Noury for sharing
their expertise on TPUs; P. Wang and R. Zhao for their help on benchmarking algorithms;
G. Holland, A. Pierce, N. Lambert and C. Meyer for assistance coordinating the research; and
our colleagues at DeepMind for encouragement and support.
Author contributions A.F. conceived the project, with support from B.R.-P. and P.K.; T.H., A.H.
and J.S. developed the initial AlphaZero codebase, and B.R.-P., M. Balog, A.F., A.N., F.J.R.R. and
G.S. developed an early supervised network prototype. A.H., T.H., B.R.-P., M. Barekatain and
J.S. designed the network architecture used in the paper. T.H., J.S., A.H., M. Barekatain, A.F.,
M. Balog and F.J.R.R. developed the tensor decomposition environment and data generation
pipeline, and A.H., T.H., M. Barekatain, M. Balog, B.R.-P., F.J.R.R. and A.N. analysed the
experimental results and algorithms discovered by AlphaTensor. A.N., A.F. and T.H. developed
the benchmarking pipeline and experiments, and B.R.-P., F.J.R.R. and A.N. extended the
approach to structured tensors. A.F., B.R.-P., G.S. and A.N. proved the results in the paper.
D.S., D.H. and P.K. contributed technical advice and ideas. A.F., M. Balog, B.R.-P., F.J.R.R., A.N.
and T.H. wrote the paper. These authors contributed equally, and are listed alphabetically by last
name after the corresponding author: A.F., M. Balog, A.H., T.H., B.R.-P. These authors contributed
equally, and are listed alphabetically by last name: M. Barekatain, A.N., F.J.R.R., J.S. and G.S.
Competing interests The authors of the paper are planning to file a patent application relating
to subject matter contained in this paper in the name of DeepMind Technologies Limited.
Additional information
Supplementary information The online version contains supplementary material available at
https://doi.org/10.1038/s41586-022-05172-4.
Correspondence and requests for materials should be addressed to Alhussein Fawzi.
Peer review information Nature thanks Grey Ballard, Jordan Ellenberg, Lek-Heng Lim, Michael
Littman and the other, anonymous, reviewer(s) for their contribution to the peer review of this
work.
Reprints and permissions information is available at http://www.nature.com/reprints.Article
Extended Data Fig. 1 | Algorithm for multiplying 4 × 4 matrices in modular arithmetic (Z 2) with 47 multiplications. This outperforms the two-level Strassen’s
algorithm, which involves 72 = 49 multiplications.Extended Data Fig. 2 | Algorithm for multiplying 4 × 5 by 5 × 5 matrices in standard arithmetic with 76 multiplications. This outperforms the previously best
known algorithm, which involves 80 multiplications.Article
Extended Data Fig. 3 | AlphaTensor’s network architecture. The network
takes as input the list of tensors containing the current state and previous
history of actions, and a list of scalars, such as the time index of the current
action. It produces two kinds of outputs: one representing the value, and the
other inducing a distribution over the action space from which we can sample
from. The architecture of the network is accordingly designed to have a
common torso, and two heads, the value and the policy heads. c is set to 512 in
all experiments.Extended Data Fig. 4 | Detailed view of AlphaTensor’s architecture, included torso, policy and value head. We refer to Algorithms A.1-A.11 in Supplementary
Information for the details of each component.Article
Extended Data Table 1 | Rank results obtained by combining decompositions (in standard arithmetic)
The table shows the cases where we were able to obtain an improvement over state-of-the-art, for tensors T n, m, p (with n, m, p≤12). The recipe column indicates the low-level matrix multiplica-
tion algorithms used to build the corresponding factorization. 〈n, m, p〉 denotes the best known bound on the rank of T n, m, p; see Appendix H in Supplementary Information for more details. For
tensors that were directly decomposed by AlphaTensor, the recipe shows a star mark, e.g. 〈3, 4, 5〉*. All the factorizations are made available.Extended Data Table 2 | Result of applying AlphaTensor to the tensor representing the cyclic convolution operation
AlphaTensor finds the discrete Fourier matrix (DFT) and the inverse DFT matrix in finite fields. The figure shows the decompositions found by AlphaTensor of the n × n × n tensor representing the
cyclic convolution of two vectors, for three different values of n in the finite field of order 17. The action space, characterized by the number of possible factor triplets {u(r), v(r), w(r)}, is thus 173n,
which is of the order of 1029 for n = 8. Despite the huge action space, AlphaTensor finds the optimal rank-n decompositions for the three values of n. The factors in the figure are stacked vertically,
i.e., U = [u(1), …, u(n)]. For ease of visualization, the factor entries have been expressed in terms of powers of an n-th primitive root of unity in the finite field. Within each column, each colour
uniquely represents one element of the field (e.g., for the column n = 4, we have depicted in grey 40 = 44 = 4−4 = 1). By inspecting the patterns in the decompositions, one could extrapolate the
results for other values of n and other fields. Indeed, the factors u(r) and v(r) correspond to the DFT coefficients, since u(kr ) = v (kr ) = z kr, whereas the factors w(r) correspond to the inverse DFT, since
w (kr ) = z −kr /n for 0≤k, r < n, where z is an n-th primitive root of unity (i.e., zn = 1 and zj ≠ 1 for any 1≤j < n).


The Illusion of Thinking:
Understanding the Strengths and Limitations of Reasoning Models
via the Lens of Problem Complexity
Parshin Shojaee∗†
Maxwell Horton
Iman Mirzadeh∗
Keivan Alizadeh
Samy Bengio
Mehrdad Farajtabar
Apple
Abstract
Recent generations of frontier language models have introduced Large Reasoning Models
(LRMs) that generate detailed thinking processes before providing answers. While these models
demonstrate improved performance on reasoning benchmarks, their fundamental capabilities, scal-
ing properties, and limitations remain insufficiently understood. Current evaluations primarily fo-
cus on established mathematical and coding benchmarks, emphasizing final answer accuracy. How-
ever, this evaluation paradigm often suffers from data contamination and does not provide insights
into the reasoning traces’ structure and quality. In this work, we systematically investigate these
gaps with the help of controllable puzzle environments that allow precise manipulation of composi-
tional complexity while maintaining consistent logical structures. This setup enables the analysis
of not only final answers but also the internal reasoning traces, offering insights into how LRMs
“think”. Through extensive experimentation across diverse puzzles, we show that frontier LRMs
face a complete accuracy collapse beyond certain complexities. Moreover, they exhibit a counter-
intuitive scaling limit: their reasoning effort increases with problem complexity up to a point, then
declines despite having an adequate token budget. By comparing LRMs with their standard LLM
counterparts under equivalent inference compute, we identify three performance regimes: (1) low-
complexity tasks where standard models surprisingly outperform LRMs, (2) medium-complexity
tasks where additional thinking in LRMs demonstrates advantage, and (3) high-complexity tasks
where both models experience complete collapse. We found that LRMs have limitations in exact
computation: they fail to use explicit algorithms and reason inconsistently across puzzles. We
also investigate the reasoning traces in more depth, studying the patterns of explored solutions
and analyzing the models’ computational behavior, shedding light on their strengths, limitations,
and ultimately raising crucial questions about their true reasoning capabilities.
1
Introduction
Large Language Models (LLMs) have recently evolved to include specialized variants explicitly
designed for reasoning tasks—Large Reasoning Models (LRMs) such as OpenAI’s o1/o3 [1, 2],
DeepSeek-R1 [3], Claude 3.7 Sonnet Thinking [4], and Gemini Thinking [5]. These models are new
artifacts, characterized by their “thinking” mechanisms such as long Chain-of-Thought (CoT) with
self-reflection, and have demonstrated promising results across various reasoning benchmarks. Their
∗
†
Equal contribution.
Work done during an internship at Apple.
{p_shojaee, imirzadeh, kalizadehvahid, mchorton, bengio, farajtabar}@apple.com
1LLM Response
Peg 0
Claude 3.7
(+thinking)
80
60
40
20
Claude 3.7
0
1 2 3 4 5 6 7 8 10
15
Complexity (number of disks)
20
Initial State
[1, 0, 2]

extract moves from thoughts  [2, 0, 1]

[1, 2, 1]

(for analysis)
[3, 0, 2]
Middle State
extract final answer 
(for measuring accuracy)
Target State
20,000
Claude 3.7
(+thinking)
15,000
10,000
Claude 3.7
5,000
0
1 2 3 4 5 6 7 8 10
15
Complexity (number of disks)
Peg 2
1
23
[1, 1, 0]

[2, 1, 2]

[1, 0, 2]
20
Position within Thoughts
Accuracy (%)
100
Response Length (Tokens)
<think>

Move disk 1 from peg 0 to peg 2 ...  
moves = [

[1, 0, 2],

[2, 0, 1],

[1, 2, 1],

[3, 0, 2],

[1, 1, 0],

[2, 1, 2],

[1, 0, 2],

]

Let me double-check this...

</think>
<answer> the final answer is moves=...
</answer>
Peg 1
1
2
3
1
2
3
1.0
0.8
0.6
0.4
Correct Solutions
Incorrect Solutions
0.2
0.0
1 2 3 4 5 6 7 8 9 10
Complexity (number of disks)
15
Figure 1: Top: Our setup enables verification of both final answers and intermediate reasoning traces,
allowing detailed analysis of model thinking behavior. Bottom left & middle: At low complexity,
non-thinking models are more accurate and token-efficient. As complexity increases, reasoning models
outperform but require more tokens—until both collapse beyond a critical threshold, with shorter
traces. Bottom right: For correctly solved cases, Claude 3.7 Thinking tends to find answers early
at low complexity and later at higher complexity. In failed cases, it often fixates on an early wrong
answer, wasting the remaining token budget. Both cases reveal inefficiencies in the reasoning process.
emergence suggests a potential paradigm shift in how LLM systems approach complex reasoning
and problem-solving tasks, with some researchers proposing them as significant steps toward more
general artificial intelligence capabilities.
Despite these claims and performance advancements, the fundamental benefits and limitations of
LRMs remain insufficiently understood. Critical questions still persist: Are these models capable
of generalizable reasoning, or are they leveraging different forms of pattern matching [6]? How
does their performance scale with increasing problem complexity? How do they compare to their
non-thinking standard LLM counterparts when provided with the same inference token compute?
Most importantly, what are the inherent limitations of current reasoning approaches, and what
improvements might be necessary to advance toward more robust reasoning capabilities?
We believe the lack of systematic analyses investigating these questions is due to limitations in
current evaluation paradigms. Existing evaluations predominantly focus on established mathematical
and coding benchmarks, which, while valuable, often suffer from data contamination issues and do
not allow for controlled experimental conditions across different settings and complexities. Moreover,
these evaluations do not provide insights into the structure and quality of reasoning traces. To
understand the reasoning behavior of these models more rigorously, we need environments that
enable controlled experimentation.
In this study, we probe the reasoning mechanisms of frontier LRMs through the lens of problem
2complexity. Rather than standard benchmarks (e.g., math problems), we adopt controllable puzzle en-
vironments that let us vary complexity systematically—by adjusting puzzle elements while preserving
the core logic—and inspect both solutions and internal reasoning (Fig. 1, top). These puzzles: (1) of-
fer fine-grained control over complexity; (2) avoid contamination common in established benchmarks;
(3) require only the explicitly provided rules, emphasizing algorithmic reasoning; and (4) support
rigorous, simulator-based evaluation, enabling precise solution checks and detailed failure analyses.
Our empirical investigation reveals several key findings about current Language Reasoning Models
(LRMs): First, despite their sophisticated self-reflection mechanisms learned through reinforcement
learning, these models fail to develop generalizable problem-solving capabilities for planning tasks,
with performance collapsing to zero beyond a certain complexity threshold. Second, our comparison
between LRMs and standard LLMs under equivalent inference compute reveals three distinct reason-
ing regimes (Fig. 1, bottom). For simpler, low-compositional problems, standard LLMs demonstrate
greater efficiency and accuracy. As problem complexity moderately increases, thinking models gain
an advantage. However, when problems reach high complexity with longer compositional depth,
both model types experience complete performance collapse (Fig. 1, bottom left). Notably, near
this collapse point, LRMs begin reducing their reasoning effort (measured by inference-time tokens)
as problem complexity increases, despite operating well below generation length limits (Fig. 1,
bottom middle). This suggests a fundamental inference time scaling limitation in LRMs’ reasoning
capabilities relative to problem complexity. Finally, our analysis of intermediate reasoning traces or
thoughts reveals complexity-dependent patterns: In simpler problems, reasoning models often identify
correct solutions early but inefficiently continue exploring incorrect alternatives—an “overthinking”
phenomenon. At moderate complexity, correct solutions emerge only after extensive exploration
of incorrect paths. Beyond a certain complexity threshold, models completely fail to find correct
solutions (Fig. 1, bottom right). This indicates LRMs possess limited self-correction capabilities
that, while valuable, reveal fundamental inefficiencies and clear scaling limitations.
These findings highlight both the strengths and limitations of existing LRMs, raising questions
about the nature of reasoning in these systems with important implications for their design and
deployment. Our key contributions are:
• We question the current evaluation paradigm of LRMs on established math benchmarks and
design a controlled experimental testbed by leveraging algorithmic puzzle environments that enable
controllable experimentation with respect to problem complexity.
• We show that state-of-the-art LRMs (e.g., o3-mini, DeepSeek-R1, Claude-3.7-Sonnet-Thinking)
still fail to develop generalizable problem-solving capabilities, with accuracy ultimately collapsing
to zero beyond certain complexities across different environments.
• We find that there exists a scaling limit in the LRMs’ reasoning effort with respect to problem
complexity, evidenced by the counterintuitive decreasing trend in the thinking tokens after a
complexity point.
• We question the current evaluation paradigm based on final accuracy and extend our evaluation
to intermediate solutions of thinking traces with the help of deterministic puzzle simulators. Our
analysis reveals that as problem complexity increases, correct solutions systematically emerge at
later positions in thinking compared to incorrect ones, providing quantitative insights into the
self-correction mechanisms within LRMs.
• We uncover surprising limitations in LRMs’ ability to perform exact computation, including their
failure to benefit from explicit algorithms and their inconsistent reasoning across puzzle types.
32
Related Works
Reasoning in Language Models. Large Language Models (LLMs) undergo multiple costly
training phases using vast amounts of training data. While these LLMs demonstrate promising
language understanding with strong compression capabilities, their intelligence and reasoning abilities
remain a critical topic of scientific debate [7, 8]. Earlier iterations of LLMs [9, 10, 11] exhibited
poor performance on reasoning benchmarks [12, 13, 14, 6]. To address these shortcomings, several
approaches have been explored with the common theme among them being “scaling” both the training
data and test-time computation. For instance, generating a Chain of Thought (CoT) [15, 16, 17, 18]
and incorporating self-verification [19, 20, 21] prior to the final answer have been shown to improve
model performance. However, obtaining high-quality and scalable CoT data is quite expensive
due to its scarcity. Another line of research focuses on compensating for the lack of supervised
data by teaching models to think more effectively through supervised learning or reinforcement
learning [22, 23, 24, 25, 26, 27]. A notable open-source example of these improvements is Deepseek-
R1 [3], which demonstrated that applying RL with verifiable rewards can significantly enhance model
performance, matching that of closed models like OpenAI’s o1 [2], leading to a new generation of
language models referred to as Large Reasoning Models (LRMs) such as Gemini flash thinking [5],
Claude 3.7 Sonnet thinking [4], etc.
Understanding Large Reasoning Models. Recent studies have explored various aspects of
reasoning behavior: Large Reasoning Models have shown emergent behaviors such as discrepancy
between thought traces and final answers [28, 29] as well as efficiency concerns through what
researchers term the “overthinking phenomenon” [30, 31, 32, 33], where models produce verbose,
redundant outputs, even after finding the solution, creating significant inference computational
overhead. In this work, we systematically analyze how much model thinks w.r.t task complexity.
Recently, Ballon et al. [34] demonstrated that in newer LRMs accuracy generally declines when
thinking increases in math problems, in contrast we observe when in controlled puzzle environment
difficulty passes a certain level the model starts to think less and opposite corelation of thinking and
task complexity only happens up to some threshold. Yue et al. [35] questioned whether reinforcement
learning truly elicits novel reasoning patterns and shows pass@k of reasoning vs non-reasoning models
converge to the same point. We also observe that in MATH-500 pass@k is close for reasoning versus
non-reasoning models but we observed different patterns under medium and high complexity of
puzzles, which is not easily observable on established math benchmarks used in common evaluations.
Controllable Evaluation Environments. Unlike earlier studies that focused on mathematical
problems to evaluate the reasoning capabilities of language models, this work introduces controllable
puzzle environments. These environments allow for precise manipulation of problem complexity while
maintaining consistent logical processes, enabling a more rigorous analysis of reasoning patterns and
limitations. Controllable environments are not uncommon in the literature [12, 36, 37]. However,
our primary aim is not to propose a new benchmark; instead, we use these benchmarks as tools
for designing experiments to understand the reasoning capabilities of language models. A closely
related study by Valmeekam et al. [38] demonstrated that o1-models show significant performance
improvements compared to previous models. Our work offers additional insights, such as examining
pairs of thinking/non-thinking models (e.g., DeepSeek-R1/V3, Claude 3.7 Sonnet thinking/non-
thinking). Furthermore, we study the reasoning traces of the LRMs in more depth, revealing different
behaviors across various complexity levels.
Overall, the promising results from recent LRMs raise a critical question: how much have the
previously reported limitations of LLMs been improved? In this work, we move beyond merely
measuring the performance of these LRMs. We analyze how well these LRMs tackle problems of
varying complexities and examine the properties of their reasoning processes.
4MATH-500
AIME24
pass@k
pass@k
95
90
85
80
claude-3-7-sonnet-thinking
claude-3-7-sonnet-no-thinking
0
5000
10000
15000
20000
25000
30000
100
8080
60
40
claude-3-7-sonnet-thinking
claude-3-7-sonnet-no-thinking
20
0
35000
0
Inference Compute Budget (Tokens)
50000
MATH-500
DeepSeek-R1
DeepSeek-V3
10000
20000
claude-3-7-sonnet-thinking
claude-3-7-sonnet-no-thinking
20
0
200000
0
50000
30000
40000
Inference Compute Budget (Tokens)
100
8080
60
40
0
DeepSeek-R1
DeepSeek-V3
0
20000
40000
60000
80000
150000
200000
AIME25
100
20
100000
Inference Compute Budget (Tokens)
pass@k
pass@k
pass@k
90
0
150000
40
AIME24
95
80
100000
60
Inference Compute Budget (Tokens)
100
85
AIME25
100
pass@k
100
100000
120000
Inference Compute Budget (Tokens)
60
40
DeepSeek-R1
DeepSeek-V3
20
0
0
20000
40000
60000
80000
100000 120000
Inference Compute Budget (Tokens)
Figure 2: Comparative analysis of thinking versus non-thinking models across math benchmarks
reveals inconsistent performance patterns. While results on the MATH-500 dataset show comparable
performance between both model types, the thinking models demonstrate superior performance
on AIME24 and AIME25 benchmarks. Additionally, the observed performance degradation from
AIME24 to AIME25 highlights the vulnerability of these benchmarks to data contamination issues.
3
Math and Puzzle Environments
Currently, it is not clear whether the performance enhancements observed in recent RL-based
thinking models are attributable to increased exposure to established mathematical benchmark
data, to the significantly greater inference compute allocated to thinking tokens, or to reasoning
capabilities developed by RL-based training? Recent studies [35, 39] have explored this question
with established math benchmarks by comparing the upper-bound capabilities (pass@k) of RL-based
thinking models with their non-thinking standard LLM counterparts. They have shown that under
equivalent inference token budgets, non-thinking LLMs can eventually reach performance comparable
to thinking models on benchmarks like MATH500 [40] and AIME24 [41]. We also conducted our
comparative analysis of frontier LRMs like Claude-3.7-Sonnet (with vs. without thinking) and
DeepSeek (R1 vs. V3). Our results (shown in Fig. 2) confirm that, on the MATH500 dataset, the
pass@k performance of thinking models is comparable to their non-thinking counterparts when
provided with the same inference token budget. However, we observed that this performance gap
widens on the AIME24 benchmark and widens further on AIME25. This widening gap presents
an interpretive challenge. It could be attributed to either: (1) increasing complexity requiring
more sophisticated reasoning processes, thus revealing genuine advantages of the thinking models
for more complex problems, or (2) reduced data contamination in newer benchmarks (particularly
AIME25). Interestingly, human performance on AIME25 was actually higher than on AIME24
[42, 43], suggesting that AIME25 might be less complex. Yet models perform worse on AIME25
than AIME24—potentially suggesting data contamination during the training of frontier LRMs.
Given these non-justified observations and the fact that mathematical benchmarks do not allow for
controlled manipulation of problem complexity, we turned to puzzle environments that enable more
precise and systematic experimentation.
5Tower of Hanoi
Checkers Jumping
River Crossing
Blocks World
Initial State
moves
Middle State
moves
Target State
Figure 3: Illustration of the four puzzle environments. Columns show the progression from initial
state (top) through intermediate state (middle) to target state (bottom) for puzzles: Tower
of Hanoi (disk transfer across pegs), Checkers Jumping (position swapping of colored tokens), River
Crossing (transporting entities across a river), and Blocks World (stack reconfiguration).
3.1
Puzzle Environments
We evaluate LRM reasoning on four controllable puzzles spanning compositional depth, planning
complexity, and distributional settings. The puzzles are defined below and illustrated in Fig. 3.
Tower of Hanoi is a puzzle featuring three pegs and n disks of different sizes stacked on the first
peg in size order (largest at bottom). The goal is to transfer all disks from the first peg to the third
peg. Valid moves include moving only one disk at a time, taking only the top disk from a peg, and
never placing a larger disk on top of a smaller one. The difficulty in this task can be controlled by
the number of initial disks as the minimum number of required moves with n initial disks will be
2n − 1. However, in this work we do not grade for optimality of final solution and only measuring
the correctness of each move and reaching the target state.
Checker Jumping is a one-dimensional puzzle arranging red checkers, blue checkers, and a single
empty space in a line. The objective is to swap the positions of all red and blue checkers, effectively
mirroring the initial configuration. Valid moves include sliding a checker into an adjacent empty
space or jumping over exactly one checker of the opposite color to land in an empty space. No checker
can move backward in the puzzle process. The complexity of this task can be controlled by the
number of checkers: with 2n checkers, the minimum number of moves required will be (n + 1)2 − 1.
River Crossing is a constraint satisfaction planning puzzle involving n actors and their corresponding
n agents who must cross a river using a boat. The goal is to transport all 2n individuals from the
left bank to the right bank. The boat can carry at most k individuals and cannot travel empty.
Invalid situations arise when an actor is in the presence of another agent without their own agent
present, as each agent must protect their client from competing agents. The complexity of this task
can also be controlled by the number of actor/agent pairs present. For n = 2, n = 3 pairs, we use
boat capacity of k = 2 and for larger number of pairs we use k = 3.
Blocks World is a block-stacking puzzle requiring rearrangement of blocks from an initial configu-
ration into a specified goal configuration. The objective is to find the minimum number of moves
needed for this transformation. Valid moves are restricted to the topmost block of any stack, which
can be placed either on an empty stack or on top of another block. The complexity in this task can
be controlled by the number of blocks present.
6Figure 4: Accuracy of thinking models (Claude 3.7 Sonnet with thinking, DeepSeek-R1) versus their
non-thinking counterparts (Claude 3.7 Sonnet, DeepSeek-V3) across all puzzle environments and
varying levels of problem complexity.
4Experiments & Results
4.1Experimental Setup
Most of our experiments are conducted on reasoning models and their non-thinking counterparts,
such as Claude 3.7 Sonnet (thinking/non-thinking) and DeepSeek-R1/V3. We chose these models
because they allow access to the thinking tokens, unlike models such as OpenAI’s o-series. For
experiments focused solely on final accuracy, we also report results on the o-series models. For Claude
3.7 Sonnet models, we allow the maximum token budget (64k). Similarly, for DeepSeek-R1/V3
models on local servers, we allow the maximum length to be up to 64k tokens. For each puzzle
instance, we generate 25 samples and report the average performance of each model across them.
Comprehensive details of our experimental setup and results are provided in the Appendix.
4.2How Does Complexity Affect Reasoning?
4.2.1Three Regimes of Complexity
Motivated by the observations in Fig. 2, to systematically investigate the impact of problem complexity
on reasoning behavior, we conducted experiments comparing thinking and non-thinking model
pairs across our controlled puzzle environments. Our analysis focused on matched pairs of LLMs
with identical model backbones, specifically Claude-3.7-Sonnet (w. vs. w/o thinking) and DeepSeek
(R1 vs. V3). In each puzzle, we vary the complexity by manipulating problem size N (representing
disk count, checker count, block count, or crossing elements).
Fig. 4 presents the accuracy of both model types as a function of problem complexity across all
puzzle environments. Complementing this, Fig. 5 shows the upper bound performance capabilities
(pass@k) of these model pairs under equivalent inference token compute (averaged across all puzzles),
extending earlier analyses from mathematical benchmarks (Fig. 2) to the controlled puzzle environ-
ments. Results from both these figures demonstrate that, unlike observations from math, there exists
three regimes in the behavior of these models with respect to complexity. In the first regime where
problem complexity is low, we observe that non-thinking models are capable to obtain performance
comparable to, or even better than thinking models with more token-efficient inference. In the
7Figure 5: Pass@k performance of thinking vs. non-thinking models across equivalent compute
budgets in puzzle environments of low , medium , and high complexity. Non-thinking models excel
in simple problems, thinking models show advantages at medium complexity, while both approaches
fail at high complexity regardless of compute allocation.
second regime with medium complexity, the advantage of reasoning models capable of generating
long chain-of-thought begin to manifest, and the performance gap between model pairs increases. The
most interesting regime is the third regime where problem complexity is higher and the performance
of both models have collapsed to zero. Results show that while thinking models delay this collapse,
they also ultimately encounter the same fundamental limitations as their non-thinking counterparts.
4.2.2
Collapse of Reasoning Models
We next examine how different specialized reasoning models equipped with thinking tokens respond
to increasing problem complexity. Our experiments evaluate five state-of-the-art thinking models:
o3-mini (medium and high configurations), DeepSeek-R1, DeepSeek-R1-Qwen-32B, and Claude-3.7-
Sonnet (thinking). Fig. 6 demonstrates these models’ performance in terms of accuracy (top) and
thinking token usage (bottom) across varying complexity levels. Results show that all reasoning
models exhibit a similar pattern with respect to complexity: accuracy progressively declines as
problem complexity increases until reaching complete collapse (zero accuracy) beyond a model-
specific complexity threshold. Analysis of inference thinking token compute also reveals an intriguing
pattern in thinking token allocation learned by these models. We observe that reasoning models
initially increase their thinking tokens proportionally with problem complexity. However, upon
approaching a critical threshold—which closely corresponds to their accuracy collapse point—models
counterintuitively begin to reduce their reasoning effort despite increasing problem difficulty. This
phenomenon is most pronounced in o3-mini variants and less severe in the Claude-3.7-Sonnet
(thinking) model. Notably, despite operating well below their generation length limits with ample
inference budget available, these models fail to take advantage of additional inference compute during
the thinking phase as problems become more complex. This behavior suggests a fundamental scaling
limitation in the thinking capabilities of current reasoning models relative to problem complexity.
8Figure 6: Accuracy and thinking tokens vs. problem complexity for reasoning models across puzzle
environments. As complexity increases, reasoning models initially spend more tokens while accuracy
declines gradually, until a critical point where reasoning collapses—performance drops sharply and
reasoning effort decreases.
4.3
What Happens Inside the Thoughts of Reasoning Models?
To gain deeper insights into the thinking processes of reasoning models, we conducted a fine-grained
analysis of their reasoning traces. As shown in Fig. 1, our setup with puzzle environments allows us
to look beyond final answer and obtain more detailed insight into the reasoning traces (“thoughts”)
produced by these models. We extract and analyze the intermediate solutions explored within the
thoughts of a model with the help of puzzle simulators. Our investigation examines the patterns and
characteristics of these intermediate solutions, their correctness relative to their sequential position
in the reasoning process, and how these patterns evolve with increasing problem complexity. For
this analysis, we focus on the reasoning traces generated by Claude-3.7-Sonnet-Thinking across
our puzzle suite. For each intermediate solution identified within the traces, we recorded: (1) its
relative position within the reasoning trace (normalized by total thought length), (2) its correctness
as validated by our puzzle simulators, and (3) the complexity of the corresponding problem. This
allows to characterize the progression and accuracy of solution development throughout the reasoning
process.
Fig. 7a demonstrates the relation between the position of intermediate solutions within thoughts, their
correctness, and problem complexity across all puzzle environments. Our analysis from reasoning
traces also further validates three regimes of complexity discussed above. For simpler problems,
reasoning models often find the correct solution early in their thinking but then continue exploring
incorrect solutions. Note the distribution of incorrect solutions (red) is shifted more upward towards
end of thinking compared to correct solutions (green). This phenomenon, referred to as “overthinking”
in the literature, leads to the waste of compute. As problems become moderately more complex,
this trend reverses: models first explore incorrect solutions and mostly later in thought arrive at
the correct ones. This time the distribution of incorrect solutions (red) is shifted more downward
compared to correct ones (green). Finally, for the problems with higher complexity, collapse emerges,
9Tower of Hanoi
Solution Accuracy (%)
100
N=1
N=2
N=3
N=4
N=5
N=6
N=7
N=8
N=10
80
60
40
20
0
0
4000
8000
12000
Position in Thinking (Token)
(a)
(b)
Figure 7: Left & Middle: Position and correctness of intermediate solutions within reasoning traces
across four puzzles at varying complexity levels. ✓ indicates correct solutions, ✗ indicates incorrect
solutions, with distribution density shown by shading; Right: Solution accuracy versus position
in thinking for Tower of Hanoi at different complexity levels. Simple problems (N=1-3) show early
accuracy declining over time (overthinking), moderate problems (N=4-7) show slight improvement
in accuracy with continued reasoning, and complex problems (N≥8) exhibit consistently near-zero
accuracy, indicating complete reasoning failure.
meaning that the model fails to generate any correct solutions within the thought.
Fig. 7b presents a complementary analysis of solution accuracy within sequential segments (bins)
of the thoughts in the Tower of Hanoi environment. It can be observed that for simpler problems
(smaller N), solution accuracy tends to decrease or oscillate as thinking progresses, providing further
evidence of the overthinking phenomenon. However, this trend changes for more complex problems,
where solution accuracy increases with thinking progression—up to a certain threshold. Beyond this
complexity threshold, in the “collapse mode”, accuracy is zero.
4.4
Open Questions: Puzzling Behavior of Reasoning Models
In this section, we present surprising results concerning the limitations of reasoning models in
executing exact problem-solving steps, as well as demonstrating different behaviors of the models
based on the number of moves.
As shown in Figures 8a and 8b, in the Tower of Hanoi environment, even when we provide the
algorithm in the prompt—so that the model only needs to execute the prescribed steps—performance
does not improve, and the observed collapse still occurs at roughly the same point. This is noteworthy
because finding and devising a solution should require substantially more computation (e.g., for search
and verification) than merely executing a given algorithm. This further highlights the limitations of
reasoning models in verification and in following logical steps to solve a problem, suggesting that
further research is needed to understand the symbolic manipulation capabilities of such models [44, 6].
Moreover, in Figures 8c and 8d, we observe very different behavior from the Claude 3.7 Sonnet think-
ing model. In the Tower of Hanoi environment, the model’s first error in the proposed solution often
occurs much later, e.g., around move 100 for (N=10), compared to the River Crossing environment,
where the model can only produce a valid solution until move 4. Note that this model also achieves
near-perfect accuracy when solving the Tower of Hanoi with (N=5), which requires 31 moves, while
it fails to solve the River Crossing puzzle when (N=3), which has a solution of 11 moves. This likely
suggests that examples of River Crossing with N>2 are scarce on the web, meaning LRMs may not
have frequently encountered or memorized such instances during training.
1040
80
60
40
2020
00
1 2 3 4 5 6 7 8 9 10
15
Complexity (Number of Disks)
(a)
20
Tower of Hanoi
Claude-3.7-Sonnet (thinking)
Algorithm Given
Default
1 2 3 4 5 6 7 8 9 10
15
20
Complexity (Number of Disks)
100
80
60
40
20
0
Claude-3.7-Sonnet (thinking)
1 2 3 4 5 6 7 8 9 10
15
Complexity (Number of Disks)
(b)
(c)
20
First Wrong Move (Median)
60
100
Accuracy (%)
80
Accuracy (%)
Tower of Hanoi
DeepSeek-R1
Algorithm Given
Default
First Wrong Move (Median)
Tower of Hanoi
100
River Crossing
10
Claude-3.7-Sonnet (thinking)
8
6
4
2
0
2 3 4 5 6
8
10
15
20
Complexity (Number of People)
(d)
Figure 8: (a) & (b) Despite providing the solution algorithm in the prompt, execution failure
occurs at similar points, highlighting reasoning model limitations in logical step execution. (c) &
(d) Notably, the Claude 3.7 Sonnet model demonstrates much longer error-free sequences in the
Tower of Hanoi compared to early errors in the River Crossing scenario.
5
Conclusion
In this paper, we systematically examine frontier Large Reasoning Models (LRMs) through the lens
of problem complexity using controllable puzzle environments. Our findings reveal fundamental
limitations in current models: despite sophisticated self-reflection mechanisms, these models fail to
develop generalizable reasoning capabilities beyond certain complexity thresholds. We identified
three distinct reasoning regimes: standard LLMs outperform LRMs at low complexity, LRMs excel at
moderate complexity, and both collapse at high complexity. Particularly concerning is the counterin-
tuitive reduction in reasoning effort as problems approach critical complexity, suggesting an inherent
compute scaling limit in LRMs. Our detailed analysis of reasoning traces further exposed complexity-
dependent reasoning patterns, from inefficient “overthinking” on simpler problems to complete failure
on complex ones. These insights challenge prevailing assumptions about LRM capabilities and
suggest that current approaches may be encountering fundamental barriers to generalizable reasoning.
Finally, we presented some surprising results on LRMs that lead to several open questions for future
work. Most notably, we observed their limitations in performing exact computation; for example,
when we provided the solution algorithm for the Tower of Hanoi to the models, their performance
on this puzzle did not improve. Moreover, investigating the first failure move of the models revealed
surprising behaviors. For instance, they could perform up to 100 correct moves in the Tower of
Hanoi but fail to provide more than 5 correct moves in the River Crossing puzzle. We believe our
results can pave the way for future investigations into the reasoning capabilities of these systems.
Limitations
We acknowledge that our work has limitations. While our puzzle environments enable controlled
experimentation with fine-grained control over problem complexity, they represent a narrow slice of
reasoning tasks and may not capture the diversity of real-world or knowledge-intensive reasoning
problems. It is notable that most of our experiments rely on black-box API access to the closed frontier
LRMs, limiting our ability to analyze internal states or architectural components. Furthermore, the
use of deterministic puzzle simulators assumes that reasoning can be perfectly validated step by
step. However, in less structured domains, such precise validation may not be feasible, limiting the
transferability of this analysis to other more generalizable reasoning.
11Acknowledgments
The authors would like to thank Scott Hoang, Yichen Jiang, Minsik Cho, Mohammad Sekhavat, David
Harrison, Mohammadreza Armandpour and Devi Krishna for the valuable feedback and support.
References
[1] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec
Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv
preprint arXiv:2412.16720, 2024.
[2] OpenAI. Introducing openai o1. Jan 2024.
[3] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu,
Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms
via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025.
[4] Anthropic. Claude 3.7 sonnet. Feb 2025.
[5] Google. Gemini flash thinking. Google AI Blog, Jan 2025.
[6] Seyed Iman Mirzadeh, Keivan Alizadeh, Hooman Shahrokhi, Oncel Tuzel, Samy Bengio,
and Mehrdad Farajtabar. GSM-symbolic: Understanding the limitations of mathematical
reasoning in large language models. In The Thirteenth International Conference on Learning
Representations, 2025.
[7] Francois Chollet, Mike Knoop, Gregory Kamradt, Bryan Landers, and Henry Pinkard. Arc-agi-2:
A new challenge for frontier ai reasoning systems. arXiv preprint arXiv:2505.11831, 2025.
[8] Gary Marcus. Five ways in which the last 3 months — and especially the deepseek era — have
vindicated "deep learning is hitting a wall". Marcus on AI (Substack), February 2025. Blog
post.
[9] Marah I Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany
Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat S. Behl, and et. al. Phi-3
technical report: A highly capable language model locally on your phone. CoRR, abs/2404.14219,
2024.
[10] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chap-
lot, Diego de Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier,
Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril,
Thomas Wang, Timothée Lacroix, and William El Sayed. Mistral 7b. CoRR, abs/2310.06825,
2023.
[11] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle,
Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony
Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark,
Arun Rao, Aston Zhang, Aurélien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Rozière,
Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris
Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong,
Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny
12Livshits, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino,
Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael
Smith, Filip Radenovic, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson,
Graeme Nail, Grégoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar,
Hu Xu, Hugo Touvron, and et al. The llama 3 herd of models. CoRR, abs/2407.21783, 2024.
[12] Nouha Dziri, Ximing Lu, Melanie Sclar, Xiang Lorraine Li, Liwei Jiang, Bill Yuchen Lin, Sean
Welleck, Peter West, Chandra Bhagavatula, Ronan Le Bras, Jena D. Hwang, Soumya Sanyal,
Xiang Ren, Allyson Ettinger, Zaïd Harchaoui, and Yejin Choi. Faith and fate: Limits of
transformers on compositionality. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko,
Moritz Hardt, and Sergey Levine, editors, Advances in Neural Information Processing Systems
36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New
Orleans, LA, USA, December 10 - 16, 2023, 2023.
[13] R. Thomas McCoy, Shunyu Yao, Dan Friedman, Matthew Hardy, and Thomas L. Griffiths.
Embers of autoregression: Understanding large language models through the problem they are
trained to solve, 2023.
[14] Marianna Nezhurina, Lucia Cipolina-Kun, Mehdi Cherti, and Jenia Jitsev. Alice in wonderland:
Simple tasks showing complete reasoning breakdown in state-of-the-art large language models.
arXiv preprint arXiv:2406.02061, 2024.
[15] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi,
Quoc V. Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language
models. In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh,
editors, Advances in Neural Information Processing Systems 35: Annual Conference on Neural
Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 -
December 9, 2022, 2022.
[16] Mehran Kazemi, Najoung Kim, Deepti Bhatia, Xin Xu, and Deepak Ramachandran. Lam-
bada: Backward chaining for automated reasoning in natural language. arXiv preprint
arXiv:2212.13894, 2022.
[17] Hattie Zhou, Azade Nova, Hugo Larochelle, Aaron Courville, Behnam Neyshabur, and Hanie
Sedghi. Teaching algorithmic reasoning via in-context learning. arXiv preprint arXiv:2211.09066,
2022.
[18] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large
language models are zero-shot reasoners. Advances in neural information processing systems,
35:22199–22213, 2022.
[19] Yixuan Weng, Minjun Zhu, Fei Xia, Bin Li, Shizhu He, Shengping Liu, Bin Sun, Kang Liu, and
Jun Zhao. Large language models are better reasoners with self-verification. In Houda Bouamor,
Juan Pino, and Kalika Bali, editors, Findings of the Association for Computational Linguistics:
EMNLP 2023, pages 2550–2575, Singapore, December 2023. Association for Computational
Linguistics.
[20] Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, and Weizhu Chen.
Making language models better reasoners with step-aware verifier. In Proceedings of the 61st
Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),
pages 5315–5333, 2023.
13[21] Eric Zhao, Pranjal Awasthi, and Sreenivas Gollapudi. Sample, scrutinize and scale: Effective
inference-time search by scaling verification. arXiv preprint arXiv:2502.01839, 2025.
[22] Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. STar: Bootstrapping reasoning
with reasoning. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors,
Advances in Neural Information Processing Systems, 2022.
[23] Sachin Goyal, Ziwei Ji, Ankit Singh Rawat, Aditya Krishna Menon, Sanjiv Kumar, and
Vaishnavh Nagarajan. Think before you speak: Training language models with pause tokens.
In The Twelfth International Conference on Learning Representations, 2024.
[24] David Herel and Tomas Mikolov. Thinking tokens for language modeling. ArXiv, abs/2405.08644,
2024.
[25] Zhihong Shao, Peiyi Wang, Runxin Xu Qihao Zhu, Junxiao Song, Mingchuan Zhang, Y.K. Li,
Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open
language models, 2024.
[26] Amirhossein Kazemnejad, Milad Aghajohari, Eva Portelance, Alessandro Sordoni, Siva Reddy,
Aaron Courville, and Nicolas Le Roux. Vineppo: Unlocking rl potential for llm reasoning
through refined credit assignment, 2024.
[27] Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze
Brahman, Lester James V. Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, Yuling Gu, Saumya
Malik, Victoria Graf, Jena D. Hwang, Jiangjiang Yang, Ronan Le Bras, Oyvind Tafjord, Chris
Wilhelm, Luca Soldaini, Noah A. Smith, Yizhong Wang, Pradeep Dasigi, and Hannaneh Ha-
jishirzi. Tülu 3: Pushing frontiers in open language model post-training. ArXiv, abs/2411.15124,
2024.
[28] Yanda Chen, Joe Benton, Ansh Radhakrishnan, Jonathan Uesato, Carson Denison, John
Schulman, Arushi Somani, Peter Hase, Misha Wagner, Fabien Roger, et al. Reasoning models
don’t always say what they think. arXiv preprint arXiv:2505.05410, 2025.
[29] Dacheng Li, Shiyi Cao, Tyler Griggs, Shu Liu, Xiangxi Mo, Eric Tang, Sumanth Hegde, Kourosh
Hakhamaneshi, Shishir G Patil, Matei Zaharia, et al. Llms can easily learn to reason from
demonstrations structure, not content, is what matters! arXiv preprint arXiv:2502.07374, 2025.
[30] Xingyu Chen, Jiahao Xu, Tian Liang, Zhiwei He, Jianhui Pang, Dian Yu, Linfeng Song, Qiuzhi
Liu, Mengfei Zhou, Zhuosheng Zhang, et al. Do not think that much for 2+ 3=? on the
overthinking of o1-like llms. arXiv preprint arXiv:2412.21187, 2024.
[31] Yang Sui, Yu-Neng Chuang, Guanchu Wang, Jiamu Zhang, Tianyi Zhang, Jiayi Yuan, Hongyi
Liu, Andrew Wen, Hanjie Chen, Xia Hu, et al. Stop overthinking: A survey on efficient reasoning
for large language models. arXiv preprint arXiv:2503.16419, 2025.
[32] Sara Vera Marjanović, Arkil Patel, Vaibhav Adlakha, Milad Aghajohari, Parishad
BehnamGhader, Mehar Bhatia, Aditi Khandelwal, Austin Kraft, Benno Krojer, Xing Han
Lù, et al. Deepseek-r1 thoughtology: Let’s< think> about llm reasoning. arXiv preprint
arXiv:2504.07128, 2025.
[33] Yuxiao Qu, Matthew YR Yang, Amrith Setlur, Lewis Tunstall, Edward Emanuel Beeching,
Ruslan Salakhutdinov, and Aviral Kumar. Optimizing test-time compute via meta reinforcement
fine-tuning. arXiv preprint arXiv:2503.07572, 2025.
14[34] Marthe Ballon, Andres Algaba, and Vincent Ginis. The relationship between reasoning and
performance in large language models–o3 (mini) thinks harder, not longer. arXiv preprint
arXiv:2502.15631, 2025.
[35] Yang Yue, Zhiqi Chen, Rui Lu, Andrew Zhao, Zhaokai Wang, Shiji Song, and Gao Huang. Does
reinforcement learning really incentivize reasoning capacity in llms beyond the base model?
arXiv preprint arXiv:2504.13837, 2025.
[36] Benjamin Estermann, Luca A. Lanzendörfer, Yannick Niedermayr, and Roger Wattenhofer.
Puzzles: A benchmark for neural algorithmic reasoning, 2024.
[37] Karthik Valmeekam, Alberto Olmo Hernandez, Sarath Sreedharan, and Subbarao Kambhampati.
Large language models still can’t plan (A benchmark for llms on planning and reasoning about
change). CoRR, abs/2206.10498, 2022.
[38] Karthik Valmeekam, Kaya Stechly, and Subbarao Kambhampati. Llms still can’t plan; can
lrms? a preliminary evaluation of openai’s o1 on planbench. 2024.
[39] Wenjie Ma, Jingxuan He, Charlie Snell, Tyler Griggs, Sewon Min, and Matei Zaharia. Reasoning
models can be effective without thinking. arXiv preprint arXiv:2504.09858, 2025.
[40] Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan
Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let’s verify step by step. arXiv preprint
arXiv:2305.20050, 2023.
[41] Mathematical
Association
of
America.
American
invitational
math-
ematics
examination
(aime).
https://maa.org/math-competitions/
american-invitational-mathematics-examination-aime, 2025. Accessed: 2025-05-15.
[42] Art of Problem Solving.
Amc historical results - aime i (february 1, 2024).
https://artofproblemsolving.com/wiki/index.php/AMC_historical_results#AIME_
I_.28February_1.2C_2024.29, 2024. Accessed: 2025-05-15.
Amc historical results – aime i (february 6, 2025).
[43] Art of Problem Solving.
https://artofproblemsolving.com/wiki/index.php/AMC_historical_results#AIME_
I_.28February_6.2C_2025.29, 2025. Accessed: 2025-05-15.
[44] Gary F Marcus. The algebraic mind: Integrating connectionism and cognitive science. MIT
press, 2003.
[45] Saul Amarel. On representations of problems of reasoning about actions. In Readings in artificial
intelligence, pages 2–22. Elsevier, 1981.
[46] Günter Rote. Crossing the bridge at night. Bulletin of the EATCS, 78:241, 2002.
15A
Appendix
In this appendix, we provide details supplementing the main text, including experimental setup
specifications, additional results, and extended analysis.
A.1 Details on Puzzle Environment Specifications and Design - Comprehensive descriptions of all
four puzzle environments, including their problem descriptions, prompt designs, and simulators.
A.1.1 Tower of Hanoi
A.1.2 Checker Jumping
A.1.3 River Crossing
A.1.4 Blocks World
A.2 Implementation Details - Full experimental setup specifications, model configurations, extrac-
tion pipeline details, and prescribed algorithm execution experiments.
A.3 Details on Computational Complexity
A.3.1 Compositional Depth Characterization
A.3.2 Performance vs Compositional Depth
A.4 Additional Results and Analysis - Extended analysis including reasoning effort patterns, and
detailed failure analysis across all models and puzzle environments.
A.1Details on Puzzle Environment Specifications and Design
A.1.1Tower of Hanoi
Problem Description. The Tower of Hanoi is a classic recursive puzzle that serves as a great
problem for evaluating sequential reasoning and planning capabilities in reasoning models. The
puzzle consists of three pegs (labeled 0, 1, and 2 from left to right) and N disks of varying sizes,
where each disk is uniquely numbered from 1 (smallest) to N (largest). In the initial configuration,
all N disks are stacked on the leftmost peg (peg 0) in descending order of size, with the largest disk
at the bottom and the smallest at the top. The remaining two pegs (1 and 2) are initially empty.
The goal is to transfer all disks from peg 0 to peg 2, maintaining the same size ordering (largest
at bottom, smallest at top). This puzzle is governed by three fundamental constraints: (1) Single
Disk Movement: Only one disk may be moved at a time; (2) Top Disk Access: Only the topmost
disk from any peg can be selected for movement; and (3) Size Ordering Constraint: A larger disk
may never be placed on top of a smaller disk. This puzzle is a good evaluation testbed for reasoning
and planning capabilities of models as it requires models to demonstrate key cognitive demands
such as breaking down the problem into subproblems (recursive thinking), tracking multiple states
and disk positions simultaneously (working memory management), adhering to movement rules and
constraints while planning ahead (constraint satisfaction), and determining the correct order of
operations to achieve the final goal (sequential planning).
The minimum number of moves required to solve the Tower of Hanoi recursive puzzle with N disks
is 2N − 1, making it an exponentially scaling problem. This property allows for fine-grained difficulty
control by adjusting the problem size with number of initial disks. However, in our evaluation
framework, we focus on solution correctness rather than optimality, assessing each of the move’s
validity and the model’s ability to reach the target state as the success criteria.
16Prompt Design. The system prompt begins with a clear problem statement describing the puzzle
setup. It explicitly states the movement rules and the objective of transferring all disks to the third
peg. To facilitate understanding, the prompt includes example demonstrations as well as the critical
formatting and reasoning expectations.
System Prompt - Tower of Hanoi
You are a helpful assistant. Solve this puzzle for me.
There are three pegs and n disks of different sizes stacked on the first peg. The disks are
numbered from 1 (smallest) to n (largest). Disk moves in this puzzle should follow:
1. Only one disk can be moved at a time.
2. Each move consists of taking the upper disk from one stack and placing it on top of
another stack.
3. A larger disk may not be placed on top of a smaller disk.
The goal is to move the entire stack to the third peg.
Example: With 3 disks numbered 1 (smallest), 2, and 3 (largest), the initial state is [[3, 2, 1],
[], []], and a solution might be:
moves = [[1 , 0 , 2] , [2 , 0 , 1] , [1 , 2 , 1] , [3 , 0 , 2] ,
[1 , 1 , 0] , [2 , 1 , 2] , [1 , 0 , 2]]
This means: Move disk 1 from peg 0 to peg 2, then move disk 2 from peg 0 to peg 1, and so on.
Requirements:
• When exploring potential solutions in your thinking process, always include the corre-
sponding complete list of moves.
• The positions are 0-indexed (the leftmost peg is 0).
• Ensure your final answer includes the complete list of moves in the format:
moves = [[disk id, from peg, to peg], ...]
The user prompt after the system prompt presents the specific puzzle instance with current configu-
ration showing the distribution of disks across pegs and the goal configuration specifying the target
state.
User Prompt Template for $N$ Disks - Tower of Hanoi
I have a puzzle with $N$ disks of different sizes with
Initial configuration:
• Peg 0: $N$ (bottom), . . . 2, 1 (top)
• Peg 1: (empty)
• Peg 2: (empty)
17Goal configuration:
• Peg 0: (empty)
• Peg 1: (empty)
• Peg 2: $N$ (bottom), . . . 2, 1 (top)
Rules:
• Only one disk can be moved at a time.
• Only the top disk from any stack can be moved.
• A larger disk may not be placed on top of a smaller disk.
Find the sequence of moves to transform the initial configuration into the goal configuration.
Simulator. Our evaluation framework employs separate puzzle simulators for each puzzle to
ensure rigorous and consistent assessment of solutions obtained from LRMs. The Tower of Hanoi
simulator is designed as a stateful environment that tracks disk configurations across three pegs
and validates each proposed move against the puzzle’s fundamental constraints. The simulator
architecture follows a modular design pattern with clear separation between state management,
move validation, and solution verification. In this simulator, we have a puzzle class which tracks the
current disk configuration and enforces the puzzle’s fundamental constraints. We also have a method
to execute each move in the puzzle setup and perform four-layer validation: checking peg boundary
conditions (0-2), verifying source pegs contain disks, confirming the specified disk is topmost, and
enforcing the size ordering constraint that prevents larger disks from being placed on smaller ones.
Upon successful validation, the method executes the disk transfer and updates the game state. Then,
the complete solution validation is processed by sequentially processing move lists, and verifying
goal state achievement.
A.1.2
Checker Jumping
Problem Description. Checker Jumping is a one-dimensional constraint-satisfaction puzzle
designed to test sequential reasoning, planning, and rule understanding capabilities. The puzzle
consists of a linear arrangement of red checkers (’R’), blue checkers (’B’), and a single empty space
(’_’). In the standard configuration, N red checkers are positioned on the left side, followed by an
empty space in the middle, and N blue checkers on the right side, forming a linear board of length
2N + 1. The objective is to swap the positions of all red and blue checkers, effectively mirroring the
initial configuration, where red checkers end up on the right and blue checkers on the left. Movement
in this puzzle is governed by two fundamental rules: (1) Slide Movement: A checker can slide
forward into an adjacent empty space; and (2) Jump Movement: A checker can jump forward over
exactly one checker of the opposite color to land in an empty space. Therefore, checkers cannot
move backward toward their starting side—red checkers can only move rightward, and blue checkers
can only move leftward from the initial configuration. This puzzle presents cognitive challenges that
make it a great testbed for reasoning models. For example, models must demonstrate some aspect of
spatial reasoning (tracking checker positions and possible moves), constraint satisfaction (adhering
to movement rules during puzzle), lookahead planning (anticipating how current moves affect future
18possibilities towards goal), and state-space exploration (searching through possible move sequences
to find a valid solution path).
The difficulty of the Checker Jumping puzzle scales with the number of checkers: with N checkers of
each color, the minimum solution requires (N + 1)2 − 1 moves, creating a quadratic relationship
between problem size and solution complexity. In our evaluation framework, we mainly focus on
solution correctness rather than optimality, evaluating each move against the puzzle constraints and
confirming that the final state matches the goal configuration. This approach allows us to precisely
identify reasoning failures and constraint violations that might occur during the solution process.
Prompt Design. The system prompt begins with a clear problem statement describing the puzzle
setup and movement rules. It explicitly states the objective and provides a concrete example with a
small board configuration to illustrate how moves should be represented.
System Prompt - Checker Jumping
You are a helpful assistant. Solve this puzzle for me.
On a one-dimensional board, there are red checkers (’R’), blue checkers (’B’), and one empty
space (’_’). A checker can move by either:
1. Sliding forward into an adjacent empty space, or
2. Jumping over exactly one checker of the opposite color to land in an empty space.
The goal is to swap the positions of all red and blue checkers, effectively mirroring the initial
state.
Example: If the initial state is [’R’, ’_’, ’B’], the goal is to reach [’B’, ’_’, ’R’]. Your solution
should be a list of moves where each move is represented as [checker_color, position_from,
position_to]. For example:
moves = [[ ’R ’ , 0 , 1] , [ ’B ’ , 2 , 0] , [ ’R ’ , 1 , 2]]
This means: Move the red checker from position 0 to 1, then move the blue checker from
position 2 to 0, and so on.
Requirements:
• When exploring potential solutions in your thinking process, always include the corre-
sponding complete list of moves.
• The positions are 0-indexed (the leftmost position is 0).
• Ensure your final answer includes the complete list of moves for final solution in the
format: moves = [[checker_color, position_from, position_to], ...]
The user prompt presents the specific puzzle instance with the initial board configuration, and the
goal state.
19User Prompt Template for $N$ Checkers - Checker Jumping
I have a puzzle with 2$N$+1 positions, where $N$ red checkers (’R’) on left, $N$ blue checkers
(’B’) on right, and one empty space (’_’) in between are arranged in a line.
Initial board: R R ... R _ B B ... B
Goal board: B B ... B _ R R ... R
Rules:
• A checker can slide into an adjacent empty space.
• A checker can jump over exactly one checker of the opposite color to land in an empty
space.
• Checkers cannot move backwards (towards their starting side).
Find the minimum sequence of moves to transform the initial board into the goal board.
Simulator. Our evaluation framework employs a custom simulator for validating Checker Jumping
puzzle solutions. The simulator implements a comprehensive validation system that enforces all
puzzle constraints while tracking the state evolution throughout the solution path. The Checker
Jumping simulator is designed as a stateful environment that tracks the position of all checkers and
the empty space, validating each move of a given solution against the puzzle’s movement rules. The
simulator begins by validating that both the initial and goal states are well-formed, containing the
same number of red and blue checkers and exactly one empty space. Then, each move is executed
with a method that performs multi-layer validation: verifying position boundaries, confirming correct
checker color at source, ensuring target positions are empty, and validating move types as either
slides (distance=1) or jumps (distance=2). The simulator enforces directional constraints preventing
backward movement (red checkers move right, blue checkers move left) and validates jump moves
by confirming the presence of an opposite-colored checker in the middle position. Upon successful
validation, the method executes the checker transfer by updating positions and clearing the source.
Then, the complete move sequences are processed with final goal state verification.
A.1.3
River Crossing
Problem Description. River Crossing is a constraint satisfaction planning puzzle that tests multi-
agent coordination and constraint management. This puzzle is a generalization of classic problems
such as the Missionaries and Cannibals problem and the Bridge and Torch problem, which have been
widely studied in planning literature [45, 46]. The river crossing puzzle involves N actors (denoted by
a1 , a2 , ..., aN ) and their corresponding N agents (denoted by A1 , A2 , ..., AN ) who must cross a river us-
ing a boat. In the initial state, all 2N individuals are on the left bank of the river. The goal is to trans-
port everyone safely to the right bank. The puzzle operates under several key movement constraints:
(1) Boat Capacity Constraint: The boat can carry at most k individuals at a time, where k is typically
set to 2 for smaller puzzles (N ≤ 3) and 3 for larger puzzles (N ≤ 5); (2) Non-Empty Boat Constraint:
The boat cannot travel empty and must have at least one person aboard; (3) Safety Constraint: An
actor cannot be in the presence of another agent unless their own agent is also present, as agents must
protect their clients from competing agents. This safety constraint applies both on the banks and in
the boat. This puzzle requires complex planning and state tracking as participants must carefully coor-
dinate their crossings while maintaining safety constraints at all times. The solver must reason through
20different combinations of individuals who can safely travel together, determine who should return with
the boat after a crossing, and strategically plan a sequence that eventually brings everyone to the right
bank without violating any constraints. The complexity of this task can be controlled by adjusting the
number of actor-agent pairs and the boat capacity, creating a scalable challenge for reasoning models.
Prompt Design. The system prompt introduces the notation for representing actors and agents,
establishes the solution format as a list of boat moves, and provides a simple example to demonstrate
the format.
System Prompt - River Crossing
You are a helpful assistant. Solve this puzzle for me.
You can represent actors with a_1, a_2, ... and agents with A_1, A_2, ... . Your solution
must be a list of boat moves where each move indicates the people on the boat. For example, if
there were two actors and two agents, you should return:
moves =[[" A_2 " , " a_2 "] , [" A_2 "] , [" A_1 " , " A_2 "] , [" A_1 "] , [" A_1 " , " a_1
"]]
which indicates that in the first move, A_2 and a_2 row from left to right, and in the second
move, A_2 rows from right to left and so on.
Requirements:
• When exploring potential solutions in your thinking process, always include the corre-
sponding complete list of boat moves.
• The list shouldn’t have comments.
• Ensure your final answer also includes the complete list of moves for final solution.
The user prompt presents the specific puzzle instance with N actor-agent pairs, and the boat capacity
k, and the safety constraint that must be maintained throughout the solution.
User Prompt Template for $N$ Pairs - River Crossing
$N$ actors and their $N$ agents want to cross a river in a boat that is capable of holding
only $k$ people at a time, with the constraint that no actor can be in the presence
of another agent, including while riding the boat, unless their own agent is also
present, because each agent is worried their rivals will poach their client. Initially, all actors
and agents are on the left side of the river with the boat. How should they cross the river?
(Note: the boat cannot travel empty)
Simulator. Our evaluation framework employs a custom simulator for validating River Crossing
puzzle extracted solutions. The simulator tracks the state of all individuals (actors and agents) and
the boat position while enforcing all puzzle constraints. Each move is executed with multi-step
validation: checking boat capacity limits, verifying all passengers are on the boat’s current side,
and enforcing the critical safety constraint that actors cannot be in the presence of other agents
without their own agent present, both on the boat and on each bank after the move. The simulator
21manages dynamic boat positioning, automatically switching sides after each crossing, and validates
the complete state after each move to ensure no safety violations occur on either bank. Then, the
complete crossing sequences are verified that all 2N individuals successfully reach the right bank.
A.1.4
Blocks World
Problem Description. Blocks World is a classical planning puzzle that has been recently studied
for analyzing the planning capabilities of LLMs [37, 38]. The puzzle involves multiple stacks of blocks
(A, B, C, etc.) that must be rearranged from an initial configuration to a specified goal configuration.
Each block is uniquely identified by its letter, and the objective is to find the minimum sequence of
moves needed to transform the initial state into the goal state. The puzzle operates only under two
fundamental constraints: (1) Top Block Movement: Only the topmost block from any stack can be
moved; and (2) Valid Placement: A block can only be placed either on an empty position or on top
of another block. These constraints create planning problem where the order of operations becomes
critical, as some configurations may require temporary placement of blocks to access those beneath
them later. Blocks World serves as a great testbed for evaluating planning capabilities in reasoning
models because it requires forward thinking, and state tracking. Recent studies have examined this
puzzle in various configurations, including simplified settings with as few as 3 to 5 blocks, to evaluate
LLM performance on sequential planning tasks [37, 38]. Models must demonstrate the ability to
decompose complex state transformations into valid sequential moves, reason about dependencies
between blocks (e.g., unblocking lower blocks before accessing them), and efficiently plan paths to
the goal state without illegal moves.
The difficulty of this puzzle can be scaled by adjusting several parameters: the number of blocks, the
number of stacks, and the complexity of the initial and goal configurations. We primarily control
complexity through the block count N , while following clear structural patterns in the initial and
goal configurations. In our experimental design, the initial configuration consistently divides the
N blocks between two stacks in alphabetical order, with the third stack empty as workspace. The
goal configuration consolidates all blocks onto the first stack in a systematic interleaved pattern
that alternates between blocks from the two initial stacks, with specific positioning that requires
complete disassembly and reassembly of the existing stacks. For example, for N = 4, the initial
state has blocks divided between two stacks [["A", "B"], ["C", "D"], []] and the goal state
[["D", "B", "C", "A"], [], []] requires interleaving blocks from both stacks; and for N = 6,
the initial state [["A", "B", "C"], ["D", "E", "F"], []] must be transformed to [["F", "C",
"E", "B", "D", "A"], [], []], forming a complex alternating pattern. As N increases, the state
space grows factorially, and the minimum solution length increases approximately linearly with
N . For small values of N (2-7), the puzzles test basic planning; for medium values (8-20), they
require more complex reasoning with longer planning horizons; and for large values (N > 20), they
challenge the limits of sequential reasoning capabilities by requiring extensive temporary movements
and pattern recognition across lengthy solution paths.
Prompt Design. The system prompt introduces the fundamental rules of the Blocks World puzzle,
establishes the move representation format, and provides a simple example to demonstrate the
solution structure.
22System Prompt - Blocks World
You are a helpful assistant. Solve this puzzle for me.
In this puzzle, there are stacks of blocks, and the goal is to rearrange them into a target
configuration using a sequence of moves where:
• Only the topmost block from any stack can be moved.
• A block can be placed either on an empty position or on top of another block.
Example: With initial state [["A", "B"], ["C"], []] and goal state [["A"], ["B"],
["C"]], a solution might be:
moves = [[" C " , 1 , 2] , [" B " , 0 , 1]]
This means: Move block C from stack 1 to stack 2, then move block B from stack 0 to stack 1.
Requirements:
• When exploring potential solutions in your thinking process, always include the corre-
sponding complete list of moves.
• Ensure your final answer also includes the complete list of moves for final solution in the
format: moves = [[block, from stack, to stack], ...]
The user prompt presents the specific puzzle instance with the initial and goal configurations provided,
and explicitly reminds the model about the movement constraint.
User Prompt Template for $N$ Blocks - BlocksWorld
I have a puzzle with $N$ blocks.
Initial state:
Stack 0: $blocks_0$ (top)
Stack 1: $blocks_1$ (top)
...
Stack $m$: $blocks_m$ (top)
Goal state:
Stack 0: $goal_blocks_0$ (top)
Stack 1: $goal_blocks_1$ (top)
...
Stack $m$: $goal_blocks_m$ (top)
Find the minimum sequence of moves to transform the initial state into the goal state. Remember
that only the topmost block of each stack can be moved.
23Simulator. Our evaluation framework employs a custom simulator for validating Blocks World
puzzle extracted solutions. The simulator manages the state of all blocks across stacks while enforcing
the puzzle’s movement constraints. Each move is executed in the puzzle setup with three-layer
validation: verifying stack indices are within bounds, confirming the source stack contains blocks,
and ensuring the specified block is at the top of its stack (enforcing the top-block-only movement
rule). Upon successful validation, the block transfer is executed and the block is popped from the
source stack and appended to the destination stack. Finally, the complete solution sequences of block
movements are processed and verified that the resulting configuration matches the target goal state.
A.2
Implementation Details
Configurations Our experiments primarily utilized reasoning models and their non-thinking
counterparts to enable thorough analysis of the thinking process. We specifically selected Claude
3.7 Sonnet (thinking/non-thinking) and DeepSeek-R1/V3 due to their ability to provide access to
thinking traces, a critical requirement for our analysis. For experiments focused solely on final
accuracy metrics, we also included results from OpenAI’s o3-mini models, as they lack access to
thoughts. For Claude 3.7 Sonnet (thinking and non-thinking) models we used maximum generation
budget of 64,000 tokens, accessed through the API interface. Temperature is set to 1.0 for all API
rus (Claude-3.7-Sonnet and o3-mini runs). The experiments with DeepSeek-R1, DeepSeek-V3, and
DeepSeek-R1-Distill-Qern-32B are conducted on local servers with maximum generation length set to
64,000 and temperature set to 1.0. In all experiments, we generated 25 samples per puzzle instance
at each complexity level (N value) and reported performance averages across all samples.
Solution Extraction A custom extraction pipeline was developed to process model responses
and intermediate reasoning traces (thoughts). The pipeline consists of several key components. We
implemented a flexible regex-based extractors to identify potential solution attempts in both the
final response and thinking trace. The extraction process identify solution patterns using regular
expressions (both explicit “moves =” patterns and alternative bracket-based solutions). We process
and clean each extracted candidate solution by (i) Removing comments from the list (text following
"#" in any line), and (ii) Normalizing move formats to what suggested in context to ensure consistent
structure. Then, we validate solution format and structure to filter out invalid matches. During
the extraction, we also capture metadata of token position for each extracted solution. Notably, for
accurate position tracking within thinking traces, we employed the same tokenizer (cl100k_base)
as the corresponding model to count tokens across all experiments. Token positions were also
normalized with respect to thought length to enable cross-sample comparison. Finally, we make sure
that the recorded solutions within the thought trace are unique and duplicate solutions (identical
moves list) were filtered. In case of duplicate solutions, only the first solution is recorded for analysis.
Solution Evaluation After extraction, each solution candidate is passed to the corresponding
simulator of puzzle for fine-grained verification. The simulator takes a solution as list of moves and
evaluate that with respect to the puzzle (check App. A.1 for details of each puzzle simulator). Each
move in the compositional solution is executed sequentially according to previous moves and the
puzzle rules. Then, the final state obtained from all moves in the sequence is compared to the goal
state of puzzle to determine full solution correctness. For incorrect solutions, details of first failure
move and the type of failure is also collected during the move verification with puzzle simulator.
Execution of Prescribed Steps In addition to open-ended problem solving across different
puzzles, we also conducted focused experiments to test how providing the explicit solving algorithm
24guidance with prescribed steps would affect behavior of these reasoning models (Sec. 4.4).
We expected that finding and devising solution from scratch should require substantially more
computation for model (e.g., for search and verification) than just following a given algorithm’s
steps. However, results in Figures 8a and 8b show that reasoning models’ behavior does not change
that much and the collapse still occurs at roughly same points as before with this setting. This
finding strengthens evidence that the limitation is not just in problem-solving and solution strategy
discovery but also in consistent logical verification and step execution limitation throughout the
generated reasoning chains.
For example, models are provided with a complete recursive algorithm of solving Tower of Hanoi
puzzle as follows. This algorithm scratchpad was appended to the standard problem prompt to test
its impact on reasoning behavior.
Example of Prescribed Algorithm for Tower of Hanoi
Here is a pseudocode of recursive algorithm to solve the puzzle:
ALGORITHM Solve(n, source, target, auxiliary, moves)
// n = number of disks to move
// source = starting peg (0, 1, or 2)
// target = destination peg (0, 1, or 2)
// auxiliary = the unused peg (0, 1, or 2)
// moves = list to store the sequence of moves
IF n equals 1 THEN
// Get the top disk from source peg
disk = the top disk on the source peg
// Add the move to our list: [disk_id, source, target]
ADD [disk, source, target] to moves
RETURN
END IF
// Move n-1 disks from source to auxiliary peg
Solve(n-1, source, auxiliary, target, moves)
// Move the nth disk from source to target
disk = the top disk on the source peg
ADD [disk, source, target] to moves
// Move n-1 disks from auxiliary to target
Solve(n-1, auxiliary, target, source, moves)
END ALGORITHM
To solve the entire puzzle of moving n disks from peg 0 to peg 2:
1. Initialize an empty list ’moves’
2. Execute Solve(n, 0, 2, 1, moves)
3. The ’moves’ list will contain the complete solution
25Note: When executing this pseudocode, track which disk is currently on top of each peg. The
disk IDs in the moves list should correspond to the actual disk being moved.
You can use this algorithm as a scratchpad to help you solve the problem step by step.
A.3Details on Computational Complexity
A.3.1Compositional Depth Characterization
Compositional Depth (# of Moves)
Compositional depth is the number of sequential op-
Blocks World
erations (i.e., moves) required to reach a full solution.
60
Checker Jumping
Figure 9 demonstrates how this depth scales with
River Crossing
problem size (N ) across our four puzzle environments.
50
Tower of Hanoi
Each puzzle has a distinct growth pattern, reflecting
40
its underlying computational complexity. For exam-
ple, Tower of Hanoi shows exponential growth (2N −1),
30
and Checker Jumping displays quadratic scaling (
20
(N + 1)2 − 1). The River Crossing and Blocks World
puzzles show more moderate, near-linear growth with
10
N . These varying compositional depth profiles enable
us to evaluate how language reasoning models handle
0
1
2
3
4
5
6
different types of sequential reasoning challenges and
Problem Size (N)
if their accuracy is always correlated with the com-
positional depth required to solve the puzzle. More Figure 9: Compositional depth (number of
details regarding this analysis is provided in Figure 10 moves required) across different problem sizes
in App. A.4.
for our four puzzle environments.
A.3.2
Performance vs Compositional Depth
While intuition suggests a negative correlation between problem complexity and model accuracy, our
analysis reveals a more nuanced relationship between compositional depth and LRM performance.
Figure 10 demonstrates this across three state-of-the-art reasoning models (Claude-3.7-Sonnet w.
thinking, DeepSeek-R1, and o3-mini) on our puzzle suite. Within individual puzzle types, we observe
the expected negative correlation: as compositional depth increases, model accuracy consistently
decreases. However, across different puzzle types, this relation breaks. Models may struggle with
puzzles of lower compositional depth while succeeding on different puzzles with higher compositional
depth. . For instance, models achieve >50% accuracy on Tower of Hanoi instances requiring
approximately 102 moves, yet consistently fail on River Crossing puzzles with substantially lower
compositional depth (∼ 101 moves).
A.4
Extended Results and Analysis
Failure Analysis. Understanding where models fail within the compositional reasoning steps
provides insights beyond binary success metrics. Our accuracy evaluation requires perfect execution
of entire move sequences—a single incorrect move results in failure. To examine failure patterns
more granularly, we analyze the compositional depth at which models first make incorrect moves
across varying problem complexity levels.
26DeepSeek-R1
100
100
40
20
0
80
Accuracy (%)
60
60
40
20
100
101
102
103
Compositional Depth (# of Moves)
Tower Hanoi
0
o3-mini (high)
100
80
Accuracy (%)
Accuracy (%)
80
Claude-3.7-Sonnet (thinking)
60
40
20
100
101
102
103
0
Compositional Depth (# of Moves)
Checker Jumping
River Crossing
100
101
102
103
Compositional Depth (# of Moves)
Blocks World
Figure 10: Accuracy versus compositional depth (number of moves required) for three LRMs
(DeepSeek-R1, Claude-3.7-Sonnet with thinking, and o3-mini) across four puzzle environments.
Figure 11 shows the failure move ID versus problem complexity (N ) within the solution sequence.
The top row compares Claude-3.7-Sonnet with and without thinking capabilities, while the bottom
row compares DeepSeek-R1 (thinking) with DeepSeek-V3 (non-thinking). These comparisons
demonstrates how thinking mechanisms of LRMs influence failure patterns in compositional reasoning
tasks of puzzles. Several counterintuitive patterns emerge from our analysis. First, models exhibit
non-monotonic failure behavior with respect to problem complexity—instances where models fail
earlier in the solution sequence for higher N values despite requiring longer overall solutions. For
example, in Tower of Hanoi, models sometimes fail at below 50 moves for N = 15 but succeed through
more than 100 moves for N = 8, contradicting the expectation that effective algorithmic planning
and execution for the same puzzle should maintain consistent failure patterns relative to solution
progress. This suggests fundamental inconsistencies in how models (both LRMs and their non-
thinking standard LLM counterparts) apply learned solution strategies across different problem scales.
Also, we observe that in the high-complexity regimes where both model variants experience complete
accuracy collapse, e.g., Tower of Hanoi with N ≥ 15 and Blocks World with N ≥ 40, non-thinking
models occasionally sustain performance deeper into the solution sequence and are able to fail at later
moves than thinking-enabled variants. This is interesting as it shows that compositional reasoning
failures in LLMs are not simply due to insufficient context length or inference compute, but rather
reflect fundamental limitations in how models maintain algorithmic consistency across problem scales.
We also analyze the distributional characteristics of failure moves to understand the consistency and
reliability of model reasoning. Figure 12 presents the density distributions of failure move positions
aggregated across all problem complexities for each puzzle environment, comparing thinking and
non-thinking models within the same family. Based on the figure, thinking models (Claude-3.7-Sonnet
with thinking and DeepSeek-R1) consistently show higher mean failure positions across all puzzles,
as indicated by the dashed vertical lines showing mean of first failure in sequence of moves. However,
the distribution shape of thinking models mostly have higher variance in their failure patterns. This
suggests that while these models can reach deeper into solution sequences on average, their reasoning
processes are more instable and prone to inconsistent performance.
Reasoning Effort Dynamics. Figure 13 demonstrates the reasoning effort (measured by inference
thinking tokens) versus problem complexity across our puzzle environments. Green dots indicate
27Figure 11: The first failure move versus problem complexity (N ) comparison for thinking and
non-thinking models across puzzle environments. Top: Claude-3.7-Sonnet comparison; Bottom:
DeepSeek-R1 vs DeepSeek-V3.
correct solutions, red crosses show incorrect ones, and blue lines track average thinking token usage at
each complexity level (N ) across different puzzles and LRMs. We observe a consistent pattern across
all three reasoning models (DeepSeek-R1, Claude-3.7-Sonnet-thinking, o3-mini) where thinking token
usage, i.e. reasoning effort, initially scales with problem complexity but counterintuitively declines
after reaching a model-specific threshold. This suggests an interesting and fundamental scaling limit
in LRM thinking process for reasoning where beyond certain complexity thresholds, models not
only fail to solve problems but counterintuitively reduce their inference compute despite facing more
difficult problems and being well below the context and generation limits.
28Figure 12: Density distribution of first failure moves for thinking and non-thinking models across
puzzle environments. Top: Claude-3.7-Sonnet comparison; Bottom: DeepSeek-R1 vs DeepSeek-V3.
29Figure 13: Detailed results on reasoning effort (measured in inference thinking tokens) versus problem
complexity (N) for three LRMs (DeepSeek-R1, Claude-3.7-Sonnet with thinking, and o3-mini) across
four puzzle environments.
30

#====
LEO: Boosting Mixture of Vision Encoders for
Multimodal Large Language Models
Mozhgan Nasr Azadani* James Riddell Sean Sedwards
University of Waterloo
Krzysztof Czarnecki
arXiv:2501.06986v1 [cs.CV] 13 Jan 2025
https://github.com/Mozhgan91/LEO
Abstract
88.0
87.6
Enhanced visual understanding serves as a cornerstone for
multimodal large language models (MLLMs). Recent hy-
brid MLLMs incorporate a mixture of vision experts to ad-
dress the limitations of using a single vision encoder and ex-
cessively long visual tokens. Despite the progress of these
MLLMs, a research gap remains in effectively integrating
diverse vision encoders. This work explores fusion strate-
gies of visual tokens for hybrid MLLMs, leading to the de-
sign of LEO, a novel MLLM with a dual-branch vision en-
coder framework that incorporates a post-adaptation fu-
sion strategy and adaptive tiling: for each segmented tile
of the input images, LEO sequentially interleaves the vi-
sual tokens from its two vision encoders. Extensive eval-
uation across 13 vision-language benchmarks reveals that
LEO outperforms state-of-the-art open-source MLLMs and
hybrid MLLMs on the majority of tasks. Furthermore, we
show that LEO can be adapted to the specialized domain
of autonomous driving without altering the model architec-
ture or training recipe, achieving competitive performance
compared to existing baselines. The code and model will be
publicly available.
86.4
64.8
64.2
62.9
60.5
85.4
65.4
64.8
64.2
62.0
59.3
37.2
72.2
31.2 31.1 29.1
78.5
71.6
67.1
66.8
66.2
65.1
38.2
48.7
64.3
72.9
64.6 65.4
65.1
50.0
52.5
57.9
53.4
57.0
58.2
65.7
80.1
63.8
67.1
68.8
71.0
Figure 1. Comparison of the performance of LEO across diverse
vision-language tasks with recent approaches [3, 7, 11, 27, 33].
effectiveness is reduced in tasks requiring high visual de-
tail, such as complex optical character recognition (OCR)
and chart understanding. Enhancing visual understanding
is essential for minimizing hallucinations [39] and improv-
ing performance in tasks that require high-resolution.
To this aim, recent studies [11, 16, 19, 23, 25, 28]
have attempted to improve MLLMs by exploring methods
that enhance their visual understanding capabilities. Some
works [6, 7, 24, 28, 50] demonstrate that strengthening vi-
sion encoders, either by scaling up model parameters and
pretraining data to match those of LLMs or by employing
tile segmentation, can significantly improve detailed scene
understanding. However, these approaches often come with
increased computational burden, particularly when multiple
images are processed. This has prompted the development
of a new class of models that exploit multiple vision en-
coders, each pretrained for distinct vision tasks and input
resolutions, and integrated using a variety of fusion tech-
1. Introduction
Advancements in multimodal large language models
(MLLMs) [2, 12, 21, 29] have harnessed the strengths of
pre-trained large language models (LLMs) alongside pow-
erful vision encoders. These models are trained through
multiple stages on large-scale image-text datasets, which
effectively aligns visual tokens extracted from vision foun-
dation models, such as CLIP [38], with the latent space of
LLMs. This multi-stage alignment has enabled advance-
ments in tasks involving vision-language comprehension
and reasoning [2, 10, 28]. Nonetheless, due to limitations in
input resolution, arising from the constraints of current vi-
sion encoders and language model sequence lengths, their
* Corresponding author: mnasraza@uwaterloo.ca
1Post-adaptation Fusion: ours
Text tokens
Vision Encoder 1
Vision Encoder 2
Fusion Block
Fusion
Block
+
Projector
Text tokens
Fused visual tokens
1. Channel Concatenation
Text
2. Sequence Concatenation
Vision Encoder 1Projector1
Vision Encoder 2Projector 2
3. MR-Adapter
ViT Block
Fusion
Block
MR-Adapter
Features 1
Features
1
Features 2 2
Features
Features 1
Features 1
Features 2
Features
2
Conv Block
+
Fusion
Block
LLM
Text
LLM
Pre-adaptation Fusion: existing MLLMs
Fused
visual tokens
4. Cross-Attention
Cross-attention
…
Q
K&V
Features 1 Features 2
Figure 2. Top: Comparison between the fusion strategy of existing hybrid MLLMs and that of LEO. Bottom: The most common fu-
sion paradigms in the literature: (1) channel concatenation [40], (2) sequence concatenation [16], (3) MR-adapter [33], and (4) cross-
attention [23].
niques, including but not limited to sequence concatena-
tion [11, 16, 26], channel concatenation [31, 40], mixture-
of-resolution adaptation [33], and cross-attention [23], to
enhance their ability to process complex multimodal data.
Despite their success, these prior studies have primarily
focused on pre-adaptation fusion, leaving tile-level post-
adaptation fusion unexplored in effectively fusing visual to-
kens from diverse vision encoders.
In this work, we introduce LEO, a novel MLLM de-
signed to enhance the integration of multiple vision en-
coders for multimodal language tasks. Unlike existing hy-
brid models, LEO adopts a unique post-adaptation fusion
strategy combined with tile segmentation (see Fig. 2, top).
Specifically, each input image is divided into 448 × 448
tiles, which are processed independently by our dual-branch
vision encoder architecture. This allows each encoder to
exploit its specialized capabilities for optimal image pro-
cessing. Our method follows a standard ViT-projector-LLM
framework, using separate MLP projectors for each vision
encoder branch. To further optimize the visual token rep-
resentation, LEO employs pixel unshuffling to reduce the
number of tokens for both encoders. These visual tokens
are then sequentially interleaved and merged with text to-
kens before being fed into the LLM. By employing tile-level
post-adaptation fusion strategy for visual token combina-
tion, LEO not only outperforms models with single vision
encoders and those that rely on higher image resolutions,
but also demonstrates superior performance over existing
hybrid MLLMs with pre-adaptation fusion techniques. This
is demonstrated by our quantitative analysis, illustrated in
Fig. 1. Our contributions are summarized as follows:
coders through a tile-level post-adaptation fusion method-
ology to enhance the visual understanding capabilities of
MLLMs.
• We conduct extensive experiments across multiple gen-
eral benchmarks, thus demonstrating the superior perfor-
mance of LEO on the majority of tasks when compared
to leading open-source single-vision-encoder and hybrid
models.
• We adapt LEO to the specialized domain of autonomous
driving (AD) without altering its architecture, collecting
extensive domain-specific pretraining data, or customiz-
ing the training recipe. To the best of our knowledge, this
is the first exploration of hybrid MLLMs for AD.
2. Related work
2.1. Multimodal LLMs
With the rapid advancement of large language models [1,
8, 43, 46], there has been considerable interest in multi-
modal models that enhance understanding and reasoning
capabilities. BLIP2 [21] introduces the Q-Former, designed
to efficiently bridge the modality gap between images and
text during pretraining. Flamingo [2] is capable of pro-
cessing sequences that combine visual and textual data in
any arbitrary order, a crucial feature that enables them to
perform in-context few-shot learning effectively. LLaMA-
Adapter v2 [12] activates a larger number of learnable pa-
rameters in the LLM through an early fusion strategy for vi-
sion and text tokens, where vision tokens are fed only into
the initial layers of the language model, enabling more ef-
ficient integration of visual information. LLaVA [27, 29]
delivers impressive results by employing a straightforward
projector, such as a linear layer or MLP, between the visual
• We propose LEO, a powerful hybrid multimodal model
that strategically fuses the capabilities of two visual en-
2and language components, and developing a streamlined
instruction-following data process powered by GPT. Most
of these methods, however, maintain low input resolutions
due to the constraints of pretrained vision encoders, such
as that of CLIP [38], and the sequence length restrictions
of large language models. All these approaches exploit a
single vision encoder in their architecture.
LLM
Tokenizer
Is it possible for you
to accelerate in this
situation, and if so,
why?
2.2. Vision encoders for MLLMs
To address the constraints of lower input resolutions, recent
MLLMs have concentrated on enhancing their vision en-
coder module. From a vision-focused standpoint, these ap-
proaches can be broadly categorized into three main strate-
gies: (1) robust vision encoders: designing stronger vision
encoders [7, 50] that effectively capture complex visual fea-
tures by incorporating larger models sizes and better train-
ing strategy, (2) tile segmentation: handling high-resolution
inputs by dividing images into smaller, lower-resolution
tiles [6, 28, 39], and (3) hybrid vision encoders: utilizing
a vision backbone that incorporates multiple vision experts.
Our research is closely tied to the third group of ap-
proaches [23, 31, 45] that develop a multi-branch vision
encoder framework to enhance perception capabilities of
MLLMs. Some models [23, 33] suggest merging high-
resolution visual details with low-resolution tokens to en-
hance visual representation. LLaVA-HR [33] proposes a
dual-pathway vision model that integrates features from
high-resolution convolutional blocks with those from low-
resolution ViT blocks. These pretrained vision experts
might nevertheless lack key capabilities, such as text under-
standing and object localization. Several studies have incor-
porated multiple vision experts trained on diverse tasks to
broaden the functionality of their encoders. Brave [16] and
Mousi [11] perform sequence append, combining vision to-
kens from multiple experts into a single, extended sequence.
Tong et al. [45] identify distinct differences in the visual
features captured by CLIP [38] and DINOv2 [37], leading
to the design of an image-level mixture-of-features strategy.
Some models combine vision tokens through channel con-
catenation to preserve the original sequence length, such as
DeepSeek-VL [31] and Eagle [40] models, or utilize ad-
vanced fusion and routing techniques [19, 51] to leverage
the strengths of various encoders. These methods perform
pre-adaptation fusion of vision tokens, which are then pro-
cessed by a single projector module (either an MLP or Q-
Former), as shown in Fig. 2.
Distinguished from previous studies, this work proposes
a novel hybrid approach that integrates a tile-level post-
adaptation fusion strategy with dynamic high-resolution in-
puts achieved through tile segmentation. In our frame-
work, each vision expert is equipped with its own projector,
and the vision tokens are sequentially interleaved following
vision-text alignment by the projectors at the tile level.
Fusion Block
Projector 1Projector 2
Pixel unshufflePixel unshuffle
Intern-ViTSAM-L
Dynamic high resolution
Figure 3. The architecture of our model. LEO adapts a dual-vision
MLLM architecture through tile-level post-adaptation fusion of vi-
sual tokens. Pixel unshuffle is adapted to decrease the visual token
quantity.
3. Method
3.1. Overview
Figure 3 illustrates the overall architecture of our proposed
multimodal large language model, LEO, which is conceptu-
ally straightforward: First, the high-resolution input images
are divided into tiles. These tiles are then processed by two
different vision experts, each exploiting its specialized pre-
training to provide distinct feature representations. Next,
pixel unshuffling is applied to the extracted visual embed-
dings to reduce the number of visual tokens for each vision
encoder. Vision-text alignment is conducted using two dis-
tinct MLP projectors, and a tile-level post-adaptation fusion
strategy is employed to sequentially interleave tile-level vi-
sual tokens. Finally, these visual tokens are combined with
text tokens and processed by the LLM for comprehensive
visual-language understanding and reasoning. The follow-
ing section gives more details of the architectural blocks
identified in Fig. 3.
3.2. LEO
Dynamic high resolution. The image processing begins
with a dynamic high-resolution approach, where an input
image is segmented into multiple tiles alongside a thumb-
nail of the original image, helping the model to capture the
global context. We utilize a dynamic resolution input strat-
egy similar to that in InternVL [6, 7]. Each input image is
resized to a closest available aspect ratio that is dividable
into square patches of size 448 × 448, with up to six tiles
for the 3:2 aspect ratio shown in Fig. 4. To capture global
context, the input image is also scaled to match the patch
size and added to the set of patches to be processed by the
3text tokens and feed them into the LLM for auto-regressive
generation. For our language model, we use InternLM2-
7B-Chat [4], identical to the LLM in the base model [7], to
ensure a fair comparison. To optimize memory and compu-
tational efficiency, the context length of the LLM is set to a
maximum of 8196 tokens, ensuring balanced performance
across various multimodal tasks. Given the input context
length constraint of the LLM, we limit each input image
to six segmented tiles, which enables efficient handling of
multiple images.
Figure 4. Tile Segmentation: Each input image is divided into
multiple tiles to capture localized details, while a resized version
maintains global context. The tiles are shown after preprocessing
with the SAM [18] preprocessor.
3.3. Training process.
Our training process consists of two main phases. In the first
phase, we initialize the vision encoders and the language
model from the base models [4, 7, 18], while the projector
layers for SAM are randomly initialized. This approach is
adopted because InternViT is already pretrained on vision-
language alignment and SAM is pretrained specifically for
segmentation. To avoid any potential representation incon-
sistencies, we focus on training the layers of the second pro-
jector. In the second phase, the vision encoders are kept
frozen, and we perform full fine-tuning of two projectors
and the language model. In Table 5, we present the results
of an ablation study that shows the effect of keeping the vi-
sion encoders frozen during the training phases.
vision encoders. This approach, when applied dynamically
to two input images, results in a total of 14 unique patches
for training. Figure 4 illustrates this tiling process with an
example driving scene image, where the tiles are shown af-
ter normalization by the SAM’s preprocessor [18].
Pixel unshuffling. Once the input images are segmented
into tiles, they are simultaneously fed to two distinct vi-
sion encoders. We select InternViT-300M-448px [7] as the
first vision encoder and SAM-L [18] as the second. For
each encoder, we apply pixel unshuffling [41], which re-
arranges the spatial layout of pixels to reduce the num-
ber of visual tokens while preserving important visual fea-
tures. Given an input tensor of shape [b, c, w, h] and a down-
scaling factor of r, the output tensor will have the shape
[b, c ∗ (r2 ), w/r, h/r], where b represents the number of
tiles, c is channel, and w and h denote width and hight.
Specifically, for each segmented tile, we apply downscaling
factors of 2 and 4 for our first and second vision encoders,
respectively, reducing the number of visual tokens to 256
per tile and encoder, resulting in a total of 512 visual tokens
per tile.
Visual token fusion. Existing hybrid multimodal mod-
els [16, 23, 33, 40] typically use a pre-adaptation fusion
strategy to combine visual tokens from two or more vi-
sual encoders, applying one of the common fusion methods
shown in Fig. 2 (bottom) prior to the vision-text alignment
process. In these approaches, all visual encoders share the
same projector module. In contrast, LEO employs an alter-
native fusion approach in which each vision encoder main-
tains its own dedicated projector module, allowing for in-
dependent processing of visual tokens before they are com-
bined. We find this to be a more flexible and effective fusion
strategy. We use a two-layer MLP for the projector archi-
tecture to ensure simplicity and efficiency. To streamline
processing, we set the output dimension of the projector to
match that of our LLM. In the fusion block, we sequentially
interleave the visual tokens from InternViT and SAM-L for
each segmented tile, while preserving their original order.
Language model. We combine the extracted visual and
3.4. Adaption to autonomous driving
Although numerous studies have successfully applied
MLLMs to autonomous driving [5, 34, 44, 47], a straight-
forward approach that avoids extensive modifications to
model architecture, training processes, or heavy data collec-
tion has yet to be fully explored. In this work, we investigate
the potential of applying LEO to the autonomous driving
domain without altering its architecture or training recipe,
aiming to offer insights into streamlined transfer learning
and facilitate MLLM adaptation to specialized domains.
Instruction tuning plays a crucial role in helping models
learn to follow user prompts, utilizing training data in visual
question answering and conversational formats. For this do-
main, we design tasks in a VQA format, with each frame
represented as: <img> <IMG-CONTEXT> </img>. At
the prompt level, the temporal aspect of video frames is
managed by treating sequential frames as multiple image
inputs. A sample prompt is formulated as:“<image1> ...
<image N> Is it safe to enter the intersection at this time?”.
The images in this setting are of high-resolution, each mea-
suring 2048 × 1280 and segmented into six patches of size
448 × 448.
4. Experiments
We first describe the evaluation setting, outlining the imple-
mentation details of our model. We then present a compar-
4Model
ChartQA DocVQA VQAT GQA VQAv2 VizWiz MMB MMMU POPE AI2D SEED SQA MMVet
Instruct-BLIP [9]
InternVL [7]
VILA [25]
QwenVL [3]
QwenVL-Chat [3]
LLaVA.1.5 [27]
LLaVA-Next [28]-
-
-
65.7
66.3
-
--
-
-
65.1
62.5
-
-50.1
57.0
64.4
63.8
61.5
58.2
-49.2
62.9
62.5
59.3
57.5
62.0
--
79.3
79.9
79.5
78.2
78.5
-45.5
52.5
57.8
-
-
50.0
-36
64.6
68.9
38.2
-
64.3
67.4-
-
-
-
-
-
35.8-
86.4
85.5
-
-
-
86.5-
-
-
62.3
57.7
-
--
65.4
61.1
64.8
-
-
70.260.5
66.2
68.2
67.1
68.2
66.8
-26.2
31.2
34.9
-
-
31.1
-
LEO (ours)71.080.168.864.878.357.972.936.488.069.672.278.537.2
Table 1. Comparison with leading MLLMs across 13 benchmarks. All models use a 7B language model. Bolded values indicate the best
performance. Some benchmark names are abbreviated due to space constraints: VQAT : TextVQA [42], SQA: ScienceQA [32], MMB:
MMBench [30], and SEED: SEED-Bench [20].
ModelFusion
PT
Brave-X5 [16]
LLaVA-HR [33]
Mini-Gemini [23]
Mousi [11]Pre-A
Pre-A
Pre-A
Pre-A
LEO (ours)Post-A 595 K
SFT VQAT GQA VQAv2 VizWiz MMB MMMUv MMMUt POPE SEED SQA MMVet
100 M
-
558 K 1.2 M
1.2 M 1.5 M
1.2 M 1.6 M
1M
-
67.1
65.2
53.452.7
64.2
64.5
60.582.5
81.9
-
75.454.2
48.7
-
--
-
69.3
65.4-
-
36.1
--
-
32.8
-87.6
87.6
-
85.4-
64.2
-
62.0-
65.1
71.1
71.6-
31.2
40.8
29.1
68.864.878.357.972.936.433.588.072.278.537.2
Table 2. Results on 11 evaluation benchmarks are compared with leading hybrid MLLMs. All models use a 7B language model. The
best values are shown in bold. X5 denotes a mixture of 5 vision encoders. The following names are shortened due to space constraints:
Pre-A: pre-adaptation, Post-A: post-adaptation, PT: pretraining data, SFT: supervised finetuning data, VQAT : TextVQA [42], SQA: Sci-
enceQA [32], MMB: MMBench [30], and SEED: SEED-Bench [20].
ative analysis of LEO against leading open-source MLLMs
and hybrid models, across diverse vision-language tasks.
each) using DeepSpeed’s Zero2 strategy, allowing training
to complete within approximately 72 hours.
Training Datasets. In the first stage of training, we use
the LLaVA-595k dataset [28], which comprises 595k sam-
ples. In the supervised finetuning stage, we employ the
same supervised fine tuning stage dataset as InternVL [7],
incorporating a total of approximately one million visual in-
struction tuning samples, all of which are fully open-source.
4.1. Setting
Implementation details. Our model architecture is built
upon InternVL [7], following a standard MLLM design
of ViT-projector-LLM. The projector aligns the visual fea-
tures by mapping them into the language embedding space.
LEO uses InternLM2-7B-Chat [4] as the large language
model, with pretrained InternViT-300M-448px [7] and
SAM-L [18] as vision encoders, and an adaptive tiling strat-
egy. Following the base model [7], we divide each input
image into 448 × 448 tiles, where the number of tiles varies
based on the aspect ratio of the image. Our training pro-
cedure consists of two stages. In the first stage, we freeze
both vision encoders and focus on optimizing the projector
module to ensure effective training. In the second stage, we
perform supervised instruction tuning, unfreezing the pro-
jector modules along with the LLM. Both stages employ
a context length of 8196, and training is conducted for a
single epoch. We optimized the model using the AdamW
optimizer with a cosine learning rate schedule. During the
the second stage, we set the learning rate to 4 × 10−5 with a
weight decay of 0.01. In the alignment stage, we increased
the learning rate to 4 × 10−4 , maintaining the same weight
decay. Training was conducted on 8 A100 GPUs (80 GB
4.2. Main Results on general benchmarks
Comparison with leading MLLMs. In this section, we
comprehensively evaluate our model’s visual understand-
ing and reasoning abilities in comparison to previous lead-
ing MLLMs, across 13 vision-language benchmarks. These
benchmarks are organized into three task categories: (1)
OCR and chart understanding, including DocVQA [36],
TextVQA [42], ChartQA [35], and AI2D [17]; (2) gen-
eral visual question answering, including VQAv2 [13],
GQA [15], and VizWiz [14]; and (3) general multimodal
benchmarks, such as MMMU [49], MMBench [30], SEED-
Bench [20], POPE [22], MMVet [48], and ScienceQA [32].
In Table 1, we see that LEO achieves state-of-the-art re-
sults in 12 out of 13 benchmarks. In the OCR and chart
understanding category, LEO consistently surpasses lead-
ing models across all four datasets, by virtue of its dual-
branch vision encoders. In the multimodal benchmark cat-
5egory, LEO demonstrates superior performance across all
six benchmarks, highlighting its broad knowledge and ad-
vanced reasoning abilities. Additionally, the results in Ta-
ble 1 show that LEO excels in more demanding bench-
marks that necessitate college-level knowledge, such as
MMMU [49], which focuses on complex problems from
various domains, including science, business, tech and en-
gineering, as well as health and medicine. Notably, com-
pared to InternVL [7], which uses the same LLM and vision
encoder as our model, LEO achieves superior performance
across 8 out of 9 benchmarks, demonstrating the benefits of
dual-branch vision encoders for vision-language tasks. This
approach mitigates the inherent biases of individual vision
encoders, providing a robust framework for the mixture of
encoders.
Comparison with leading Hybrid MLLMs. We com-
pare the performance of LEO with recent hybrid approaches
across 11 benchmarks. In Table 2, we see that LEO demon-
strates strong performance on the majority of benchmarks.
Our model is trained on the least amount of data in both pre-
training and SFT stages, yet it outperforms models trained
on larger datasets, such as Mousi [11], highlighting the gen-
eralization capability of our model. Compared to mod-
els with more complex fusion strategies, such as LLaVA-
HR [33] and Mini-Gemini [23], our model excels across
most benchmarks, especially on multimodal benchmarks
like MMBench, SEED, and ScienceQA. Notably, compared
to Brave [16], which combines five distinct vision encoders
through pre-adaptation fusion and sequence concatenation,
LEO achieves competitive performance on most tasks. This
result underscores that post-adaptation fusion of visual to-
kens from only two vision experts can be as effective as
pre-adaptation fusion of visual tokens from a larger set of
vision experts.
Comparison with Eagle. We conduct a comparison
with Eagle [40], a concurrent work that integrates vision en-
coders through pre-adaptation fusion and channel concate-
nation. Table 3 shows that LEO outperforms Eagle-X2 [40],
which combines two vision experts, on 7 out of 9 bench-
marks, particularly excelling in the OCR and general VQA
categories. Notably, LEO also surpasses Eagle-X4 [40],
which uses four vision encoders, on 5 out of 7 benchmarks,
with an identical score for GQA and a near-dentical score on
POPE. It is worth mentioning that these results are achieved
despite LEO being trained with less SFT data, highlighting
the robustness and reasoning capability of the enhanced fu-
sion design in LEO.
Eagle-X4 [40]Eagle-X2 [40]LEO (ours)
Pre-adapt.
1024Pre-adapt.
1024Post-adapt.
512
PT
SFT595 K
1.8 M595 K
1.8 M595 K
1M
ChartQA
DocVQA
VizWiz
GQA67.5
-
50.8
64.867.0
77.7
48
63.271.0
80.1
57.9
64.8
MMMU
SEED
MMBench
POPE
ScienceQA-
73.4
67.8
88.4
70.436.0
73.5
-
88.3
70.736.4
72.2
72.9
88.0
78.5
Fusion
#-Tokens
Table 3. Results compared to a concurrent approach [40], which
combines vision encoders through pre-adaptation fusion and chan-
nel concatenation. Here, XN denotes a mixture of N vision en-
coders and #-Tokens denotes number of visual tokens.
Model
N Lingo-J ↑ BLUE ↑ METEOR ↑ CIDEr ↑
BLIP 2 [21]
1
LLaVA.1.5 [27] 5
5
InternVL [7]52.20
51.00
58.0013.00
10.62
13.5317.40
29.44
34.2760.10
48.18
67.17
LingoQA [34]
LingoQA [34]3
559.80
60.8014.61
15.0018.44
18.5662.61
65.62
LEO (ours)261.0014.9135.4469.72
Table 4. Results on the LingoQA benchmark [34]. All models are
fine-tuned, where N denotes the number of frames used during
training. Lingo-J represents the Lingo-Judge metric. Leo demon-
strates competitive performance without requiring tailored model
architecture for the autonomous driving domain.
cludes data covering nine distinct task types, such as ac-
tion, justification, localization, and anticipation, providing
a thorough representation of scenarios encountered in au-
tonomous driving.
Data format. In LingoQA, images are captured from a
front-view camera with sequences of five frames. Due to
computational limitations, we use only two frames during
training. For this evaluation, we maintain the same pretrain-
ing data as in general training and use the LingoQA Scenery
and Action training dataset [34] for the second stage. The
training process remains as described in Section 4.1. Ad-
ditionally, we reformat the LingoQA data into the standard
conversational format described in Section 3.4.
Results. We evaluate our model on the LingoQA val-
idation set [34], with results presented in Table 4. Our
model demonstrates competitive performance against the
closed-source LingoQA baseline [34], which is pretrained
4.3. Results in autonomous driving domain
Settings. We use the LingoQA benchmark [34] to eval-
uate the performance of LEO in the autonomous driving
domain. LingoQA contains over 400K samples and cov-
ers various aspects of the driving process. This dataset in-
6InternViT-300MSAM-ViT-LargeFreezeVQATGQAVizWizMMBPOPESEEDSQAMMVet
✓
×
×
✓
✓×
✓
✓
✓
✓×
×
✓
×
✓57.0
45.2
49.5
67.2
68.862.9
56.4
58.2
63.1
64.852.5
47.5
50.6
55.7
57.964.6
44.7
48.3
71.0
72.986.4
84.2
85.4
87.6
88.065.4
51.3
54.7
69.6
72.266.2
64.0
65.2
75.8
78.531.2
18.2
19.8
35.0
37.2
Table 5. Ablation study on various training settings.
BenchmarkSequence Concat.Channel Concat.
VQAT
GQA
VizWiz
MMB
POPE
SEED
SQA
MMVet68.8
64.8
57.9
72.9
88.0
72.2
78.5
37.267.3
62.8
54.3
70.9
87.6
72.0
78.4
35.7
scale pretraining, although when combined with InternViT,
SAM enhances performance on this type of task. Finally,
regardless of whether the SAM vision encoder is frozen, a
hybrid MLLM consistently outperforms models with a sin-
gle vision encoder.
Effect of fusion method. Sequence and channel con-
catenation are two primary approaches for fusing visual
tokens in hybrid MLLMs. To investigate the impact of
these fusion strategies on the performance of LEO, we
conduct an experiment, with results presented in Table 6.
Our findings reveal that sequence concatenation consis-
tently outperforms channel concatenation across all bench-
marks, highlighting its effectiveness in enhancing model
performance. It is worth noting that these results are spe-
cific to post-adaptation fusion. For a broader compari-
son, refer to the results presented in Table 2 and Table 3,
where our model performance is compared with models em-
ploying pre-adaptation fusion through four different meth-
ods: Brave [16] and Mousi [11] use sequence concate-
nation, LLaVA-HR [33] employs an MR-adaptor, Mini-
Gemini [23] uses cross-attention, and Eagle [40] utilizes
channel concatenation.
Effect of tiling. To investigate the effect of tiling, we
train LEO with and without tiling using two frames. This
yields 14 tiles with tiling. The performances of these mod-
els applied to the LingoQA benchmark [34] are given in Ta-
ble 7. We see that tiling improves performance across
all four metrics; for instance, it enhances Lingo-Judge by
3.4%. These results confirm that incorporating dynamic
high-resolution inputs can enhance the model capacity for
understanding driving scenes.
Table 6. Comparison of fusion methods in LEO.
Model
Tiling
No tiling
#-Frame Lingo-J ↑ BLUE ↑ METEOR ↑ CIDEr ↑
2
2
61.00
59.02
14.91
13.78
35.44
34.32
69.72
65.12
Table 7. Ablation study on tiling evaluated on the LingoQA bench-
mark [34]
on over 22M data samples, significantly outperforming it
on the METEOR and CIDEr metrics. Without modifying
its architecture or training recipe, LEO also surpasses all
existing open-source baselines across all four metrics. No-
tably, LEO achieves higher scores than the base model [7],
highlighting the effectiveness of its dual-branch design.
4.4. Ablation studies
Comparison with different training settings. To more ef-
fectively analyze the impact of training strategies for vision
encoders in multimodal large language models, we con-
duct an ablation study on the vision encoder modules in
our model (SAM-ViT-Large [18] and InternViT-300M [7]).
This also provides insights into the contributions of each
encoder to the model effectiveness. Each vision encoder
processes 256 visual tokens. Results in Table 5 highlight
three key findings. First, keeping the vision encoders frozen
during training improves evaluation scores; for instance,
unfreezing SAM reduces SEED performance by 6.62%.
Second, InternViT alone performs better than SAM alone
across all benchmarks, with SAM struggling on tasks like
text recognition. This is likely due to InternViT’s large-
4.5. Visualization
To highlight the visual understanding capabilities of LEO,
we conduct a qualitative analysis as shown in Fig. 5. Our
model is applied to a variety of vision-language tasks, in-
cluding complex reasoning, detailed counting, OCR, spa-
tial and mathematical reasoning, accounting analysis, and
multi-image and multi-frame reasoning. With an efficient
tile-level post-adaptation fusion strategy, LEO exhibits im-
pressive performance across these challenging tasks. For
example, our model can perform attribute-based counting,
such as identifying the absence of parked cars while there
7Reasoning:
Complex counting:
Is it summertime in the scene? Answer it and give the rationale.
LEO: No. Pumpkin patches are typically associated with autumn, as they are
harvested during this season. The presence of pumpkins in the scene suggest
that the image was taken during the fall season, not in the summer.
OCR:
How many parked cars can you spot?
LEO: I can’t see any parked cars.
Accounting and math:
What is located to the
right of the shampoo?What is the first website name
on the page?How many grams are in the
white chocolate?
LEO: Hair conditionerLEO: london-
underground.blogspot.comLEO: 70
Baxter Company has a relevant range of production between 15,000
and 30,000 units. The following cost data represents average variable
costs per unit for 25,000 units of production. If 15,000 units are
produced, what are the per unit manufacturing overhead costs incurred?
A. $10. B. $11. C. $12. D. $13.
What option is the
equation of the graph?
A. Y = cos (2!x)
B. Y = sin (2!x)
C. Y = cos (!x)
D. Y = sin (!x)
LEO: A
LEO: C
Multi-image reasoning:
What does the o in ola stand for?Tell me the difference between image1 and image 2.
LEO: OntarioLEO: The difference between the two images is the dog’s head position. In
image 1, the dog head is turned to the left, while in image 2, the dog is facing
directly to the camera.
Multi-frame reasoning:
What is the current action and its justification? Answer in
the form "action, justification".How many cars are driving in your direction?
LEO: I am stopping to let the pedestrian safely cross on
the zebra or pedestrian crossing ahead.LEO: There are no cars driving in my direction.
Figure 5. Qualitative results of LEO’s enhanced visual understanding on various vision-language tasks. Some images are taken from the
following benchamrks: MMVet [48], MMMU [49], TextVQA [42], and LingoQA [34]
5. Conclusion
are several moving vehicles in the driving scene. Beyond
simple recognition, LEO demonstrates spatial awareness,
enabling it to answer OCR-related questions like,“What is
located to the right of the shampoo?”. In multi-image rea-
soning, it accurately identifies detail differences between
images, such as the dog’s head being in different positions.
LEO also demonstrates strong capabilities in multi-frame
reasoning in the autonomous driving domain, including rec-
ognizing safe actions in dynamic scenes, such as stopping
to allow a pedestrian to cross. Additionally, LEO excels in
OCR tasks, effectively interpreting dense text, and handles
complex mathematical and accounting problems, showcas-
ing its strong reasoning abilities.
In this work, we have introduced LEO, a powerful frame-
work for multimodal large language models, whose core
lies in a strategic design for hybrid multimodal models
that enhances performance through a tailored combination
of post-adaptation fusion and tile segmentation. We have
also demonstrated that LEO can be easily extended to the
specialized domain of autonomous driving without the
need for extensive domain-specific adjustments. Com-
prehensive experiments on various zero-shot benchmarks
demonstrate LEO’s effectiveness, which surpasses pre-
vious state-of-the-art models on the majority of tasks.
We hope LEO serves as a foundation for advancing
hybrid multimodal models and provides straightforward
inspiration for adapting MLLMs to specialized domains.
4.6. Limitation
References
The processing capacity of our model for input images is
limited to a maximum of six patches (i.e., excluding the
global context) due to the constraints of the language model
context length and available computational resources. This
restriction prevents support for higher-resolution images or
a larger number of multi-image inputs.
[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ah-
mad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida,
Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al.
Gpt-4 technical report. arXiv preprint arXiv:2303.08774,
2023. 2
8angyu Yue, et al. Llama-adapter v2: Parameter-efficient vi-
sual instruction model. arXiv preprint arXiv:2304.15010,
2023. 1, 2
[13] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Ba-
tra, and Devi Parikh. Making the v in vqa matter: Elevating
the role of image understanding in visual question answer-
ing. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pages 6904–6913, 2017. 5
[14] Danna Gurari, Qing Li, Abigale J Stangl, Anhong Guo, Chi
Lin, Kristen Grauman, Jiebo Luo, and Jeffrey P Bigham.
Vizwiz grand challenge: Answering visual questions from
blind people. In Proceedings of the IEEE conference on
computer vision and pattern recognition, pages 3608–3617,
2018. 5
[15] Drew A Hudson and Christopher D Manning. Gqa: A new
dataset for real-world visual reasoning and compositional
question answering. In Proceedings of the IEEE/CVF con-
ference on computer vision and pattern recognition, pages
6700–6709, 2019. 5
[16] Oğuzhan Fatih Kar, Alessio Tonioni, Petra Poklukar, Achin
Kulshrestha, Amir Zamir, and Federico Tombari. Brave:
Broadening the visual encoding of vision-language models.
arXiv preprint arXiv:2404.07204, 2024. 1, 2, 3, 4, 5, 6, 7
[17] Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon
Seo, Hannaneh Hajishirzi, and Ali Farhadi. A diagram is
worth a dozen images. In Computer Vision–ECCV 2016:
14th European Conference, Amsterdam, The Netherlands,
October 11–14, 2016, Proceedings, Part IV 14, pages 235–
251. Springer, 2016. 5
[18] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,
Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-
head, Alexander C Berg, Wan-Yen Lo, et al. Segment any-
thing. In Proceedings of the IEEE/CVF International Con-
ference on Computer Vision, pages 4015–4026, 2023. 4, 5,
7
[19] Byung-Kwan Lee, Beomchan Park, Chae Won Kim, and
Yong Man Ro.
Moai: Mixture of all intelligence
for large language and vision models. arXiv preprint
arXiv:2403.07508, 2024. 1, 3
[20] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yix-
iao Ge, and Ying Shan. Seed-bench: Benchmarking mul-
timodal llms with generative comprehension. arXiv preprint
arXiv:2307.16125, 2023. 5
[21] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
Blip-2: Bootstrapping language-image pre-training with
frozen image encoders and large language models. In In-
ternational conference on machine learning, pages 19730–
19742. PMLR, 2023. 1, 2, 6
[22] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin
Zhao, and Ji-Rong Wen. Evaluating object hallucina-
tion in large vision-language models.
arXiv preprint
arXiv:2305.10355, 2023. 5
[23] Yanwei Li, Yuechen Zhang, Chengyao Wang, Zhisheng
Zhong, Yixin Chen, Ruihang Chu, Shaoteng Liu, and Jiaya
Jia. Mini-gemini: Mining the potential of multi-modality
vision language models. arXiv preprint arXiv:2403.18814,
2024. 1, 2, 3, 4, 5, 6, 7
[2] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine
Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Men-
sch, Katherine Millican, Malcolm Reynolds, et al. Flamingo:
a visual language model for few-shot learning. Advances
in neural information processing systems, 35:23716–23736,
2022. 1, 2
[3] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan
Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren
Zhou. Qwen-vl: A frontier large vision-language model with
versatile abilities. arXiv preprint arXiv:2308.12966, 2023. 1,
5
[4] Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu
Chen, Xin Chen, Xun Chen, Zehui Chen, Zhi Chen, Pei
Chu, et al. Internlm2 technical report. arXiv preprint
arXiv:2403.17297, 2024. 4, 5
[5] Xu Cao, Tong Zhou, Yunsheng Ma, Wenqian Ye, Can
Cui, Kun Tang, Zhipeng Cao, Kaizhao Liang, Ziran Wang,
James M Rehg, et al. Maplm: A real-world large-scale
vision-language benchmark for map and traffic scene un-
derstanding. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition, pages 21819–
21830, 2024. 4
[6] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhang-
wei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng
Luo, Zheng Ma, et al. How far are we to gpt-4v? closing
the gap to commercial multimodal models with open-source
suites. arXiv preprint arXiv:2404.16821, 2024. 1, 3
[7] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen,
Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu,
Lewei Lu, et al. Internvl: Scaling up vision foundation mod-
els and aligning for generic visual-linguistic tasks. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition, pages 24185–24198, 2024. 1, 3, 4,
5, 6, 7
[8] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao
Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao
Zhuang, Joseph E Gonzalez, et al. Vicuna: An open-source
chatbot impressing gpt-4 with 90%* chatgpt quality, march
2023. URL https://lmsys. org/blog/2023-03-30-vicuna, 3(5),
2023. 2
[9] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat
Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale
Fung, and Steven Hoi. Instructblip: Towards general-
purpose vision-language models with instruction tuning.
arXiv preprint arXiv:2305.06500, 2023. 5
[10] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch,
Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid,
Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palm-
e: An embodied multimodal language model. arXiv preprint
arXiv:2303.03378, 2023. 1
[11] Xiaoran Fan, Tao Ji, Changhao Jiang, Shuo Li, Senjie Jin,
Sirui Song, Junke Wang, Boyang Hong, Lu Chen, Guodong
Zheng, et al. Mousi: Poly-visual-expert vision-language
models. arXiv preprint arXiv:2401.17221, 2024. 1, 2, 3,
5, 6, 7
[12] Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie
Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He, Xi-
9[36] Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar.
Docvqa: A dataset for vqa on document images. In Proceed-
ings of the IEEE/CVF winter conference on applications of
computer vision, pages 2200–2209, 2021. 5
[37] Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy
Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez,
Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al.
Dinov2: Learning robust visual features without supervision.
arXiv preprint arXiv:2304.07193, 2023. 3
[38] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning
transferable visual models from natural language supervi-
sion. In International conference on machine learning, pages
8748–8763. PMLR, 2021. 1, 3
[39] Baifeng Shi, Ziyang Wu, Maolin Mao, Xin Wang, and Trevor
Darrell. When do we not need larger vision models? arXiv
preprint arXiv:2403.13043, 2024. 1, 3
[40] Min Shi, Fuxiao Liu, Shihao Wang, Shijia Liao, Subhashree
Radhakrishnan, De-An Huang, Hongxu Yin, Karan Sapra,
Yaser Yacoob, Humphrey Shi, et al. Eagle: Exploring the
design space for multimodal llms with mixture of encoders.
arXiv preprint arXiv:2408.15998, 2024. 2, 3, 4, 6, 7
[41] Wenzhe Shi, Jose Caballero, Ferenc Huszár, Johannes Totz,
Andrew P Aitken, Rob Bishop, Daniel Rueckert, and Zehan
Wang. Real-time single image and video super-resolution
using an efficient sub-pixel convolutional neural network. In
Proceedings of the IEEE conference on computer vision and
pattern recognition, pages 1874–1883, 2016. 4
[42] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang,
Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus
Rohrbach. Towards vqa models that can read. In Proceedings
of the IEEE/CVF conference on computer vision and pattern
recognition, pages 8317–8326, 2019. 5, 8
[43] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui
Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan
Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a
family of highly capable multimodal models. arXiv preprint
arXiv:2312.11805, 2023. 2
[44] Ran Tian, Boyi Li, Xinshuo Weng, Yuxiao Chen, Edward
Schmerling, Yue Wang, Boris Ivanovic, and Marco Pavone.
Tokenize the world into object-level knowledge to address
long-tail events in autonomous driving. arXiv preprint
arXiv:2407.00959, 2024. 4
[45] Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann
LeCun, and Saining Xie. Eyes wide shut? exploring the
visual shortcomings of multimodal llms. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 9568–9578, 2024. 3
[46] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste
Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al.
Llama: Open and efficient foundation language models.
arXiv preprint arXiv:2302.13971, 2023. 2
[47] Shihao Wang, Zhiding Yu, Xiaohui Jiang, Shiyi Lan, Min
Shi, Nadine Chang, Jan Kautz, Ying Li, and Jose M Alvarez.
Omnidrive: A holistic llm-agent framework for autonomous
[24] Zhang Li, Biao Yang, Qiang Liu, Zhiyin Ma, Shuo Zhang,
Jingxu Yang, Yabo Sun, Yuliang Liu, and Xiang Bai. Mon-
key: Image resolution and text label are important things for
large multi-modal models. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition,
pages 26763–26773, 2024. 1
[25] Ji Lin, Hongxu Yin, Wei Ping, Pavlo Molchanov, Moham-
mad Shoeybi, and Song Han. Vila: On pre-training for vi-
sual language models. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition, pages
26689–26699, 2024. 1, 5
[26] Ziyi Lin, Chris Liu, Renrui Zhang, Peng Gao, Longtian Qiu,
Han Xiao, Han Qiu, Chen Lin, Wenqi Shao, Keqin Chen,
et al. Sphinx: The joint mixing of weights, tasks, and visual
embeddings for multi-modal large language models. arXiv
preprint arXiv:2311.07575, 2023. 2
[27] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee.
Improved baselines with visual instruction tuning. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition, pages 26296–26306, 2024. 1, 2, 5,
6
[28] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan
Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Im-
proved reasoning, ocr, and world knowledge, 2024. 1, 3,
5
[29] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.
Visual instruction tuning. Advances in neural information
processing systems, 36, 2024. 1, 2
[30] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang
Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He,
Ziwei Liu, et al. Mmbench: Is your multi-modal model an
all-around player? In European Conference on Computer
Vision, pages 216–233. Springer, 2025. 5
[31] Haoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai
Dong, Bo Liu, Jingxiang Sun, Tongzheng Ren, Zhuoshu Li,
Yaofeng Sun, et al. Deepseek-vl: towards real-world vision-
language understanding. arXiv preprint arXiv:2403.05525,
2024. 2, 3
[32] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei
Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and
Ashwin Kalyan. Learn to explain: Multimodal reasoning via
thought chains for science question answering. Advances
in Neural Information Processing Systems, 35:2507–2521,
2022. 5
[33] Gen Luo, Yiyi Zhou, Yuxin Zhang, Xiawu Zheng, Xi-
aoshuai Sun, and Rongrong Ji. Feast your eyes: Mixture-of-
resolution adaptation for multimodal large language models.
arXiv preprint arXiv:2403.03003, 2024. 1, 2, 3, 4, 5, 6, 7
[34] Ana-Maria Marcu, Long Chen, Jan Hünermann, Alice Karn-
sund, Benoit Hanotte, Prajwal Chidananda, Saurabh Nair,
Vijay Badrinarayanan, Alex Kendall, Jamie Shotton, et al.
Lingoqa: Visual question answering for autonomous driving.
arXiv preprint arXiv:2312.14115, 2024. 4, 6, 7, 8
[35] Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty,
and Enamul Hoque. Chartqa: A benchmark for question an-
swering about charts with visual and logical reasoning. arXiv
preprint arXiv:2203.10244, 2022. 5
10driving with 3d perception, reasoning and planning. arXiv
preprint arXiv:2405.01533, 2024. 4
[48] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang,
Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang.
Mm-vet: Evaluating large multimodal models for integrated
capabilities. arXiv preprint arXiv:2308.02490, 2023. 5, 8
[49] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi
Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming
Ren, Yuxuan Sun, et al. Mmmu: A massive multi-discipline
multimodal understanding and reasoning benchmark for ex-
pert agi. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition, pages 9556–
9567, 2024. 5, 6, 8
[50] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and
Lucas Beyer. Sigmoid loss for language image pre-training.
In Proceedings of the IEEE/CVF International Conference
on Computer Vision, pages 11975–11986, 2023. 1, 3
[51] Zhuofan Zong, Bingqi Ma, Dazhong Shen, Guanglu Song,
Hao Shao, Dongzhi Jiang, Hongsheng Li, and Yu Liu. Mova:
Adapting mixture of vision experts to multimodal context.
arXiv preprint arXiv:2404.13046, 2024. 3
11

#======
Training Large Language Models to Reason in a
Continuous Latent Space
arXiv:2412.06769v2 [cs.CL] 11 Dec 2024
Shibo Hao1,2,∗ , Sainbayar Sukhbaatar1 , DiJia Su1 , Xian Li1 , Zhiting Hu2 , Jason Weston1 , Yuandong Tian1
1FAIR at Meta, 2 UC San Diego
∗Work done at Meta
Large language models (LLMs) are restricted to reason in the “language space”, where they typically
express the reasoning process with a chain-of-thought (CoT) to solve a complex reasoning problem.
However, we argue that language space may not always be optimal for reasoning. For example, most
word tokens are primarily for textual coherence and not essential for reasoning, while some critical
tokens require complex planning and pose huge challenges to LLMs. To explore the potential of
LLM reasoning in an unrestricted latent space instead of using natural language, we introduce a new
paradigm Coconut (Chain of Continuous Thought). We utilize the last hidden state of the LLM
as a representation of the reasoning state (termed “continuous thought”). Rather than decoding this
into a word token, we feed it back to the LLM as the subsequent input embedding directly in the
continuous space. Experiments show that Coconut can effectively augment the LLM on several
reasoning tasks. This novel latent reasoning paradigm leads to emergent advanced reasoning patterns:
the continuous thought can encode multiple alternative next reasoning steps, allowing the model to
perform a breadth-first search (BFS) to solve the problem, rather than prematurely committing to a
single deterministic path like CoT. Coconut outperforms CoT in certain logical reasoning tasks that
require substantial backtracking during planning, with fewer thinking tokens during inference. These
findings demonstrate the promise of latent reasoning and offer valuable insights for future research.
Date: December 12, 2024
1
Introduction
Large language models (LLMs) have demonstrated remarkable reasoning abilities, emerging from extensive
pretraining on human languages (Dubey et al., 2024; Achiam et al., 2023). While next token prediction is an
effective training objective, it imposes a fundamental constraint on the LLM as a reasoning machine: the
explicit reasoning process of LLMs must be generated in word tokens. For example, a prevalent approach,
known as chain-of-thought (CoT) reasoning (Wei et al., 2022), involves prompting or training LLMs to
generate solutions step-by-step using natural language. However, this is in stark contrast to certain human
cognition results. Neuroimaging studies have consistently shown that the language network – a set of brain
regions responsible for language comprehension and production – remains largely inactive during various
reasoning tasks (Amalric and Dehaene, 2019; Monti et al., 2012, 2007, 2009; Fedorenko et al., 2011). Further
evidence indicates that human language is optimized for communication rather than reasoning (Fedorenko
et al., 2024).
A significant issue arises when LLMs use language for reasoning: the amount of reasoning required for
each particular reasoning token varies greatly, yet current LLM architectures allocate nearly the same
computing budget for predicting every token. Most tokens in a reasoning chain are generated solely for fluency,
contributing little to the actual reasoning process. On the contrary, some critical tokens require complex
planning and pose huge challenges to LLMs. While previous work has attempted to fix these problems by
prompting LLMs to generate succinct reasoning chains (Madaan and Yazdanbakhsh, 2022), or performing
additional reasoning before generating some critical tokens (Zelikman et al., 2024), these solutions remain
constrained within the language space and do not solve the fundamental problems. On the contrary, it would
be ideal for LLMs to have the freedom to reason without any language constraints, and then translate their
findings into language only when necessary.
1Figure 1 A comparison of Chain of Continuous Thought (Coconut) with Chain-of-Thought (CoT). In CoT, the model
generates the reasoning process as a word token sequence (e.g., [xi , xi+1 , ..., xi+j ] in the figure). Coconut regards the
last hidden state as a representation of the reasoning state (termed “continuous thought”), and directly uses it as the
next input embedding. This allows the LLM to reason in an unrestricted latent space instead of a language space.
In this work we instead explore LLM reasoning in a latent space by introducing a novel paradigm, Coconut
(Chain of Continuous Thought). It involves a simple modification to the traditional CoT process: instead of
mapping between hidden states and language tokens using the language model head and embedding layer,
Coconut directly feeds the last hidden state (a continuous thought) as the input embedding for the next
token (Figure 1). This modification frees the reasoning from being within the language space, and the system
can be optimized end-to-end by gradient descent, as continuous thoughts are fully differentiable. To enhance
the training of latent reasoning, we employ a multi-stage training strategy inspired by Deng et al. (2024),
which effectively utilizes language reasoning chains to guide the training process.
Interestingly, our proposed paradigm leads to an efficient reasoning pattern. Unlike language-based reasoning,
continuous thoughts in Coconut can encode multiple potential next steps simultaneously, allowing for a
reasoning process akin to breadth-first search (BFS). While the model may not initially make the correct
decision, it can maintain many possible options within the continuous thoughts and progressively eliminate
incorrect paths through reasoning, guided by some implicit value functions. This advanced reasoning
mechanism surpasses traditional CoT, even though the model is not explicitly trained or instructed to operate
in this manner, as seen in previous works (Yao et al., 2023; Hao et al., 2023).
Experimentally, Coconut successfully enhances the reasoning capabilities of LLMs. For math reasoning
(GSM8k, Cobbe et al., 2021), using continuous thoughts is shown to be beneficial to reasoning accuracy,
mirroring the effects of language reasoning chains. This indicates the potential to scale and solve increasingly
challenging problems by chaining more continuous thoughts. On logical reasoning including ProntoQA (Saparov
and He, 2022), and our newly proposed ProsQA (Section 4.1) which requires stronger planning ability, Coconut
and some of its variants even surpasses language-based CoT methods, while generating significantly fewer
tokens during inference. We believe that these findings underscore the potential of latent reasoning and could
provide valuable insights for future research.
2
Related Work
Chain-of-thought (CoT) reasoning. We use the term chain-of-thought broadly to refer to methods that generate
an intermediate reasoning process in language before outputting the final answer. This includes prompting
LLMs (Wei et al., 2022; Khot et al., 2022; Zhou et al., 2022), or training LLMs to generate reasoning chains,
either with supervised finetuning (Yue et al., 2023; Yu et al., 2023) or reinforcement learning (Wang et al.,
2024; Havrilla et al., 2024; Shao et al., 2024; Yu et al., 2024a). Madaan and Yazdanbakhsh (2022) classified
the tokens in CoT into symbols, patterns, and text, and proposed to guide the LLM to generate concise CoT
based on analysis of their roles. Recent theoretical analyses have demonstrated the usefulness of CoT from
the perspective of model expressivity (Feng et al., 2023; Merrill and Sabharwal, 2023; Li et al., 2024). By
employing CoT, the effective depth of the transformer increases because the generated outputs are looped
back to the input (Feng et al., 2023). These analyses, combined with the established effectiveness of CoT,
2motivated our design that feeds the continuous thoughts back to the LLM as the next input embedding. While
CoT has proven effective for certain tasks, its autoregressive generation nature makes it challenging to mimic
human reasoning on more complex problems (LeCun, 2022; Hao et al., 2023), which typically require planning
and search. There are works that equip LLMs with explicit tree search algorithms (Xie et al., 2023; Yao et al.,
2023; Hao et al., 2024), or train the LLM on search dynamics and trajectories (Lehnert et al., 2024; Gandhi
et al., 2024; Su et al., 2024). In our analysis, we find that after removing the constraint of a language space, a
new reasoning pattern similar to BFS emerges, even though the model is not explicitly trained in this way.
Latent reasoning in LLMs. Previous works mostly define latent reasoning in LLMs as the hidden computation
in transformers (Yang et al., 2024; Biran et al., 2024). Yang et al. (2024) constructed a dataset of two-hop
reasoning problems and discovered that it is possible to recover the intermediate variable from the hidden
representations. Biran et al. (2024) further proposed to intervene the latent reasoning by “back-patching” the
hidden representation. Shalev et al. (2024) discovered parallel latent reasoning paths in LLMs. Another line
of work has discovered that, even if the model generates a CoT to reason, the model may actually utilize a
different latent reasoning process. This phenomenon is known as the unfaithfulness of CoT reasoning (Wang
et al., 2022; Turpin et al., 2024). To enhance the latent reasoning of LLM, previous research proposed to
augment it with additional tokens. Goyal et al. (2023) pretrained the model by randomly inserting a learnable
<pause> tokens to the training corpus. This improves LLM’s performance on a variety of tasks, especially
when followed by supervised finetuning with <pause> tokens. On the other hand, Pfau et al. (2024) further
explored the usage of filler tokens, e.g., “...”, and concluded that they work well for highly parallelizable
problems. However, Pfau et al. (2024) mentioned these methods do not extend the expressivity of the LLM like
CoT; hence, they may not scale to more general and complex reasoning problems. Wang et al. (2023) proposed
to predict a planning token as a discrete latent variable before generating the next reasoning step. Recently,
it has also been found that one can “internalize” the CoT reasoning into latent reasoning in the transformer
with knowledge distillation (Deng et al., 2023) or a special training curriculum which gradually shortens
CoT (Deng et al., 2024). Yu et al. (2024b) also proposed to distill a model that can reason latently from data
generated with complex reasoning algorithms. These training methods can be combined to our framework,
and specifically, we find that breaking down the learning of continuous thoughts into multiple stages, inspired
by iCoT (Deng et al., 2024), is very beneficial for the training. Recently, looped transformers (Giannou et al.,
2023; Fan et al., 2024) have been proposed to solve algorithmic tasks, which have some similarities to the
computing process of continuous thoughts, but we focus on common reasoning tasks and aim at investigating
latent reasoning in comparison to language space.
3
Coconut: Chain of Continuous Thought
In this section, we introduce our new paradigm Coconut (Chain of Continuous Thought) for reasoning in
an unconstrained latent space. We begin by introducing the background and notation we use for language
models. For an input sequence x = (x1 , ..., xT ), the standard large language model M can be described as:
Ht = Transformer(Et )
M(xt+1 | x≤t ) = softmax(W ht )
where Et = [e(x1 ), e(x2 ), ..., e(xt )] is the sequence of token embeddings up to position t; Ht ∈ Rt×d is the
matrix of the last hidden states for all tokens up to position t; ht is the last hidden state of position t, i.e.,
ht = Ht [t, :]; e(·) is the token embedding function; W is the parameter of the language model head.
Method Overview. In the proposed Coconut method, the LLM switches between the “language mode” and
“latent mode” (Figure 1). In language mode, the model operates as a standard language model, autoregressively
generating the next token. In latent mode, it directly utilizes the last hidden state as the next input embedding.
This last hidden state represents the current reasoning state, termed as a “continuous thought”.
Special tokens <bot> and <eot> are employed to mark the beginning and end of the latent thought
mode, respectively. As an example, we assume latent reasoning occurs between positions i and j, i.e., xi =
<bot> and xj = <eot>. When the model is in the latent mode (i < t < j), we use the last hidden state
from the previous token to replace the input embedding, i.e., Et = [e(x1 ), e(x2 ), ..., e(xi ), hi , hi+1 , ..., ht−1 ].
3Figure 2 Training procedure of Chain of Continuous Thought (Coconut). Given training data with language reasoning
steps, at each training stage we integrate c additional continuous thoughts (c = 1 in this example), and remove one
language reasoning step. The cross-entropy loss is then used on the remaining tokens after continuous thoughts.
After the latent mode finishes (t ≥ j), the input reverts to using the token embedding, i.e., Et =
[e(x1 ), e(x2 ), ..., e(xi ), hi , hi+1 , ..., hj−1 , e(xj ), ..., e(xt )]. It is worth noting that the last hidden states have
been processed by the final normalization layer, so they are not too large in magnitude. M(xt+1 | x≤t ) is
not defined when i < t < j, since the latent thought is not intended to be mapped back to language space.
However, softmax(W ht ) can still be calculated for probing purposes (see Section 4).
Training Procedure. In this work, we focus on a problem-solving setting where the model receives a question as
input and is expected to generate an answer through a reasoning process. We leverage language CoT data
to supervise continuous thought by implementing a multi-stage training curriculum inspired by Deng et al.
(2024). As shown in Figure 2, in the initial stage, the model is trained on regular CoT instances. In the
subsequent stages, at the k-th stage, the first k reasoning steps in the CoT are replaced with k × c continuous
thoughts1 , where c is a hyperparameter controlling the number of latent thoughts replacing a single language
reasoning step. Following Deng et al. (2024), we also reset the optimizer state when training stages switch. We
insert <bot> and <eot> tokens (which are not counted towards c) to encapsulate the continuous thoughts.
During the training process, we optimize the normal negative log-likelihood loss, but mask the loss on questions
and latent thoughts. It is important to note that the objective does not encourage the continuous thought to
compress the removed language thought, but rather to facilitate the prediction of future reasoning. Therefore,
it’s possible for the LLM to learn more effective representations of reasoning steps compared to human
language.
Training Details. Our proposed continuous thoughts are fully differentiable and allow for back-propagation. We
perform n + 1 forward passes when n latent thoughts are scheduled in the current training stage, computing
a new latent thought with each pass and finally conducting an additional forward pass to obtain a loss
on the remaining text sequence. While we can save any repetitive computing by using a KV cache, the
sequential nature of the multiple forward passes poses challenges for parallelism. Further optimizing the
training efficiency of Coconut remains an important direction for future research.
Inference Process. The inference process for Coconut is analogous to standard language model decoding,
except that in latent mode, we directly feed the last hidden state as the next input embedding. A challenge
lies in determining when to switch between latent and language modes. As we focus on the problem-solving
setting, we insert a <bot> token immediately following the question tokens. For <eot>, we consider two
potential strategies: a) train a binary classifier on latent thoughts to enable the model to autonomously
decide when to terminate the latent reasoning, or b) always pad the latent thoughts to a constant length. We
found that both approaches work comparably well. Therefore, we use the second option in our experiment for
simplicity, unless specified otherwise.
1 If a language reasoning chain is shorter than k steps, then all the language thoughts will be removed.
44
Experiments
We validate the feasibility of LLM reasoning in a continuous latent space through experiments on three
datasets. We mainly evaluate the accuracy by comparing the model-generated answers with the ground truth.
The number of newly generated tokens per question is also analyzed, as a measure of reasoning efficiency. We
report the clock-time comparison in Appendix B.
4.1
Reasoning Tasks
Math Reasoning. We use GSM8k (Cobbe et al., 2021) as the dataset for math reasoning. It consists of grade
school-level math problems. Compared to the other datasets in our experiments, the problems are more
diverse and open-domain, closely resembling real-world use cases. Through this task, we explore the potential
of latent reasoning in practical applications. To train the model, we use a synthetic dataset generated by Deng
et al. (2023).
Logical Reasoning. Logical reasoning involves the proper application of known conditions to prove or disprove a
conclusion using logical rules. This requires the model to choose from multiple possible reasoning paths, where
the correct decision often relies on exploration and planning ahead. We use 5-hop ProntoQA (Saparov and
He, 2022) questions, with fictional concept names. For each problem, a tree-structured ontology is randomly
generated and described in natural language as a set of known conditions. The model is asked to judge
whether a given statement is correct based on these conditions. This serves as a simplified simulation of more
advanced reasoning tasks, such as automated theorem proving (Chen et al., 2023; DeepMind, 2024).
We found that the generation process of ProntoQA could be more challenging, especially since the size of
distracting branches in the ontology is always small, reducing the need for complex planning. To fix that, we
apply a new dataset construction pipeline using randomly generated DAGs to structure the known conditions.
The resulting dataset requires the model to perform substantial planning and searching over the graph to find
the correct reasoning chain. We refer to this new dataset as ProsQA (Proof with Search Question-Answering).
A visualized example is shown in Figure 6. More details of datasets can be found in Appendix A.
4.2
Experimental Setup
We use a pre-trained GPT-2 (Radford et al., 2019) as the base model for all experiments. The learning rate is
set to 1 × 10−4 while the effective batch size is 128. Following Deng et al. (2024), we also reset the optimizer
when the training stages switch.
Math Reasoning. By default, we use 2 latent thoughts (i.e., c = 2) for each reasoning step. We analyze the
correlation between performance and c in Section 4.4. The model goes through 3 stages besides the initial
stage. Then, we have an additional stage, where we still use 3 × c continuous thoughts as in the penultimate
stage, but remove all the remaining language reasoning chain. This handles the long-tail distribution of
reasoning chains longer than 3 steps. We train the model for 6 epochs in the initial stage, and 3 epochs in
each remaining stage.
Logical Reasoning. We use one continuous thought for every reasoning step (i.e., c = 1). The model goes
through 6 training stages in addition to the initial stage, because the maximum number of reasoning steps is
6 in these two datasets. The model then fully reasons with continuous thoughts to solve the problems in the
last stage. We train the model for 5 epochs per stage.
For all datasets, after the standard schedule, the model stays in the final training stage, until the 50th epoch.
We select the checkpoint based on the accuracy on the validation set. For inference, we manually set the
number of continuous thoughts to be consistent with their final training stage. We use greedy decoding for all
experiments.
4.3
Baselines and Variants of Coconut
We consider the following baselines: (1) CoT : We use the complete reasoning chains to train the language model
with supervised finetuning, and during inference, the model generates a reasoning chain before outputting an
5Method
GSM8k
ProntoQA
ProsQA
Acc. (%)# TokensAcc. (%)# TokensAcc. (%)# Tokens
CoT42.9 ±0.225.098.8 ±0.892.577.5 ±1.949.4
No-CoT
iCoT
Pause Token16.5 ±0.5
30.0∗
16.4 ±1.82.2
2.2
2.293.8 ±0.7
99.8 ±0.3
77.7 ±21.03.0
3.0
3.076.7 ±1.0
98.2 ±0.3
75.9 ±0.78.2
8.2
8.2
Coconut (Ours)
- w/o curriculum
- w/o thought
- pause as thought34.1 ±1.5
14.4 ±0.8
21.6 ±0.5
24.1 ±0.78.2
8.2
2.3
2.299.8 ±0.2
52.4 ±0.4
99.9 ±0.1
100.0 ±0.19.0
9.0
3.0
3.097.0 ±0.3
76.1 ±0.2
95.5 ±1.1
96.6 ±0.814.2
14.2
8.2
8.2
Table 1 Results on three datasets: GSM8l, ProntoQA and ProsQA. Higher accuracy indicates stronger reasoning ability,
while generating fewer tokens indicates better efficiency. ∗ The result is from Deng et al. (2024).
answer. (2) No-CoT : The LLM is trained to directly generate the answer without using a reasoning chain.
(3) iCoT (Deng et al., 2024): The model is trained with language reasoning chains and follows a carefully
designed schedule that “internalizes” CoT. As the training goes on, tokens at the beginning of the reasoning
chain are gradually removed until only the answer remains. During inference, the model directly predicts
the answer. (4) Pause token (Goyal et al., 2023): The model is trained using only the question and answer,
without a reasoning chain. However, different from No-CoT, special <pause> tokens are inserted between
the question and answer, which are believed to provide the model with additional computational capacity
to derive the answer. For a fair comparison, the number of <pause> tokens is set the same as continuous
thoughts in Coconut.
We also evaluate some variants of our method: (1) w/o curriculum: Instead of the multi-stage training,
we directly use the data from the last stage which only includes questions and answers to train Coconut.
The model uses continuous thoughts to solve the whole problem. (2) w/o thought: We keep the multi-stage
training which removes language reasoning steps gradually, but don’t use any continuous latent thoughts.
While this is similar to iCoT in the high-level idea, the exact training schedule is set to be consistent with
Coconut, instead of iCoT. This ensures a more strict comparison. (3) Pause as thought: We use special
<pause> tokens to replace the continuous thoughts, and apply the same multi-stage training curriculum as
Coconut.
Results and Discussion
We show the overall results on all datasets in Table 1. Contin-
uous thoughts effectively enhance LLM reasoning, as shown
by the consistent improvement over no-CoT. It even shows
better performance than CoT on ProntoQA and ProsQA.
We describe several key conclusions from the experiments as
follows.
“Chaining” continuous thoughts enhances reasoning. In conven-
tional CoT, the output token serves as the next input, which
proves to increase the effective depth of LLMs and enhance
their expressiveness (Feng et al., 2023). We explore whether
latent space reasoning retains this property, as it would sug-
gest that this method could scale to solve increasingly complex
problems by chaining multiple latent thoughts.
36
34
Accuracy (%)
4.4
32
30
28
26
0
1
# Thoughts per step
2
Figure 3 Accuracy on GSM8k with different
In our experiments with GSM8k, we found that Coconut
number of continuous thoughts.
outperformed other architectures trained with similar strate-
gies, particularly surpassing the latest baseline, iCoT (Deng
et al., 2024). The performance is significantly better than Coconut (pause as thought) which also enables
more computation in the LLMs. While Pfau et al. (2024) empirically shows that filler tokens, such as
the special <pause> tokens, can benefit highly parallelizable problems, our results show that Coconut
6architecture is more effective for general problems, e.g., math word problems, where a reasoning step often
heavily depends on previous steps. Additionally, we experimented with adjusting the hyperparameter c,
which controls the number of latent thoughts corresponding to one language reasoning step (Figure 3). As
we increased c from 0 to 1 to 2, the model’s performance steadily improved.2 These results suggest that a
chaining effect similar to CoT can be observed in the latent space.
In two other synthetic tasks, we found that the variants of Coconut (w/o thoughts or pause as thought), and
the iCoT baseline also achieve impressive accuracy. This indicates that the model’s computational capacity
may not be the bottleneck in these tasks. In contrast, GSM8k, being an open-domain question-answering
task, likely involves more complex contextual understanding and modeling, placing higher demands on
computational capability.
Latent reasoning outperforms language reasoning in planning-intensive tasks. Complex reasoning often requires
the model to “look ahead” and evaluate the appropriateness of each step. Among our datasets, GSM8k and
ProntoQA are relatively straightforward for next-step prediction, due to intuitive problem structures and
limited branching. In contrast, ProsQA’s randomly generated DAG structure significantly challenges the
model’s planning capabilities. As shown in Table 1, CoT does not offer notable improvement over No-CoT.
However, Coconut, its variants, and iCoT substantially enhance reasoning on ProsQA, indicating that latent
space reasoning provides a clear advantage in tasks demanding extensive planning. An in-depth analysis of
this process is provided in Section 5.
The LLM still needs guidance to learn latent reasoning. In the ideal case, the model should learn the most effective
continuous thoughts automatically through gradient descent on questions and answers (i.e., Coconut w/o
curriculum). However, from the experimental results, we found the models trained this way do not perform
any better than no-CoT.
With the multi-stage curriculum which decomposes
the training into easier objectives, Coconut is able
to achieve top performance across various tasks. The
multi-stage training also integrates well with pause
tokens (Coconut- pause as thought). Despite using
the same architecture and similar multi-stage train-
ing objectives, we observed a small gap between the
performance of iCoT and Coconut (w/o thoughts).
The finer-grained removal schedule (token by token)
and a few other tricks in iCoT may ease the training
process. We leave combining iCoT and Coconut
as future work. While the multi-stage training used
for Coconut has proven effective, further research
is definitely needed to develop better and more gen-
eral strategies for learning reasoning in latent space,
especially without the supervision from language rea-
soning chains.
Continuous thoughts are efficient representations of
reasoning. Though the continuous thoughts are not
Figure 4 A case study where we decode the continuous
thought into language tokens.
intended to be decoded to language tokens, we can
still use it as an intuitive interpretation of the continuous thought. We show a case study in Figure 4 of
a math word problem solved by Coconut (c = 1). The first continuous thought can be decoded into
tokens like “180”, “ 180” (with a space), and “9”. Note that, the reasoning trace for this problem should be
3 × 3 × 60 = 9 × 60 = 540, or 3 × 3 × 60 = 3 × 180 = 540. The interpretations of the first thought happen to be
the first intermediate variables in the calculation. Moreover, it encodes a distribution of different traces into
the continuous thoughts. As shown in Section 5.3, this feature enables a more advanced reasoning pattern for
planning-intense reasoning tasks.
2We discuss the case of larger c in Appendix C.
7Figure 5 The accuracy of final answer (left) and reasoning process (right) of multiple variants of Coconut and baselines
on ProsQA.
5
Understanding the Latent Reasoning in Coconut
In this section, we present an analysis of the latent reasoning process with a variant of Coconut. By
leveraging its ability to switch between language and latent space reasoning, we are able to control the
model to interpolate between fully latent reasoning and fully language reasoning and test their performance
(Section 5.2). This also enables us to interpret the the latent reasoning process as tree search (Section 5.3).
Based on this perspective, we explain why latent reasoning can make the decision easier for LLMs (Section 5.4).
5.1
Experimental Setup
Methods. The design of Coconut allows us to control the number of latent thoughts by manually setting the
position of the <eot> token during inference. When we enforce Coconut to use k continuous thoughts, the
model is expected to output the remaining reasoning chain in language, starting from the k + 1 step. In our
experiments, we test variants of Coconut on ProsQA with k ∈ {0, 1, 2, 3, 4, 5, 6}. Note that all these variants
only differ in inference time while sharing the same model weights. Besides, we report the performance of
CoT and no-CoT as references.
To address the issue of forgetting earlier training stages, we modify the original multi-stage training curriculum
by always mixing data from other stages with a certain probability (p = 0.3). This updated training curriculum
yields similar performance and enables effective control over the switch between latent and language reasoning.
Metrics. We apply two sets of evaluation metrics. One of them is based on the correctness of the final answer,
regardless of the reasoning process. It is the metric used in the main experimental results above (Section 4.4).
To enable fine-grained analysis, we define another metric on the reasoning process. Assuming we have a
complete language reasoning chain which specifies a path in the graph, we can classify it into (1) Correct Path:
The output is one of the shortest paths to the correct answer. (2) Longer Path: A valid path that correctly
answers the question but is longer than the shortest path. (3) Hallucination: The path includes nonexistent
edges or is disconnected. (4) Wrong Target: A valid path in the graph, but the destination node is not the one
being asked. These four categories naturally apply to the output from Coconut (k = 0) and CoT, which
generate the full path. For Coconut with k > 0 that outputs only partial paths in language (with the initial
steps in continuous reasoning), we classify the reasoning as a Correct Path if a valid explanation can complete
it. Also, we define Longer Path and Wrong Target for partial paths similarly. If no valid explanation completes
the path, it’s classified as hallucination. In no-CoT and Coconut with larger k, the model may only output
the final answer without any partial path, and it falls into (5) Correct Label or (6) Incorrect Label. These six
categories cover all cases without overlap.
8Figure 6 A case study of ProsQA. The model trained with CoT hallucinates an edge (Every yumpus is a rempus) after
getting stuck in a dead end. Coconut (k=1) outputs a path that ends with an irrelevant node. Coconut (k=2)
solves the problem correctly.
Figure 7 An illustration of the latent search trees. The example is the same test case as in Figure 6. The height of
a node (denoted as h in the figure) is defined as the longest distance to any leaf nodes in the graph. We show the
probability of the first concept predicted by the model following latent thoughts (e.g., “lempus” in the left figure). It is
calculated as the multiplication of the probability of all tokens within the concept conditioned on previous context
(omitted in the figure for brevity). This metric can be interpreted as an implicit value function estimated by the model,
assessing the potential of each node leading to the correct answer.
5.2
Interpolating between Latent and Language Reasoning
Figure 5 shows a comparative analysis of different reasoning methods on ProsQA. As more reasoning is done
with continuous thoughts (increasing k), both final answer accuracy (Figure 5, left) and the rate of correct
reasoning processes (“Correct Label” and “Correct Path” in Figure 5, right) improve. Additionally, the rate of
“Hallucination” and “Wrong Target” decrease, which typically occur when the model makes a wrong move
earlier. This also indicates the better planning ability when more reasoning happens in the latent space.
A case study is shown in Figure 6, where CoT hallucinates an nonexistent edge, Coconut (k = 1) leads to
a wrong target, but Coconut (k = 2) successfully solves the problem. In this example, the model cannot
accurately determine which edge to choose at the earlier step. However, as latent reasoning can avoid making
a hard choice upfront, the model can progressively eliminate incorrect options in subsequent steps and achieves
higher accuracy at the end of reasoning. We show more evidence and details of this reasoning process in
Section 5.3.
The comparison between CoT and Coconut (k = 0) reveals another interesting observation: even when
9Figure 8 Analysis of parallelism in latent tree search. The left plot depicts the cumulative value of the top-1, top-2, and
top-3 candidate nodes for the first thoughts, calculated across test cases and ranked by percentile. The significant gaps
between the lines reflect the model’s ability to explore alternative latent thoughts in parallel. The right plot shows
the corresponding analysis for the second thoughts, where the gaps between lines are narrower, indicating reduced
parallelism and increased certainty in reasoning as the search tree develops. This shift highlights the model’s transition
toward more focused exploration in later stages.
Coconut is forced to generate a complete reasoning chain, the accuracy of the answers is still higher than
CoT. The generated reasoning paths are also more accurate with less hallucination. From this, we can infer
that the training method of mixing different stages improves the model’s ability to plan ahead. The training
objective of CoT always concentrates on the generation of the immediate next step, making the model
“shortsighted”. In later stages of Coconut training, the first few steps are hidden, allowing the model to focus
more on future steps. This is related to the findings of Gloeckle et al. (2024), where they propose multi-token
prediction as a new pretraining objective to improve the LLM’s ability to plan ahead.
5.3
Interpreting the Latent Search Tree
Given the intuition that continuous thoughts can encode multiple potential next steps, the latent reasoning
can be interpreted as a search tree, rather than merely a reasoning “chain”. Taking the case of Figure 6 as a
concrete example, the first step could be selecting one of the children of Alex, i.e., {lempus, sterpus, zhorpus,
grimpus}. We depict all possible branches in the left part of Figure 7. Similarly, in the second step, the
frontier nodes will be the grandchildren of Alex (Figure 7, right).
Unlike a standard breadth-first search (BFS), which explores all frontier nodes uniformly, the model demon-
strates the ability to prioritize promising nodes while pruning less relevant ones. To uncover the model’s
preferences, we analyze its subsequent outputs in language space. For instance, if the model is forced to
switch back to language space after a single latent thought (k = 1), it predicts the next step in a structured
format, such as “every [Concept A] is a [Concept B].” By examining the probability distribution over potential
fillers for [Concept A], we can derive numeric values for the children of the root node Alex (Figure 7, left).
Similarly, when k = 2, the prediction probabilities for all frontier nodes—the grandchildren of Alex —are
obtained (Figure 7, right).
The probability distribution can be viewed as the model’s implicit value function, estimating each node’s
potential to reach the target. As shown in the figure, “lempus”, “zhorpus”, “grimpus”, and “sterpus” have a
value of 0.33, 0.16, 0.32, and 0.01, respectively. This indicates that in the first continuous thought, the model
has mostly ruled out “sterpus” as an option but remains uncertain about the correct choice among the other
three. In the second thought, however, the model has mostly ruled out other options but focused on “rorpus”.
Figure 8 presents an analysis of the parallelism in the model’s latent reasoning across the first and second
thoughts. For the first thoughts (left panel), the cumulative values of the top-1, top-2, and top-3 candidate
nodes are computed and plotted against their respective percentiles across the test set. The noticeable gaps
between the three lines indicate that the model maintains significant diversity in its reasoning paths at this
10stage, suggesting a broad exploration of alternative possibilities. In contrast, the second thoughts (right panel)
show a narrowing of these gaps. This trend suggests that the model transitions from parallel exploration to
more focused reasoning in the second latent reasoning step, likely as it gains more certainty about the most
promising paths.
5.4
Why is a Latent Space Better for Planning?
In this section, we explore why latent reasoning is ad-
vantageous for planning, drawing on the search tree per-
spective and the value function defined earlier. Referring
to our illustrative example, a key distinction between
“sterpus” and the other three options lies in the structure
of the search tree: “sterpus” is a leaf node (Figure 6).
This makes it immediately identifiable as an incorrect
choice, as it cannot lead to the target node “bompus”.
In contrast, the other nodes have more descendants to
explore, making their evaluation more challenging.
To quantify a node’s exploratory potential, we measure
its height in the tree, defined as the shortest distance to
any leaf node. Based on this notion, we hypothesize that
nodes with lower heights are easier to evaluate accurately,
as their exploratory potential is limited. Consistent
with this hypothesis, in our example, the model exhibits
greater uncertainty between “grimpus” and “lempus”,
both of which have a height of 2—higher than the other
candidates.
To test this hypothesis more rigorously, we analyze the
correlation between the model’s prediction probabili-
ties and node heights during the first and second latent
reasoning steps across the test set. Figure 9 reveals a
clear pattern: the model successfully assigns lower values
to incorrect nodes and higher values to correct nodes
when their heights are low. However, as node heights
increase, this distinction becomes less pronounced, indi-
cating greater difficulty in accurate evaluation.
Figure 9 The correlation between prediction probabil-
ity of concepts and their heights.
In conclusion, these findings highlight the benefits of leveraging latent space for planning. By delaying definite
decisions and expanding the latent reasoning process, the model pushes its exploration closer to the search
tree’s terminal states, making it easier to distinguish correct nodes from incorrect ones.
6
Conclusion
In this paper, we presented Coconut, a novel paradigm for reasoning in continuous latent space. Through
extensive experiments, we demonstrated that Coconut significantly enhances LLM reasoning capabilities.
Notably, our detailed analysis highlighted how an unconstrained latent space allows the model to develop an
effective reasoning pattern similar to BFS. Future work is needed to further refine and scale latent reasoning
methods. One promising direction is pretraining LLMs with continuous thoughts, which may enable models
to generalize more effectively across a wider range of reasoning scenarios. We anticipate that our findings will
inspire further research into latent reasoning methods, ultimately contributing to the development of more
advanced machine reasoning systems.
11Acknowledgement
The authors express their sincere gratitude to Jihoon Tack for his valuable discussions throughout the course
of this work.
References
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida,
Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774,
2023.
Marie Amalric and Stanislas Dehaene. A distinct cortical network for mathematical knowledge in the human brain.
NeuroImage, 189:19–31, 2019.
Eden Biran, Daniela Gottesman, Sohee Yang, Mor Geva, and Amir Globerson. Hopping too late: Exploring the
limitations of large language models on multi-hop queries. arXiv preprint arXiv:2406.12775, 2024.
Wenhu Chen, Ming Yin, Max Ku, Pan Lu, Yixin Wan, Xueguang Ma, Jianyu Xu, Xinyi Wang, and Tony Xia.
Theoremqa: A theorem-driven question answering dataset. In Proceedings of the 2023 Conference on Empirical
Methods in Natural Language Processing, pages 7889–7901, 2023.
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert,
Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint
arXiv:2110.14168, 2021.
Google DeepMind. Ai achieves silver-medal standard solving international mathematical olympiad problems, 2024.
https://deepmind.google/discover/blog/ai-solves-imo-problems-at-silver-medal-level/.
Yuntian Deng, Kiran Prasad, Roland Fernandez, Paul Smolensky, Vishrav Chaudhary, and Stuart Shieber. Implicit
chain of thought reasoning via knowledge distillation. arXiv preprint arXiv:2311.01460, 2023.
Yuntian Deng, Yejin Choi, and Stuart Shieber. From explicit cot to implicit cot: Learning to internalize cot step by
step. arXiv preprint arXiv:2405.14838, 2024.
Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil
Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783,
2024.
Ying Fan, Yilun Du, Kannan Ramchandran, and Kangwook Lee. Looped transformers for length generalization. arXiv
preprint arXiv:2409.15647, 2024.
Evelina Fedorenko, Michael K Behr, and Nancy Kanwisher. Functional specificity for high-level linguistic processing in
the human brain. Proceedings of the National Academy of Sciences, 108(39):16428–16433, 2011.
Evelina Fedorenko, Steven T Piantadosi, and Edward AF Gibson. Language is primarily a tool for communication
rather than thought. Nature, 630(8017):575–586, 2024.
Guhao Feng, Bohang Zhang, Yuntian Gu, Haotian Ye, Di He, and Liwei Wang. Towards revealing the mystery behind
chain of thought: a theoretical perspective. Advances in Neural Information Processing Systems, 36, 2023.
Kanishk Gandhi, Denise Lee, Gabriel Grand, Muxin Liu, Winson Cheng, Archit Sharma, and Noah D Goodman.
Stream of search (sos): Learning to search in language. arXiv preprint arXiv:2404.03683, 2024.
Angeliki Giannou, Shashank Rajput, Jy-yong Sohn, Kangwook Lee, Jason D Lee, and Dimitris Papailiopoulos. Looped
transformers as programmable computers. In International Conference on Machine Learning, pages 11398–11442.
PMLR, 2023.
Fabian Gloeckle, Badr Youbi Idrissi, Baptiste Rozière, David Lopez-Paz, and Gabriel Synnaeve. Better & faster large
language models via multi-token prediction. arXiv preprint arXiv:2404.19737, 2024.
Sachin Goyal, Ziwei Ji, Ankit Singh Rawat, Aditya Krishna Menon, Sanjiv Kumar, and Vaishnavh Nagarajan. Think
before you speak: Training language models with pause tokens. arXiv preprint arXiv:2310.02226, 2023.
Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, and Zhiting Hu. Reasoning with
language model is planning with world model. arXiv preprint arXiv:2305.14992, 2023.
12Shibo Hao, Yi Gu, Haotian Luo, Tianyang Liu, Xiyan Shao, Xinyuan Wang, Shuhua Xie, Haodi Ma, Adithya Samavedhi,
Qiyue Gao, et al. Llm reasoners: New evaluation, library, and analysis of step-by-step reasoning with large language
models. arXiv preprint arXiv:2404.05221, 2024.
Alex Havrilla, Yuqing Du, Sharath Chandra Raparthy, Christoforos Nalmpantis, Jane Dwivedi-Yu, Maksym Zhuravin-
skyi, Eric Hambro, Sainbayar Sukhbaatar, and Roberta Raileanu. Teaching large language models to reason with
reinforcement learning. arXiv preprint arXiv:2403.04642, 2024.
Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, and Ashish Sabharwal.
Decomposed prompting: A modular approach for solving complex tasks. arXiv preprint arXiv:2210.02406, 2022.
Yann LeCun. A path towards autonomous machine intelligence version 0.9. 2, 2022-06-27. Open Review, 62(1):1–62,
2022.
Lucas Lehnert, Sainbayar Sukhbaatar, Paul Mcvay, Michael Rabbat, and Yuandong Tian. Beyond a*: Better planning
with transformers via search dynamics bootstrapping. arXiv preprint arXiv:2402.14083, 2024.
Zhiyuan Li, Hong Liu, Denny Zhou, and Tengyu Ma. Chain of thought empowers transformers to solve inherently
serial problems. arXiv preprint arXiv:2402.12875, 2024.
Aman Madaan and Amir Yazdanbakhsh. Text and patterns: For effective chain of thought, it takes two to tango.
arXiv preprint arXiv:2209.07686, 2022.
William Merrill and Ashish Sabharwal. The expresssive power of transformers with chain of thought. arXiv preprint
arXiv:2310.07923, 2023.
Martin M Monti, Daniel N Osherson, Michael J Martinez, and Lawrence M Parsons. Functional neuroanatomy of
deductive inference: a language-independent distributed network. Neuroimage, 37(3):1005–1016, 2007.
Martin M Monti, Lawrence M Parsons, and Daniel N Osherson. The boundaries of language and thought in deductive
inference. Proceedings of the National Academy of Sciences, 106(30):12554–12559, 2009.
Martin M Monti, Lawrence M Parsons, and Daniel N Osherson. Thought beyond language: neural dissociation of
algebra and natural language. Psychological science, 23(8):914–922, 2012.
Jacob Pfau, William Merrill, and Samuel R Bowman. Let’s think dot by dot: Hidden computation in transformer
language models. arXiv preprint arXiv:2404.15758, 2024.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are
unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.
Abulhair Saparov and He He. Language models are greedy reasoners: A systematic formal analysis of chain-of-thought.
arXiv preprint arXiv:2210.01240, 2022.
Yuval Shalev, Amir Feder, and Ariel Goldstein. Distributional reasoning in llms: Parallel reasoning processes in
multi-hop reasoning. arXiv preprint arXiv:2406.13858, 2024.
Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, YK Li, Yu Wu, and Daya
Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint
arXiv:2402.03300, 2024.
DiJia Su, Sainbayar Sukhbaatar, Michael Rabbat, Yuandong Tian, and Qinqing Zheng. Dualformer: Controllable fast
and slow thinking by learning with randomized reasoning traces. arXiv preprint arXiv:2410.09918, 2024.
Miles Turpin, Julian Michael, Ethan Perez, and Samuel Bowman. Language models don’t always say what they think:
unfaithful explanations in chain-of-thought prompting. Advances in Neural Information Processing Systems, 36,
2024.
Boshi Wang, Sewon Min, Xiang Deng, Jiaming Shen, You Wu, Luke Zettlemoyer, and Huan Sun. Towards understanding
chain-of-thought prompting: An empirical study of what matters. arXiv preprint arXiv:2212.10001, 2022.
Peiyi Wang, Lei Li, Zhihong Shao, Runxin Xu, Damai Dai, Yifei Li, Deli Chen, Yu Wu, and Zhifang Sui. Math-shepherd:
Verify and reinforce llms step-by-step without human annotations. In Proceedings of the 62nd Annual Meeting of the
Association for Computational Linguistics (Volume 1: Long Papers), pages 9426–9439, 2024.
Xinyi Wang, Lucas Caccia, Oleksiy Ostapenko, Xingdi Yuan, William Yang Wang, and Alessandro Sordoni. Guiding
language model reasoning with planning tokens. arXiv preprint arXiv:2310.05707, 2023.
13Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al.
Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing
systems, 35:24824–24837, 2022.
Yuxi Xie, Kenji Kawaguchi, Yiran Zhao, James Xu Zhao, Min-Yen Kan, Junxian He, and Michael Xie. Self-evaluation
guided beam search for reasoning. Advances in Neural Information Processing Systems, 36, 2023.
Sohee Yang, Elena Gribovskaya, Nora Kassner, Mor Geva, and Sebastian Riedel. Do large language models latently
perform multi-hop reasoning? arXiv preprint arXiv:2402.16837, 2024.
Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of
thoughts: Deliberate problem solving with large language models. Advances in Neural Information Processing
Systems, 36, 2023.
Fangxu Yu, Lai Jiang, Haoqiang Kang, Shibo Hao, and Lianhui Qin. Flow of reasoning: Efficient training of llm policy
with divergent thinking. arXiv preprint arXiv:2406.05673, 2024a.
Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T Kwok, Zhenguo Li, Adrian
Weller, and Weiyang Liu. Metamath: Bootstrap your own mathematical questions for large language models. arXiv
preprint arXiv:2309.12284, 2023.
Ping Yu, Jing Xu, Jason Weston, and Ilia Kulikov. Distilling system 2 into system 1. arXiv preprint arXiv:2407.06023,
2024b.
Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Mammoth: Building
math generalist models through hybrid instruction tuning. arXiv preprint arXiv:2309.05653, 2023.
Eric Zelikman, Georges Harik, Yijia Shao, Varuna Jayasiri, Nick Haber, and Noah D Goodman. Quiet-star: Language
models can teach themselves to think before speaking. arXiv preprint arXiv:2403.09629, 2024.
Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui,
Olivier Bousquet, Quoc Le, et al. Least-to-most prompting enables complex reasoning in large language models.
arXiv preprint arXiv:2205.10625, 2022.
14Appendix
ADatasets
A.1Examples
We provide some examples of the questions and CoT solutions for the datasets used in our experiments.
GSM8k
Question = "John cuts his grass to 2 inches. It grows .5 inches per month. When it gets to 4
inches he cuts it back down to 2 inches. It cost $100 to get his grass cut. How much does he pay
per year?"
Steps = ["«4-2=2»", "«2/.5=4»", "«12/4=3»", "«100*3=300»"]
Answer = "300"
ProntoQA
Question = "Brimpuses are not luminous. Shumpuses are amenable. Each yumpus is a lorpus.
Gorpuses are shumpuses. Each zumpus is a grimpus. Gorpuses are rompuses. Dumpuses are
not floral. Lempuses are cold. Brimpuses are impuses. Every lorpus is floral. Every rompus
is transparent. Grimpuses are muffled. Rompuses are yumpuses. Rompuses are wumpuses.
Zumpuses are fast. Wumpuses are bitter. Every sterpus is orange. Each lorpus is a vumpus.
Yumpuses are feisty. Each yumpus is a lempus. Gorpuses are snowy. Zumpuses are gorpuses.
Every lorpus is a sterpus. Stella is a brimpus. Stella is a zumpus. True or false: Stella is not
floral."
Steps = ["Stella is a zumpus. Zumpuses are gorpuses.", "Stella is a gorpus. Gorpuses are
rompuses.", "Stella is a rompus. Rompuses are yumpuses.", "Stella is a yumpus. Each yumpus
is a lorpus.", "Stella is a lorpus. Every lorpus is floral.", "Stella is floral."]
Answer = "False"
ProsQA
Question = "Every shumpus is a rempus. Every shumpus is a yimpus. Every terpus is a fompus.
Every terpus is a gerpus. Every gerpus is a brimpus. Alex is a rempus. Every rorpus is a
scrompus. Every rorpus is a yimpus. Every terpus is a brimpus. Every brimpus is a lempus.
Tom is a terpus. Every shumpus is a timpus. Every yimpus is a boompus. Davis is a shumpus.
Every gerpus is a lorpus. Davis is a fompus. Every shumpus is a boompus. Every shumpus is a
rorpus. Every terpus is a lorpus. Every boompus is a timpus. Every fompus is a yerpus. Tom is
a dumpus. Every rempus is a rorpus. Is Tom a lempus or scrompus?"
Steps = ["Tom is a terpus.", "Every terpus is a brimpus.", "Every brimpus is a lempus."]
Answer = "Tom is a lempus."
A.2
Construction of ProsQA
To construct the dataset, we first compile a set of typical entity names, such as “Alex” and “Jack,” along
with fictional concept names like “lorpus” and “rorpus,” following the setting of ProntoQA (Saparov and He,
2022). Each problem is structured as a binary question: “Is [Entity] a [Concept A] or [Concept B]?” Assuming
[Concept A] is the correct answer, we build a directed acyclic graph (DAG) where each node represents an
entity or a concept. The graph is constructed such that a path exists from [Entity] to [Concept A] but not to
[Concept B].
Algorithm 1 describes the graph construction process. The DAG is incrementally built by adding nodes and
randomly connecting them with edges. To preserve the validity of the binary choice, with some probability, we
15# Nodes# EdgesLen. of Shortest Path# Shortest Paths
23.036.03.81.6
Table 2 Statistics of the graph structure in ProsQA.
enforce that the new node cannot simultaneously serve as a descendant to both node 0 and 1. This separation
maintains distinct families of nodes and balances their sizes to prevent model shortcuts.
After the graph is constructed, nodes without parents are assigned entity names, while other nodes receive
concept names. To formulate a question of the form “Is [Entity] a [Concept A] or [Concept B]?”, we designate
node 0 in the graph as [Entity], a leaf node labeled 1 as [Concept A], and a leaf node labeled 2 as [Concept B].
This setup ensures a path from [Entity] to [Concept A] without any connection to [Concept B], introducing a
moderately complex reasoning path. Finally, to avoid positional biases, [Concept A] and [Concept B] are
randomly permuted in each question.
Algorithm 1 Graph Construction for ProsQA
edges ← {}
nodes ← {0, 1}
labels ← {0 : 1, 1 : 2}
▷ Labels: 1 (descendant of node 0), 2 (descendant of node 1), 3 (both), 0 (neither).
groups ← {0 : {}, 1 : {0}, 2 : {1}, 3 : {}}
idx ← 2
while idx < N do
▷ For each new node, randomly add edges from existing nodes
n_in_nodes ← poisson(1.5)
rand ← random()
if rand ≤ 0.35 then
candidates ← groups[0] ∪ groups[1]
▷ Cannot be a descendant of node 1.
else if rand ≤ 0.7 then
candidates ← groups[0] ∪ groups[2]
▷ Cannot be a descendant of node 0.
else
candidates ← nodes
end if
n_in_nodes ← min(len(candidates), n_in_nodes)
weights ← [depth_to_root(c) · 1.5 + 1 ∀c ∈ candidates]
▷ Define sampling weights to prioritize deeper nodes.
▷ This way, the solution reasoning chain is expected to be longer.
in_nodes ← random_choice(candidates, n_in_nodes, prob = weights/sum(weights))
cur_label ← 0
for in_idx ∈ in_nodes do
cur_label ← cur_label | labels[in_idx]
▷ Update label using bitwise OR.
edges.append((in_idx, idx))
end for
groups[cur_label].append(idx)
labels[idx] ← cur_label
nodes ← nodes ∪ {idx}
idx ← idx + 1
end while
A.3
Statistics
We show the size of all datasets in Table 3.
16DatasetTrainingValidationTest
GSM8k
ProntoQA
ProsQA385,620
9,000
17,886500
200
3001319
800
500
Table 3 Statistics of the datasets.
B
Clock-Time Reasoning Efficiency Metric
We present a clock-time comparison to evaluate reasoning efficiency. The reported values represent the average
inference time per test case (in seconds), with a batch size of 1, measured on an Nvidia A100 GPU. For the
no-CoT and CoT baselines, we employ the standard generate method from the transformers3 library. Our
results show that clock time is generally proportional to the number of newly generated tokens, as detailed in
Table 1.
MethodGSM8kProntoQAProsQA
No-CoT
CoT
Coconut0.03
0.26
0.090.03
0.85
0.110.08
0.47
0.15
Table 4 Inference time (in seconds) comparison across tasks and methods.
C
Using More Continuous Thoughts
In Figure 3, we present the performance of Coconut on GSM8k using c ∈ {0, 1, 2}. When experimenting
with c = 3, we observe a slight performance drop accompanied by increased variance. Analysis of the training
logs indicates that adding three continuous thoughts at once – particularly during the final stage transition –
leads to a sharp spike in training loss, causing instability. Future work will explore finer-grained schedules,
such as incrementally adding continuous thoughts one at a time while removing fewer language tokens, as
in iCoT (Deng et al., 2024). Additionally, combining language and latent reasoning—e.g., generating the
reasoning skeleton in language and completing the reasoning process in latent space—could provide a promising
direction for improving performance and stability.
3 https://github.com/huggingface/transformers


#=======
Hierarchical Reasoning Model
Guan Wang1,† , Jin Li1 , Yuhao Sun1 , Xing Chen1 , Changling Liu1 ,
Yue Wu1 , Meng Lu1,† , Sen Song2,† , Yasin Abbasi Yadkori1,†
1
Sapient Intelligence, Singapore
Abstract
ARC-AGI-2
1
0
0.9
20
1.3
0.0
Chain-of-thought, pretrained
0
0.0 0.0 0.0 0.0
40
20
0
0.0 0.0 0.0 0.0
HRM
2
60
40
3.0
Direct pred
o3-mini-high
Claude 3.7 8K
Deepseek R1
4
HRM
5
3
HRM
0
Claude 3.7 8K
10
Direct pred
20 15.8
21.0 21.2
o3-mini-high
30
Deepseek R1
Accuracy %
34.5
Direct pred
o3-mini-high
Claude 3.7 8K
Deepseek R1
40.3
40
Sudoku-Extreme (9x9) Maze-Hard (30x30)
1120 training examples 1000 training examples 1000 training examples
5.0 60
55.0 80
74.5
HRM
ARC-AGI-1
960 training examples
Direct pred
Claude 3.7 8K
Deepseek R1
o3-mini-high
arXiv:2506.21734v3 [cs.AI] 4 Aug 2025
Reasoning, the process of devising and executing complex goal-oriented action sequences,
remains a critical challenge in AI. Current large language models (LLMs) primarily employ
Chain-of-Thought (CoT) techniques, which suffer from brittle task decomposition, extensive
data requirements, and high latency. Inspired by the hierarchical and multi-timescale pro-
cessing in the human brain, we propose the Hierarchical Reasoning Model (HRM), a novel
recurrent architecture that attains significant computational depth while maintaining both train-
ing stability and efficiency. HRM executes sequential reasoning tasks in a single forward pass
without explicit supervision of the intermediate process, through two interdependent recurrent
modules: a high-level module responsible for slow, abstract planning, and a low-level mod-
ule handling rapid, detailed computations. With only 27 million parameters, HRM achieves
exceptional performance on complex reasoning tasks using only 1000 training samples. The
model operates without pre-training or CoT data, yet achieves nearly perfect performance on
challenging tasks including complex Sudoku puzzles and optimal path finding in large mazes.
Furthermore, HRM outperforms much larger models with significantly longer context windows
on the Abstraction and Reasoning Corpus (ARC), a key benchmark for measuring artificial
general intelligence capabilities. These results underscore HRM’s potential as a transformative
advancement toward universal computation and general-purpose reasoning systems.
Direct prediction, small-sample learning
Figure 1: Left: HRM is inspired by hierarchical processing and temporal separation in the brain. It
has two recurrent networks operating at different timescales to collaboratively solve tasks. Right:
With only about 1000 training examples, the HRM (~27M parameters) surpasses state-of-the-art
CoT models on inductive benchmarks (ARC-AGI) and challenging symbolic tree-search puzzles
(Sudoku-Extreme, Maze-Hard) where CoT models failed completely. The HRM was randomly
initialized, and it solved the tasks directly from inputs without chain of thoughts.
2
Tsinghua University † Corresponding author. Contact: research@sapient.inc.
Code available at: github.com/sapientinc/HRM
11
Introduction
Deep learning, as its name suggests, emerged from the idea of stacking more layers to achieve
increased representation power and improved performance 1,2 . However, despite the remarkable
success of large language models, their core architecture is paradoxically shallow 3 . This imposes
a fundamental constraint on their most sought-after capability: reasoning. The fixed depth of stan-
dard Transformers places them in computational complexity classes such as AC 0 or T C 0 4 , prevent-
ing them from solving problems that require polynomial time 5,6 . LLMs are not Turing-complete
and thus they cannot, at least in a purely end-to-end manner, execute complex algorithmic rea-
soning that is necessary for deliberate planning or symbolic manipulation tasks 7,8 . For example,
our results on the Sudoku task show that increasing Transformer model depth can improve per-
formance,1 but performance remains far from optimal even with very deep models (see Figure 2),
which supports the conjectured limitations of the LLM scaling paradigm 9 .
The LLMs literature has relied largely on Chain-of-Thought (CoT) prompting for reasoning 10 .
CoT externalizes reasoning into token-level language by breaking down complex tasks into sim-
pler intermediate steps, sequentially generating text using a shallow model 11 . However, CoT for
reasoning is a crutch, not a satisfactory solution. It relies on brittle, human-defined decompositions
where a single misstep or a misorder of the steps can derail the reasoning process entirely 12,13 . This
dependency on explicit linguistic steps tethers reasoning to patterns at the token level. As a result,
CoT reasoning often requires significant amount of training data and generates a large number of
tokens for complex reasoning tasks, resulting in slow response times. A more efficient approach is
needed to minimize these data requirements 14 .
Towards this goal, we explore “latent reasoning”, where the model conducts computations within
its internal hidden state space 15,16 . This aligns with the understanding that language is a tool for
human communication, not the substrate of thought itself 17 ; the brain sustains lengthy, coherent
chains of reasoning with remarkable efficiency in a latent space, without constant translation back
to language. However, the power of latent reasoning is still fundamentally constrained by a model’s
effective computational depth. Naively stacking layers is notoriously difficult due to vanishing gra-
dients, which plague training stability and effectiveness 1,18 . Recurrent architectures, a natural al-
ternative for sequential tasks, often suffer from early convergence, rendering subsequent computa-
tional steps inert, and rely on the biologically implausible, computationally expensive and memory
intensive Backpropagation Through Time (BPTT) for training 19 .
The human brain provides a compelling blueprint for achieving the effective computational depth
that contemporary artificial models lack. It organizes computation hierarchically across corti-
cal regions operating at different timescales, enabling deep, multi-stage reasoning 20,21,22 . Recur-
rent feedback loops iteratively refine internal representations, allowing slow, higher-level areas to
guide, and fast, lower-level circuits to execute—subordinate processing while preserving global
coherence 23,24,25 . Notably, the brain achieves such depth without incurring the prohibitive credit-
assignment costs that typically hamper recurrent networks from backpropagation through time 19,26 .
Inspired by this hierarchical and multi-timescale biological architecture, we propose the Hierar-
chical Reasoning Model (HRM). HRM is designed to significantly increase the effective compu-
tational depth. It features two coupled recurrent modules: a high-level (H) module for abstract,
deliberate reasoning, and a low-level (L) module for fast, detailed computations. This structure
1
Simply increasing the model width does not improve performance here.
2Accuracy %
100
100
Scaling Width - 8 layers fixed
Scaling Depth - 512 hidden size fixed
80
80
6060
4040
2020
27M
54M
109M
218M
Parameters
436M
Transformer
Recurrent Transformer
HRM
872M
8
16
32
64
128
256
Depth / Transformer layers computed
512
Figure 2: The necessity of depth for complex reasoning. Left: On Sudoku-Extreme Full, which
require extensive tree-search and backtracking, increasing a Transformer’s width yields no perfor-
mance gain, while increasing depth is critical. Right: Standard architectures saturates, failing to
benefit from increased depth. HRM overcomes this fundamental limitation, effectively using its
computational depth to achieve near-perfect accuracy.
avoids the rapid convergence of standard recurrent models through a process we term “hierarchi-
cal convergence.” The slow-updating H-module advances only after the fast-updating L-module
has completed multiple computational steps and reached a local equilibrium, at which point the
L-module is reset to begin a new computational phase.
Furthermore, we propose a one-step gradient approximation for training HRM, which offers im-
proved efficiency and eliminates the requirement for BPTT. This design maintains a constant mem-
ory footprint (O(1) compared to BPTT’s O(T ) for T timesteps) throughout the backpropagation
process, making it scalable and more biologically plausible.
Leveraging its enhanced effective depth, HRM excels at tasks that demand extensive search and
backtracking. Using only 1,000 input-output examples, without pre-training or CoT supervi-
sion, HRM learns to solve problems that are intractable for even the most advanced LLMs. For
example, it achieves near-perfect accuracy in complex Sudoku puzzles (Sudoku-Extreme Full) and
optimal pathfinding in 30x30 mazes, where state-of-the-art CoT methods completely fail (0% ac-
curacy). In the Abstraction and Reasoning Corpus (ARC) AGI Challenge 27,28,29 - a benchmark
of inductive reasoning - HRM, trained from scratch with only the official dataset (~1000 exam-
ples), with only 27M parameters and a 30x30 grid context (900 tokens), achieves a performance
of 40.3%, which substantially surpasses leading CoT-based models like o3-mini-high (34.5%)
and Claude 3.7 8K context (21.2%), despite their considerably larger parameter sizes and con-
text lengths, as shown in Figure 1. This represents a promising direction toward the development
of next-generation AI reasoning systems with universal computational capabilities.
2
Hierarchical Reasoning Model
We present the HRM, inspired by three fundamental principles of neural computation observed in
the brain:
• Hierarchical processing: The brain processes information across a hierarchy of cortical ar-
eas. Higher-level areas integrate information over longer timescales and form abstract repre-
sentations, while lower-level areas handle more immediate, detailed sensory and motor process-
ing 20,22,21 .
3• Temporal Separation: These hierarchical levels in the brain operate at distinct intrinsic timescales,
reflected in neural rhythms (e.g., slow theta waves, 4–8 Hz and fast gamma waves, 30–100
Hz) 30,31 . This separation allows for stable, high-level guidance of rapid, low-level computa-
tions 32,33 .
• Recurrent Connectivity: The brain features extensive recurrent connections. These feedback
loops enable iterative refinement, yielding more accurate and context-sensitive representations
at the cost of additional processing time. Additionally, the brain largely avoids the problematic
deep credit assignment problem associated with BPTT 19 .
The HRM model consists of four learnable components: an input network fI (·; θI ), a low-level re-
current module fL (·; θL ), a high-level recurrent module fH (·; θH ), and an output network fO (·; θO ).
The model’s dynamics unfold over N high-level cycles of T low-level timesteps each2 . We index
the total timesteps of one forward pass by i = 1, . . . , N × T . The modules fL and fH each keep a
i
0
hidden state—zLi for fL and zH
for fH —which are initialized with the vectors zL0 and zH
, respec-
tively.
The HRM maps an input vector x to an output prediction vector ŷ as follows. First, the input x is
projected into a working representation x̃ by the input network:
x̃ = fI (x; θI ) .
At each timestep i, the L-module updates its state conditioned on its own previous state, the H-
module’s current state (which remains fixed throughout the cycle), and the input representation.
The H-module only updates once per cycle (i.e., every T timesteps) using the L-module’s final
state at the end of that cycle:

i−1
zLi = fL zLi−1 , zH
, x̃; θL ,
(

i−1 i−1
fH zH
, zL ; θH
if i ≡ 0 (mod T ) ,
i
zH =
i−1
zH
otherwise .
Finally, after N full cycles, a prediction ŷ is extracted from the hidden state of the H-module:
NT
ŷ = fO (zH
; θO ) .
This entire N T -timestep process represents a single forward pass of the HRM. A halting mecha-
nism (detailed later in this section) determines whether the model should terminate, in which case
ŷ will be used as the final prediction, or continue with an additional forward pass.
Hierarchical convergence Although convergence is crucial for recurrent networks, standard RNNs
are fundamentally limited by their tendency to converge too early. As the hidden state settles toward
a fixed point, update magnitudes shrink, effectively stalling subsequent computation and capping
the network’s effective depth. To preserve computational power, we actually want convergence to
proceed very slowly–but engineering that gradual approach is difficult, since pushing convergence
too far edges the system toward instability.
2
While inspired by temporal separation in the brain, our model’s “high-level” and “low-level” modules are concep-
tual abstractions and do not map directly to specific neural oscillation frequencies.
40
20
40
60
Step Index #
60
250
200
150
100
50
0
30
Principal Components
250
200
150
100
50
0
Recurrent Neural Net
0
20
40
Step Index #
60
60
Layer Index #
HRM H
HRM L
Step Index #
Forward residual
Step Index #
250
200
150
100
50
0
30
Principal Components
Deep Neural Net
0
100
200
Layer Index #
200
100
Principal Components
Figure 3: Comparison of forward residuals and PCA trajectories. HRM shows hierarchical conver-
gence: the H-module steadily converges, while the L-module repeatedly converges within cycles
before being reset by H, resulting in residual spikes. The recurrent neural network exhibits rapid
convergence with residuals quickly approaching zero. In contrast, the deep neural network experi-
ences vanishing gradients, with significant residuals primarily in the initial (input) and final layers.
HRM is explicitly designed to counteract this premature convergence through a process we term
hierarchical convergence. During each cycle, the L-module (an RNN) exhibits stable convergence
to a local equilibrium. This equilibrium, however, depends on the high-level state zH supplied
during that cycle. After completing the T steps, the H-module incorporates the sub-computation’s
outcome (the final state zL ) and performs its own update. This zH update establishes a fresh context
for the L-module, essentially “restarting” its computational path and initiating a new convergence
phase toward a different local equilibrium.
This process allows the HRM to perform a sequence of distinct, stable, nested computations, where
the H-module directs the overall problem-solving strategy and the L-module executes the intensive
search or refinement required for each step. Although a standard RNN may approach convergence
within T iterations, the hierarchical convergence benefits from an enhanced effective depth of N T
steps. As empirically shown in Figure 3, this mechanism allows HRM both to maintain high
computational activity (forward residual) over many steps (in contrast to a standard RNN, whose
activity rapidly decays) and to enjoy stable convergence. This translates into better performance at
any computation depth, as illustrated in Figure 2.
Approximate gradient Recurrent models typically use BPTT to compute gradients. However,
BPTT requires storing the hidden states from the forward pass and then combining them with
gradients during the backward pass, which demands O(T ) memory for T timesteps. This heavy
memory burden forces smaller batch sizes and leads to poor GPU utilization, especially for large-
scale networks. Additionally, because retaining the full history trace through time is biologically
implausible, it is unlikely that the brain implements BPTT 19 .
Fortunately, if a recurrent neural network converges to a fixed point, we can avoid unrolling its state
sequence by applying backpropagation in a single step at that equilibrium point. Moreover, such a
mechanism could plausibly be implemented in the brain using only local learning rules 34,35 . Based
5on this finding, we propose a one-step approximation of the HRM gradient–using the gradient of
the last state of each module and treating other states as constant. The gradient path is, therefore,
Output head → final state of the H-module → final state of the L-module → input embedding
The above method needs O(1) memory, does not require unrolling through time, and can be easily
implemented with an autograd framework such as PyTorch, as shown in Figure 4. Given that
each module only needs to back-propagate errors through its most recent local synaptic activity,
this approach aligns well with the perspective that cortical credit assignment relies on short-range,
temporally local mechanisms rather than on a global replay of activity patterns.
The one-step gradient approximation is theoretically
grounded in the mathematics of Deep Equilibrium Mod-
els (DEQ) 36 which employs the Implicit Function Theo-
rem (IFT) to bypass BPTT, as detailed next. Consider an
idealized HRM behavior where, during high-level cycle
k, the L-module repeatedly updates until its state zL con-
verges to a local fixed point zL⋆ . This fixed point, given
k−1
the current high-level state zH
, can be expressed as
k−1
zL⋆ = fL (zL⋆ , zH
, x̃; θL ) .
The H-module then performs a single update using this
converged L-state:
k−1 ⋆
k
zH
= fH (zH
, zL ; θH ) .
def hrm(z, x, N=2, T=2):
x = input_embedding(x)
zH, zL = z
with torch.no_grad():
for _i in range(N ∗ T − 1):
zL = L_net(zL, zH, x)
if (_i + 1) % T == 0:
zH = H_net(zH, zL)
# 1−step grad
zL = L_net(zL, zH, x)
zH = H_net(zH, zL)
return (zH, zL), output_head(zH)
# Deep Supervision
for x, y_true in train_dataloader:
z = z_init
for step in range(N_supervision):
z, y_hat = hrm(z, x)
With a proper mapping F, the updates to the high-level
k
=
state can be written in a more compact form as zH
k−1
loss = softmax_cross_entropy(y_hat, y_true)
F(zH ; x̃, θ), where θ = (θI , θL ), and the fixed-point
z = z.detach()
∂F
⋆
⋆
can be written as zH = F(zH ; x̃, θ). Let JF = ∂zH be
loss.backward()
opt.step()
the Jacobian of F, and assume that the matrix I − JF is
opt.zero_grad()
⋆
invertible at zH and that the mapping F is continuously
differentiable. The Implicit Function Theorem then al- Figure 4: Top: Diagram of HRM with
⋆
approximate gradient. Bottom: Pseu-
lows us to calculate the exact gradient of fixed point zH
with respect to the parameters θ without explicit back- docode of HRM with deep supervision
training in PyTorch.
propagation:

−1 ∂F
⋆
∂zH
= I − JF z⋆
.
(1)
H
∂θ
∂θ z⋆
H
Calculating the above gradient requires evaluating and inverting matrix (I − JF ) that can be com-
putationally expensive. Given the Neumann series expansion,
(I − JF )−1 = I + JF + JF2 + JF3 + . . . ,
the so-called 1-step gradient 37 approximates the series by considering only its first term, i.e. (I −
JF )−1 ≈ I, and leads to the following approximation of Equation (1):
∗
∂fH
∂zH
≈
,
∂θH
∂θH
∗
∂zH
∂fH ∂zL∗
≈
·
,
∂θL
∂zL∗ ∂θL
∗
∂zH
∂fH ∂zL∗
≈
·
.
∂θI
∂zL∗ ∂θI
(2)
6∂z ∗
∂z ∗
The gradients of the low-level fixed point, ∂θLL and ∂θLI , can also be approximated using another
application of the 1-step gradient:
∂zL∗
∂fL
≈
,
∂θL
∂θL
∂zL∗
∂fL
≈
.
∂θI
∂θI
(3)
By substituting Equation (3) back into Equation (2), we arrive at the final simplified gradients.
Before defining our loss function, we must first introduce two key elements of our proposed
method: deep supervision and adaptive computational time.
Deep supervision Inspired by the principle that periodic neural oscillations regulate when learning
occurs in the brain 38 , we incorporate a deep supervision mechanism into HRM, as detailed next.
Given a data sample (x, y), we run multiple forward passes of the HRM model, each of which we
refer to as a segment. Let M denote the total number of segments executed before termination.
mN T
For each segment m ∈ {1, . . . , M }, let z m = (zH
, zLmN T ) represent the hidden state at the
conclusion of segment m, encompassing both high-level and low-level state components.
At each segment m, we apply a deep supervision step as follows:
1. Given the state z m−1 from the previous segment, compute the next state z m and its associated
output ŷ m through a forward pass in the HRM model:
(z m , ŷ m ) ← HRM(z m−1 , x; θ)
2. Compute the loss for the current segment:
Lm ← L OSS(ŷ m , y)
3. Update parameters:
θ ← O PTIMIZER S TEP(θ, ∇θ Lm )
The crucial aspect of this procedure is that the hidden state z m is “detached” from the computa-
tion graph before being used as the input state for the next segment. Consequently, gradients from
segment m + 1 do not propagate back through segment m, effectively creating a 1-step approxi-
mation of the gradient of the recursive deep supervision process 39,40 . This approach provides more
frequent feedback to the H-module and serves as a regularization mechanism, demonstrating supe-
rior empirical performance and enhanced stability in deep equilibrium models when compared to
more complex, Jacobian-based regularization techniques 39,41 . Figure 4 shows pseudocode of deep
supervision training.
Adaptive computational time (ACT) The brain dynamically alternates between automatic think-
ing (“System 1”) and deliberate reasoning (“System 2”) 42 . Neuroscientific evidence shows that
these cognitive modes share overlapping neural circuits, particularly within regions such as the
prefrontal cortex and the default mode network 43,44 . This indicates that the brain dynamically mod-
ulates the “runtime” of these circuits according to task complexity and potential rewards 45,46 .
Inspired by the above mechanism, we incorporate an adaptive halting strategy into HRM that en-
ables “thinking, fast and slow”. This integration leverages deep supervision and uses the Q-learning
7algorithm 47 to adaptively determine the number of segments. A Q-head uses the final state of the
m
H-module to predict the Q-values Q̂m = (Q̂m
halt , Q̂continue ) of the “halt” and “continue” actions:
⊤ mN T
Q̂m = σ(θQ
zH ) ,
where σ denotes the sigmoid function applied element-wise. The halt or continue action is chosen
using a randomized strategy as detailed next. Let Mmax denote the maximum number of segments
(a fixed hyperparameter) and Mmin denote the minimum number of segments (a random variable).
The value of Mmin is determined stochastically: with probability ε, it is sampled uniformly from the
set {2, · · · , Mmax } (to encourage longer thinking), and with probability 1 − ε, it is set to 1. The halt
action is selected under two conditions: when the segment count surpasses the maximum threshold
Mmax , or when the estimated halt value Q̂halt exceeds the estimated continue value Q̂continue and the
segment count has reached at least the minimum threshold Mmin .
The Q-head is updated through a Q-learning algorithm, which is defined on the following episodic
Markov Decision Process (MDP). The state of the MDP at segment m is z m , and the action space
is {halt, continue}. Choosing the action “halt” terminates the episode and returns a binary reward
indicating prediction correctness, i.e., 1{ŷ m = y}. Choosing “continue” yields a reward of 0 and
m
the state transitions to z m+1 . Thus, the Q-learning targets for the two actions Ĝm = (Ĝm
halt , Ĝcontinue )
are given by
m
Ĝm
halt = 1{ŷ = y} ,

Q̂m+1
if m ≥ Nmax ,
halt ,
m
Ĝcontinue =
max(Q̂m+1 , Q̂m+1 ) , otherwise .
halt
continue
We can now define the loss function of our learning procedure. The overall loss for each supervision
segment combines both the Q-head loss and the sequence-to-sequence loss:
m
m
m
Lm
ACT = L OSS (ŷ , y) + B INARY C ROSS E NTROPY (Q̂ , Ĝ ) .
Minimizing the above loss enables both accurate predictions and nearly optimal stopping decisions.
Selecting the “halt” action ends the supervision loop. In practice, sequences are processed in
batches, which can be easily handled by substituting any halted sample in the batch with a fresh
sample from the dataloader.
Figure 5 presents a performance comparison between two HRM variants: one incorporating ACT
and another employing a fixed computational step count equivalent to ACT’s Mmax parameter. It
shows that ACT effectively adapts its computational resources based on task complexity, achieving
significant computational savings with minimal impact on performance.
Inference-time scaling An effective neural model should exploit additional computational re-
sources during inference to enhance performance. As illustrated in Figure 5-(c), HRM seamlessly
achieves inference-time scaling by simply increasing the computational limit parameter, Mmax
without requiring further training or architectural modifications.
Additional compute is especially effective for tasks that demand deeper reasoning. On Sudoku—
a problem that often requires long-term planning—HRM exhibits strong inference-time scaling.
On the other hand, we find that extra computational resources yield minimal gains in ARC-AGI
challenge, as solutions generally require only a few transformations.
8(b) ACT Performance
2
4
M (Fixed) or Mmax (ACT)
8
100.0
97.5
95.0
92.5
90.0
87.5
85.0
82.5
(c) Inference-time scaling
Fixed M
ACT (Mmax limit)
Accuracy %
Fixed M
ACT (Mmax limit)
Accuracy %
Mean Compute Steps
(a) ACT Compute Spent
8
7
6
5
4
3
2
1
2
4
M (Fixed) or Mmax (ACT)
8
100.0
97.5
95.0
92.5
90.0
87.5
85.0
82.5
Train Mmax = 2
Train Mmax = 4
Train Mmax = 8
2
4
8
Inference Mmax
16
Figure 5: Effectiveness of Adaptive Computation Time (ACT) on the Sudoku-Extreme-Full. (a)
Mean compute steps used by models with ACT versus models with a fixed number of compute steps
(M ). ACT maintains a low and stable number of average compute steps even as the maximum limit
(Mmax ) increases. (b) Accuracy comparison. The ACT model achieves performance comparable
to the fixed-compute model while utilizing substantially fewer computational steps on average. (c)
Inference-time scalability. Models trained with a specific Mmax can generalize to higher compu-
tational limits during inference, leading to improved accuracy. For example, a model trained with
Mmax = 8 continues to see accuracy gains when run with Mmax = 16 during inference.
Stability of Q-learning in ACT The deep Q-learning that underpins our ACT mechanism is
known to be prone to instability, often requiring stabilization techniques such as replay buffers
and target networks 48 , which are absent in our design. Our approach, however, achieves stability
through the intrinsic properties of our model and training procedure. Recent theoretical work by
Gallici et al. 49 shows that Q-learning can achieve convergence if network parameters are bounded,
weight decay is incorporated during training, and post-normalization layers are implemented. Our
model satisfies these conditions through its Post-Norm architecture that employs RMSNorm (a
layer normalization variant) and the AdamW optimizer. AdamW has been shown to solve an L∞ -
constrained optimization problem, ensuring that model parameters remain bounded by 1/λ 50 .
Architectural details We employ a sequence-to-sequence architecture for HRM. Both input and
output are represented as token sequences: x = (x1 , . . . , xl ) and y = (y1 , . . . , yl′ ) respectively.
The model includes an embedding layer fI that converts discrete tokens into vector representa-
tions, and an output head fO (z; θO ) = softmax(θO z) that transforms hidden states into token prob-
ability distributions ŷ. For small-sample experiments, we replace softmax with stablemax 51 to
improve generalization performance. The sequence-to-sequence loss is averaged over all tokens,
Pl′
log p(yi ), where p(yi ) is the probability that distribution ŷi assigns to token
L OSS(ŷ, y) = l1′ i=1
yi . The initial hidden states z 0 are initialized by sampling from a truncated normal distribution with
standard deviation of 1, truncation of 2, and kept fixed throughout training.
Both the low-level and high-level recurrent modules fL and fH are implemented using encoder-
only Transformer 52 blocks with identical architectures and dimensions. These modules take mul-
tiple inputs, and we use straightforward element-wise addition to combine them, though more
sophisticated merging techniques such as gating mechanisms could potentially improve perfor-
mance and is left for future work. For all Transformer blocks in this work—including those in
the baseline models—we incorporate the enhancements found in modern LLMs (based on Llama 53
architectures). These improvements include Rotary Positional Encoding 54 , Gated Linear Units 55 ,
RMSNorm 56 , and the removal of bias terms from linear layers.
Furthermore, both HRM and recurrent Transformer models implement a Post-Norm architecture
98
4
5
8
3
6
7
4
3
8
4
2
6
3
8
9
6
5
2
2
(a) ARC-AGI
5
1
3
8
784125963
261389745
359647812
538496127
416273598
972851436
693518274
847962351
125734689
(b) Sudoku-Hard
(c) Maze navigation
(d) Sudoku-Extreme subset difficulty
Figure 6: Left: Visualization of benchmark tasks. Right: Difficulty of Sudoku-Extreme examples.
with weights initialized via truncated LeCun Normal initialization 57,58,59 , while the scale and bias
parameters are excluded from RMSNorm. All parameters are optimized using the Adam-atan2 op-
timizer 60 , a scale-invariant variant of Adam 61 , combined with a constant learning rate that includes
linear warm-up.
3
Results
This section begins by describing the ARC-AGI, Sudoku, and Maze benchmarks, followed by an
overview of the baseline models and their results. Figure 6-(a,b,c) presents a visual representa-
tion of the three benchmark tasks, which are selected to evaluate various reasoning abilities in AI
models.
3.1
Benchmarks
ARC-AGI Challenge The ARC-AGI benchmark evaluates general fluid intelligence through IQ-
test-like puzzles that require inductive reasoning 27 . The initial version, ARC-AGI-1, presents chal-
lenges as input-label grid pairs that force AI systems to extract and generalize abstract rules from
just a few examples. Each task provides a few input–output demonstration pairs (usually 2–3) and
a test input. An AI model has two attempts to produce the correct output grid. Although some be-
lieve that mastering ARC-AGI would signal true artificial general intelligence, its primary purpose
is to expose the current roadblocks in AGI progress. In fact, both conventional deep learning meth-
ods and CoT techniques have faced significant challenges with ARC-AGI-1, primarily because it
requires the ability to generalize to entirely new tasks 28 .
Addressing the limitations identified in ARC-AGI-1, ARC-AGI-2 significantly expands the bench-
mark by providing a more comprehensive and carefully refined collection of tasks. These new
tasks emphasize deeper compositional reasoning, multi-step logic, contextual rule application, and
symbolic abstraction. Human calibration studies show these tasks are challenging but doable for
people, while being much harder for current AI systems, offering a clearer measure of general
reasoning abilities 29 .
10Sudoku-Extreme Sudoku is a 9×9 logic puzzle, requiring each row, column, and 3×3 block to
contain the digits 1–9 exactly once. A prediction is considered correct if it exactly matches the
puzzle’s unique solution. Sudoku’s complex logical structure makes it a popular benchmark for
evaluating logical reasoning in machine learning 62,63,64 .
The most frequently used Sudoku dataset in research, namely the Kaggle dataset 65 , can be fully
solved using elementary single-digit techniques 66 . The minimal 17-clue puzzles 62 , another widely-
used collection, might seem more challenging due to its small number of clues. However, this
perception is misleading—since 17 represents the minimum number of clues required to guarantee
a unique Sudoku solution, these hints need to be highly orthogonal to each other. This orthogonal
arrangement leads to many direct, easily-resolved solution paths 67 .
We introduce Sudoku-Extreme, a more challenging dataset that is compiled from the aforemen-
tioned easy datasets as well as puzzles recognized by the Sudoku community as exceptionally
difficult for human players:
• Easy puzzles compiled from Kaggle, 17-clue, plus unbiased samples from the Sudoku puzzle
distribution 67 : totaling 1 149 158 puzzles.
• Challenging puzzles compiled from Magictour 1465, Forum-Hard and Forum-Extreme subsets:
totaling 3 104 157 puzzles.
The compiled data then undergo a strict 90/10 train-test split, ensuring that the test set puzzles
cannot be derived through equivalent transformations of any training samples. Sudoku-Extreme is
a down-sampled subset of this data containing 1000 training examples. We use Sudoku-Extreme in
our main experiments (Figure 1), which focuses on small-sample learning scenarios. To guarantee
convergence and control overfitting effects in our analysis experiments (Figures 2, 3 and 5), we use
the complete training data, Sudoku-Extreme-Full, containing 3 831 994 examples.
We measure puzzle difficulty by counting the number of search backtracks (“guesses”) required
by a smart Sudoku solver program tdoku, which uses propositional logic to reduce the number of
guesses 67 . Our Sudoku-Extreme dataset exhibits a mean difficulty of 22 backtracks per puzzle, sig-
nificantly higher than existing datasets, including recent handmade puzzles Sudoku-Bench 68 which
average just 0.45 backtracks per puzzle. These subset complexity levels are shown in Figure 6-(d).
Maze-Hard This task involves finding the optimal path in a 30×30 maze, making it interpretable
and frequently used for training LLMs in search tasks 69,70,71 . We adopt the instance generation
procedure of Lehnert et al. 71 , but introduce an additional filter to retain only those instances whose
difficulty exceeds 110. Here, “difficulty” is defined as the length of the shortest path, which aligns
with the linear time complexity of the wavefront breadth-first search algorithm on GPUs 72 . A path
is considered correct if it is valid and optimal—that is, the shortest route from the start to the goal.
The training and test set both include 1000 examples.
3.2
Evaluation Details
For all benchmarks, HRM models were initialized with random weights and trained in the sequence-
to-sequence setup using the input-output pairs. The two-dimensional input and output grids were
flattened and then padded to the maximum sequence length. The resulting performance is shown in
Figure 1. Remarkably, HRM attains these results with just ~1000 training examples per task—and
without pretraining or CoT labels.
11For ARC-AGI challenge, we start with (1) all demonstration and test input-label pairs from the
training set, and (2) all demonstration pairs along with test inputs from the evaluation set. The
dataset is augmented by applying translations, rotations, flips, and color permutations to the puz-
zles. Each task example is prepended with a learnable special token that represents the puzzle it
belongs to. At test time, we proceed as follows for each test input in the evaluation set: (1) Gener-
ate and solve 1000 augmented variants and, for each, apply the inverse-augmentation transform to
obtain a prediction. (2) Choose the two most popular predictions as the final outputs.3 All reported
results are obtained by comparing the outputs with the withheld test labels from the evaluation set.
We augment Sudoku puzzles by applying band and digit permutations, while data augmentation is
disabled for Maze tasks. Both tasks undergo only a single inference pass.
For ARC-AGI, the scores of the CoT models are taken from the official leaderboard 29 , while for
Sudoku and Maze, the scores are obtained by evaluating through the corresponding API.
In Figure 1, the baselines are grouped based on whether they are pre-trained and use CoT, or neither.
The “Direct pred” baseline means using “direct prediction without CoT and pre-training”, which
retains the exact training setup of HRM but swaps in a Transformer architecture. Interestingly, on
ARC-AGI-1, “Direct pred” matches the performance of Liao and Gu 73 , who built a carefully de-
signed, domain-specific equivariant network for learning the ARC-AGI task from scratch, without
pre-training. By substituting the Transformer architecture with HRM’s hierarchical framework and
implementing ACT, we achieve more than a twofold performance improvement.
On the Sudoku-Extreme and Maze-Hard benchmarks, the performance gap between HRM and the
baseline methods is significant, as the baselines almost never manage to solve the tasks. These
benchmarks that demand lengthy reasoning traces are particularly difficult for CoT-based methods.
With only 1000 training examples, the “Direct pred” baseline—which employs an 8-layer Trans-
former identical in size to HRM—fails entirely on these challenging reasoning problems. When
trained on the larger Sudoku-Extreme-Full dataset, however, “Direct pred” can solve some easy
Sudoku puzzles and reaches 16.9% accuracy (see Figure 2). Lehnert et al. 71 showed that a large
vanilla Transformer model with 175M parameters, trained on 1 million examples across multiple
trials, achieved only marginal success on 30x30 Maze tasks, with accuracy below 20% using the
pass@64 evaluation metric.
3.3
Visualization of intermediate timesteps
Although HRM demonstrates strong performance on complex reasoning tasks, it raises an intrigu-
ing question: what underlying reasoning algorithms does the HRM neural network actually imple-
ment? Addressing this question is important for enhancing model interpretability and developing a
deeper understanding of the HRM solution space.
While a definitive answer lies beyond our current scope, we begin our investigation by analyzing
state trajectories and their corresponding solution evolution. More specifically, at each timestep
i
i and given the low-level and high-level state pair (zLi and zH
) we perform a preliminary forward
i
i
i
pass through the H-module to obtain z̄ = fH (zH , zL ; θH ) and its corresponding decoded prediction
ȳ i = fO (z̄ i ; θO ). The prediction ȳ i is then visualized in Figure 7.
In the Maze task, HRM appears to initially explore several potential paths simultaneously, subse-
quently eliminating blocked or inefficient routes, then constructing a preliminary solution outline
3
The ARC-AGI allows two attempts for each test input.
12Timestep i = 0
4
7
Initial
Timestep i = 1
Timestep i = 0
2 4 3 5 7 1 8 9 6
6 7 8 6 3 4 1 5 4
2
6 5 1 2 7 9 7 3 4
67
8 3 4 8 6 7 2 1 2
3
4
7 2 8 3 1 8 6 4 8
1 64 23
15649237 9
27 3
8 1 2 7 9 3 4 6 6
46 12
468128 7 3 7
3 7
6
1 3979 964 21
3
89
1
[7666fa5d] Example Input
Timestep i = 2
Timestep i = 1
2 4 3 5 7 1 8 9 6
6 7 8 6 3 4 1 5 4
6 5 1 2 8 9 7 3 4
5 3 4 8 6 7 2 1 9
7 2 5 3 1 5 6 4 8
19645237 7
9 1 2 7 4 3 9 8 6
465128 9 7 3
3875 564 21
[7666fa5d] Example Output
Timestep i = 2
2 4 3 5 7 1 8 9 3
8 7 9 6 3 4 1 5 2
6 5 1 2 8 8 7 3 4
5 3 4 8 6 7 2 1 5
7 2 5 3 1 5 6 4 9
19645237 7
9 1 2 7 4 3 5 6 8
465128 9 7 3
3879 564 21
[7666fa5d] Test Input
[7b80bb43] Test Input
[7b80bb43] Example Input
Timestep i = 3
Timestep i = 3
2 4 3 5 7 1 8 9 3
8 7 9 6 3 4 1 5 2
6 5 1 2 8 9 7 3 4
5 3 4 8 6 7 2 1 5
7 2 5 3 1 1 6 4 6
19645237 7
9 1 2 7 4 3 5 6 8
468128 9 7 3
3879 564 21
Timestep i = 0
Timestep i = 0
Timestep i = 4
Timestep i = 1
Timestep i = 4
2 4 1 6 7 5 8 9 3
6 7 9 6 3 4 1 5 2
6 5 3 2 1 8 7 6 4
5 3 4 9 6 7 2 1 5
7 2 5 3 8 1 6 4 6
19645238 7
9 1 2 7 4 3 5 6 8
468128 9 7 3
3875 864 21
Timestep i = 1
Timestep i = 2
Timestep i = 5
Timestep i = 5
2 4 1 6 7 5 8 9 3
6 7 9 9 3 4 1 5 2
9 5 1 2 8 8 7 3 4
5 3 4 8 6 7 2 1 8
7 2 5 3 1 1 6 4 6
19645237 7
9 1 2 7 4 3 5 8 6
468128 9 3 7
3875 864 21
Timestep i = 6
2 4 1 6 7 5 8 9 3
6 7 9 8 3 4 1 5 2
8 5 3 2 1 9 7 6 4
5 3 4 9 6 7 2 1 8
7 2 8 3 8 1 6 4 6
19648237 5
9 1 2 7 4 3 5 8 6
465128 9 3 7
3875 964 21
Timestep i = 2
Timestep i = 3
Timestep i = 6
Timestep i = 3
Timestep i = 4
Timestep i = 7
2 4 1 6 7 5 8 9 3
6 7 9 8 3 4 1 5 2
8 5 3 2 1 9 7 6 4
5 3 4 9 6 7 2 1 8
7 2 8 3 5 1 6 4 9
19648237 5
9 1 2 7 4 3 5 8 6
465128 9 3 7
3875 964 21
Timestep i = 4
Timestep i = 5
Timestep i = 6
[7b80bb43] Example Output
Figure 7: Visualization of intermediate predictions by HRM on benchmark tasks. Top: Maze-
Hard—blue cells indicate the predicted path. Middle: Sudoku-Extreme—bold cells represent ini-
tial givens; red highlights cells violating Sudoku constraints; grey shading indicates changes from
the previous timestep. Bottom: ARC-AGI-2 Task—left: provided example input-output pair; right:
intermediate steps solving the test input.
followed by multiple refinement iterations. In Sudoku, the strategy resembles a depth-first search
approach, where the model appears to explore potential solutions and backtracks when it hits dead
ends. HRM uses a different approach for ARC tasks, making incremental adjustments to the board
and iteratively improving it until reaching a solution. Unlike Sudoku, which involves frequent
backtracking, the ARC solution path follows a more consistent progression similar to hill-climbing
optimization.
Importantly, the model shows that it can adapt to different reasoning approaches, likely choosing an
effective strategy for each particular task. Further research is needed to gain more comprehensive
insights into these solution strategies.
4
Brain Correspondence
A key principle from systems neuroscience is that a brain region’s functional repertoire—its ability
to handle diverse and complex tasks—is closely linked to the dimensionality of its neural represen-
tations 75,76 . Higher-order cortical areas, responsible for complex reasoning and decision-making,
must handle a wide variety of tasks, demanding more flexible and context-dependent processing 77 .
In dynamical systems, this flexibility is often realized through higher-dimensional state-space tra-
jectories, which allow for a richer repertoire of potential computations 78 . This principle gives rise
to an observable dimensionality hierarchy, where a region’s position in the processing hierarchy
13(a)
Participation Ratio (PR)
(b)
5.0
(c)(e)
(d)(f)
4.5
4.0
3.5
3.0
2.5
2.0
0
20
40
Position in the hierarchy
Figure 8: Hierarchical Dimensionality Organization in the HRM and Mouse Cortex. (a,b) are
adapted from Posani et al. 74 . (a) Anatomical illustration of mouse cortical areas, color-coded by
functional modules. (b) Correlation between Participation Ratio (PR), a measure of effective neural
dimensionality, and hierarchical position across different mouse cortical areas. Higher positions in
the hierarchy (e.g., MOs, ACAd) exhibit significantly higher PR values compared to lower sensory
areas (e.g., SSp-n), with a Spearman correlation coefficient of ρ = 0.79 (P = 0.0003). (c,d) Trained
HRM. (c) PR scaling of the trained HRM with task diversity. The dimensionality of the high-
level module (zH ) scales with the number of unique tasks (trajectories) included in the analysis,
indicating an adaptive expansion of its representational capacity. In contrast, the low-level module’s
(zL ) dimensionality remains stable. (d) PR values for the low-level (zL , PR = 30.22) and high-
level (zH , PR = 89.95) modules of the trained HRM, computed from neural activity during 100
unique Sudoku-solving trajectories. A clear dimensionality hierarchy is observed, with the high-
level module operating in a substantially higher-dimensional space. (e,f) Analysis of Untrained
Network. To verify that the dimensionality hierarchy is an emergent property of training, the same
analyses were performed on an untrained HRM with random weights. (e) In contrast to the trained
model’s scaling in (c), the dimensionality of both modules in the untrained model remains low and
stable, failing to scale with the number of tasks. (f) Similarly, contrasting with the clear separation
in (d), the PR values for the untrained model’s modules (zL , PR = 42.09; zH , PR = 40.75) are
low and nearly identical, showing no evidence of hierarchical separation. This confirms that the
observed hierarchical organization of dimensionality is a learned property that emerges through
training, not an artifact of the model’s architecture.
14correlates with its effective dimensionality. To quantify this phenomenon, we can examine the
Participation Ratio (PR), which serves as a standard measure of the effective dimensionality of a
high-dimensional representation 79 . The PR is calculated using the formula
P
( i λi )2
PR = P 2 ,
i λi
where {λi } are the eigenvalues of the covariance matrix of neural trajectories. Intuitively, a higher
PR value signifies that variance is distributed more evenly across many dimensions, corresponding
to a higher-dimensional representation. Conversely, a lower PR value indicates that variance is
concentrated in only a few principal components, reflecting a more compact, lower-dimensional
structure.
The dimensionality hierarchy can be observed, for example, in the mouse cortex, where the PR of
population activity increases monotonically from low-level sensory areas to high-level associative
areas, supporting this link between dimensionality and functional complexity 74 (Figure 8 (a,b)).
We evaluated whether HRM reproduces this neuroscientific principle by calculating the PR for
both recurrent modules after training on the Sudoku-Extreme Full dataset. The PR computation
used the covariance matrix derived from neural states gathered across multiple Sudoku-solving
trajectories. The results show a striking parallel to the biological findings. The low-level module’s
state (zL ) occupies a relatively small subspace with a participation ratio of 30.22, whereas the high-
level module’s state (zH ) operates in a substantially larger subspace with a participation ratio of
89.95, as shown in Figure 8(c). Furthermore, Figure 8(d) shows that increasing the number of
unique tasks (trajectories) from 10 to 100 causes zH dimensionality to scale up accordingly, while
zL dimensionality remains stable. These results suggest an emergent separation of representational
capacity between the modules that parallels their functional roles.
To confirm that this hierarchical organization is an emergent property of training, and not an artifact
of the network’s architecture, we performed a control analysis using an identical but untrained
network with random weights.
We initialized an identical HRM architecture with random weights and, without any training, mea-
sured the PR of its modules as the network processed the same task-specific inputs given to the
trained model.
The results, shown in Figure 8(e,f), reveal a stark contrast: the high-level and low-level modules of
the untrained network exhibit no hierarchical separation, with their PR values remaining low and
nearly indistinguishable from each other. This control analysis validates that the dimensionality
hierarchy is an emergent property that arises as the model learns to perform complex reasoning.
The high-to-low PR ratio in HRM (zH /zL ≈ 2.98) closely matches that measured in the mouse
cortex (≈ 2.25). In contrast, conventional deep networks often exhibit neural collapse, where
last-layer features converge to a low-dimensional subspace 80,81,82 . HRM therefore departs from the
collapse pattern and instead fosters a high-dimensional representation in its higher module. This
is significant because such representations are considered crucial for cognitive flexibility and are a
hallmark of higher-order brain regions like the prefrontal cortex (PFC), which is central to complex
reasoning.
This structural parallel suggests the model has discovered a fundamental organizational principle.
By learning to partition its representations into a high-capacity, high-dimensional subspace (zH )
15and a more specialized, low-dimensional one (zL ), HRM autonomously discovers an organizational
principle that is thought to be fundamental for achieving robust and flexible reasoning in biological
systems. This provides a potential mechanistic explanation for the model’s success on complex,
long-horizon tasks that are intractable for models lacking such a differentiated internal structure.
We emphasize, however, that this evidence is correlational. While a causal link could be tested
via intervention (e.g., by constraining the H-module’s dimensionality), such methods are difficult
to interpret in deep learning due to potential confounding effects on the training process itself.
Thus, the causal necessity of this emergent hierarchy remains an important question for future
investigation.
5
Related Work
Reasoning and algorithm learning Given the central role of reasoning problems and their close
relation to algorithms, researchers have long explored neural architectures that enable algorithm
learning from training instances. This line of work includes Neural Turing Machines (NTM) 83 ,
the Differentiable Neural Computer (DNC) 84 , and Neural GPUs 85 –all of which construct iterative
neural architectures that mimic computational hardware for algorithm execution, and are trained to
learn algorithms from data. Another notable work in this area is Recurrent Relational Networks
(RRN) 62 , which executes algorithms on graph representations through graph neural networks.
Recent studies have integrated algorithm learning approaches with Transformer-based architec-
tures. Universal Transformers extend the standard Transformer model by introducing a recurrent
loop over the layers and implementing an adaptive halting mechanism. Geiping et al. 86 demonstrate
that looped Transformers can generalize to a larger number of recurrent steps during inference than
what they were trained on. Shen et al. 16 propose adding continuous recurrent reasoning tokens
to the Transformer. Finally, TransNAR 8 combine recurrent graph neural networks with language
models.
Building on the success of CoT-based reasoning, a line of work have introduced fine-tuning meth-
ods that use reasoning paths from search algorithms (like A*) as SFT targets 87,71,70 .
We also mention adaptive halting mechanisms designed to allocate additional computational re-
sources to more challenging problems. This includes the Adaptive Computation Time (ACT) for
RNNs 88 and follow-up research like PonderNet 89 , which aims to improve the stability of this allo-
cation process.
HRM further pushes the boundary of algorithm learning through a brain-inspired computational
architecture that achieves exceptional data efficiency and model expressiveness, successfully dis-
covering complex and diverse algorithms from just 1000 training examples.
Brain-inspired reasoning architectures Developing a model with the reasoning power of the
brain has long been a goal in brain-inspired computing. Spaun 90 is one notable example, which uses
spiking neural networks to create distinct modules corresponding to brain regions like the visual
cortex and prefrontal cortex. This design enables an architecture to perform a range of cognitive
tasks, from memory recall to simple reasoning puzzles. However, its reasoning relies on hand-
designed algorithms, which may limit its ability to learn new tasks. Another significant model is the
Tolman-Eichenbaum Machine (TEM) 91 , which is inspired by the hippocampal-entorhinal system’s
role in spatial and relational memory tasks. TEM proposes that medial entorhinal cells create a
basis for structural knowledge, while hippocampal cells link this basis to sensory information. This
16allows TEM to generalize and explains the emergence of various cell types like grid, border, and
place cells. Another approach involves neural sampling models 92 , which view the neural signaling
process as inference over a distribution, functioning similarly to a Boltzmann machine. These
models often require hand-made rules to be set up for solving a specific reasoning task. In essence,
while prior models are restricted to simple reasoning problems, HRM is designed to solve complex
tasks that are hard for even advanced LLMs, without pre-training or task-specific manual design.
Hierarchical memory The hierarchical multi-timescale structure also plays an important role in
how the brain processes memory. Models such as Hierarchical Sequential Models 93 and Clockwork
RNN 94 use multiple recurrent modules that operate at varying time scales to more effectively cap-
ture long-range dependencies within sequences, thereby mitigating the forgetting issue in RNNs.
Similar mechanisms have also been adopted in linear attention methods for memorizing long con-
texts (see the Discussions section). Since HRM focuses on reasoning, full attention is applied for
simplicity. Incorporating hierarchical memory into HRM could be a promising future direction.
6
Discussions
Turing-completeness of HRM Like earlier neural reasoning algorithms including the Universal
Transformer 95 , HRM is computationally universal when given sufficient memory and time con-
straints. In other words, it falls into the category of models that can simulate any Turing machine,
overcoming the computational limitations of standard Transformers discussed previously in the in-
troduction. Given that earlier neural algorithm reasoners were trained as recurrent neural networks,
they suffer from premature convergence and memory intensive BPTT. Therefore, in practice, their
effective computational depth remains limited, though still deeper than that of a standard Trans-
former. By resolving these two challenges and being equipped with adaptive computation, HRM
could be trained on long reasoning processes, solve complex puzzles requiring intensive depth-first
search and backtracking, and move closer to practical Turing-completeness.
Reinforcement learning with chain-of-thought Beyond fine-tuning using human-annotated CoT,
reinforcement learning (RL) represents another widely adopted training methodology. However,
recent evidence suggests that RL primarily unlocks existing CoT-like capabilities rather than dis-
covering fundamentally new reasoning mechanisms 96,97,98,99 . Additionally, CoT-training with RL
is known for its instability and data inefficiency, often requiring extensive exploration and careful
reward design. In contrast, HRM takes feedback from dense gradient-based supervision rather than
relying on a sparse reward signal. Moreover, HRM operates naturally in a continuous space, which
is biologically plausible and avoids allocating same computational resources to each token, even
though tokens vary in their reasoning and planning complexity 16 .
Linear attention Recurrence has been explored not only for its capability in universal computa-
tion, but also as a means to replace the attention mechanism in Transformers, which suffers from
quadratic time and memory complexity 100 . Recurrent alternatives offer a more efficient design by
processing input tokens sequentially and predicting the next token at each time step, similar to early
RNN-based language models.
Some linear-attention variants, such as Log-linear Attention 101 , share an RNN-like state-update that
can be interpreted as propagating multi-timescale summary statistics, thereby retaining long-range
context without the quadratic memory growth of standard self-attention. However, substituting the
attention mechanism alone does not change the fact that Transformers are still fixed-depth, and
17require CoT as a compensatory mechanism. Notably, linear attention can operate with a reduced
key-value cache over extended contexts, making them more suitable for deployment on resource-
constrained edge devices.
7
Conclusion
This work introduces the Hierarchical Reasoning Model, a brain-inspired architecture that lever-
ages hierarchical structure and multi-timescale processing to achieve substantial computational
depth without sacrificing training stability or efficiency. With only 27M parameters and train-
ing on just 1000 examples, HRM effectively solves challenging reasoning problems such as ARC,
Sudoku, and complex maze navigation–tasks that typically pose significant difficulties for contem-
porary LLM and chain-of-thought models.
Although the brain relies heavily on hierarchical structures to enable most cognitive processes,
these concepts have largely remained confined to academic literature rather than being translated
into practical applications. The prevailing AI approach continues to favor non-hierarchical models.
Our results challenge this established paradigm and suggest that the Hierarchical Reasoning Model
represents a viable alternative to the currently dominant chain-of-thought reasoning methods, ad-
vancing toward a foundational framework capable of Turing-complete universal computation.
Acknowledgements We thank Mingli Yuan, Ahmed Murtadha Hasan Mahyoub and Hengshuai
Yao for their insightful discussions and valuable feedback throughout the course of this work.
18References
1. Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016.
http://www.deeplearningbook.org.
2. Kaiming He, X. Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
pages 770–778, 2015.
3. Lena Strobl. Average-hard attention transformers are constant-depth uniform threshold
circuits, 2023.
4. Tom Bylander. Complexity results for planning. In Proceedings of the 12th International Joint
Conference on Artificial Intelligence - Volume 1, IJCAI’91, page 274–279, San Francisco,
CA, USA, 1991. Morgan Kaufmann Publishers Inc. ISBN 1558601600.
5. William Merrill and Ashish Sabharwal. A logic for expressing log-precision transformers. In
Neural Information Processing Systems, 2023.
6. David Chiang. Transformers in DLOGTIME-uniform TC0 . Transactions on Machine
Learning Research, 2025.
7. Lucas Lehnert, Sainbayar Sukhbaatar, DiJia Su, Qinqing Zheng, Paul McVay, Michael
Rabbat, and Yuandong Tian. Beyond a*: Better planning with transformers via search
dynamics bootstrapping. In First Conference on Language Modeling, 2024.
8. Wilfried Bounsi, Borja Ibarz, Andrew Dudzik, Jessica B. Hamrick, Larisa Markeeva, Alex
Vitvitskyi, Razvan Pascanu, and Petar Velivckovi’c. Transformers meet neural algorithmic
reasoners. ArXiv, abs/2406.09308, 2024.
9. William Merrill and Ashish Sabharwal. The parallelism tradeoff: Limitations of log-precision
transformers. Transactions of the Association for Computational Linguistics, 11:531–545,
2023. doi: 10.1162/tacl_a_00562.
10. Jason Wei, Yi Tay, et al. Chain-of-thought prompting elicits reasoning in large language
models, 2022. arXiv preprint arXiv:2201.11903.
11. William Merrill and Ashish Sabharwal. The expressive power of transformers with chain of
thought. In ICLR, 2024.
12. Xinyun Chen, Ryan A. Chi, Xuezhi Wang, and Denny Zhou. Premise order matters in
reasoning with large language models. ArXiv, abs/2402.08939, 2024.
13. Rongwu Xu, Zehan Qi, and Wei Xu. Preemptive answer "attacks" on chain-of-thought
reasoning. In Annual Meeting of the Association for Computational Linguistics, 2024.
14. Pablo Villalobos, Anson Ho, Jaime Sevilla, Tamay Besiroglu, Lennart Heim, and Marius
Hobbhahn. Will we run out of data? limits of llm scaling based on human-generated data.
arXiv preprint arXiv:2211.04325, 2022.
15. Xinghao Chen, Anhao Zhao, Heming Xia, Xuan Lu, Hanlin Wang, Yanjun Chen, Wei Zhang,
Jian Wang, Wenjie Li, and Xiaoyu Shen. Reasoning beyond language: A comprehensive
survey on latent chain-of-thought reasoning, 2025.
16. Xuan Shen, Yizhou Wang, Xiangxi Shi, Yanzhi Wang, Pu Zhao, and Jiuxiang Gu.
Training large language models to reason in a continuous latent space. arXiv preprint
arXiv:2412.07423, 2024.
1917. Evelina Fedorenko, Steven T Piantadosi, and Edward AF Gibson. Language is primarily a
tool for communication rather than thought. Nature, 630(8017):575–586, 2024.
18. Hongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Dongdong Zhang, and Furu Wei.
Deepnet: Scaling transformers to 1,000 layers. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 2024.
19. Timothy P Lillicrap and Adam Santoro. Backpropagation through time and the brain. Current
Opinion in Neurobiology, 55:82–89, 2019. ISSN 0959-4388. doi: https://doi.org/10.1016/j.
conb.2019.01.011.
20. John D Murray, Alberto Bernacchia, David J Freedman, Ranulfo Romo, Jonathan D Wallis,
Xinying Cai, Camillo Padoa-Schioppa, Tatiana Pasternak, Hyojung Seo, Daeyeol Lee, et al.
A hierarchy of intrinsic timescales across primate cortex. Nature neuroscience, 17(12):1661–
1663, 2014.
21. Roxana Zeraati, Yan-Liang Shi, Nicholas A Steinmetz, Marc A Gieselmann, Alexander
Thiele, Tirin Moore, Anna Levina, and Tatiana A Engel. Intrinsic timescales in the
visual cortex change with selective attention and reflect spatial connectivity. Nature
communications, 14(1):1858, 2023.
22. Julia M Huntenburg, Pierre-Louis Bazin, and Daniel S Margulies. Large-scale gradients in
human cortical organization. Trends in cognitive sciences, 22(1):21–31, 2018.
23. Victor AF Lamme and Pieter R Roelfsema. The distinct modes of vision offered by
feedforward and recurrent processing. Trends in neurosciences, 23(11):571–579, 2000.
24. Andre M Bastos, W Martin Usrey, Rick A Adams, George R Mangun, Pascal Fries, and Karl J
Friston. Canonical microcircuits for predictive coding. Neuron, 76(4):695–711, 2012.
25. Klara Kaleb, Barbara Feulner, Juan Gallego, and Claudia Clopath. Feedback control guides
credit assignment in recurrent neural networks. Advances in Neural Information Processing
Systems, 37:5122–5144, 2024.
26. Timothy P Lillicrap, Adam Santoro, Luke Marris, Colin J Akerman, and Geoffrey Hinton.
Backpropagation and the brain. Nature Reviews Neuroscience, 21(6):335–346, 2020.
27. François Chollet. On the measure of intelligence (abstraction and reasoning corpus), 2019.
arXiv preprint arXiv:1911.01547.
28. Francois Chollet, Mike Knoop, Gregory Kamradt, and Bryan Landers. Arc prize 2024:
Technical report. ArXiv, abs/2412.04604, 2024.
29. Francois Chollet, Mike Knoop, Gregory Kamradt, Bryan Landers, and Henry Pinkard. Arc-
agi-2: A new challenge for frontier ai reasoning systems. arXiv preprint arXiv:2505.11831,
2025.
30. György Buzsáki. Gamma, alpha, delta, and theta oscillations govern cognitive processes.
International Journal of Psychophysiology, 39:241–248, 2000.
31. György Buzsáki. Rhythms of the Brain. Oxford university press, 2006.
32. Anja Pahor and Norbert Jaušovec. Theta–gamma cross-frequency coupling relates to the level
of human intelligence. Intelligence, 46:283–290, 2014.
33. Adriano BL Tort, Robert W Komorowski, Joseph R Manns, Nancy J Kopell, and Howard
Eichenbaum.
Theta–gamma coupling increases during the learning of item–context
associations. Proceedings of the National Academy of Sciences, 106(49):20942–20947, 2009.
2034. Benjamin Scellier and Yoshua Bengio. Equilibrium propagation: Bridging the gap between
energy-based models and backpropagation. Frontiers in Computational Neuroscience, 11,
2016.
35. Guillaume Bellec, Franz Scherr, Anand Subramoney, Elias Hajek, Darjan Salaj, Robert
Legenstein, and Wolfgang Maass. A solution to the learning dilemma for recurrent
networks of spiking neurons. Nature Communications, 11, 07 2020. doi: 10.1038/
s41467-020-17236-y.
36. Shaojie Bai, J Zico Kolter, and Vladlen Koltun. Deep equilibrium models. In Advances in
Neural Information Processing Systems, pages 690–701, 2019.
37. Zhengyang Geng, Xinyu Zhang, Shaojie Bai, Yisen Wang, and Zhouchen Lin. On training
implicit models. ArXiv, abs/2111.05177, 2021.
38. Katarina Begus and Elizabeth Bonawitz. The rhythm of learning: Theta oscillations as an
index of active learning in infancy. Developmental Cognitive Neuroscience, 45:100810, 2020.
ISSN 1878-9293. doi: https://doi.org/10.1016/j.dcn.2020.100810.
39. Shaojie Bai, Zhengyang Geng, Yash Savani, and J. Zico Kolter.
Deep Equilibrium
Optical Flow Estimation . In 2022 IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR), pages 610–620, 2022.
40. Zaccharie Ramzi, Florian Mannel, Shaojie Bai, Jean-Luc Starck, Philippe Ciuciu, and
Thomas Moreau. Shine: Sharing the inverse estimate from the forward pass for bi-level
optimization and implicit models. ArXiv, abs/2106.00553, 2021.
41. Shaojie Bai, Vladlen Koltun, and J. Zico Kolter. Stabilizing equilibrium models by jacobian
regularization. In International Conference on Machine Learning, 2021.
42. Daniel Kahneman and P Egan. Thinking, fast and slow (farrar, straus and giroux, new york),
2011.
43. Matthew D Lieberman. Social cognitive neuroscience: a review of core processes. Annu. Rev.
Psychol., 58(1):259–289, 2007.
44. Randy L Buckner, Jessica R Andrews-Hanna, and Daniel L Schacter. The brain’s default
network: anatomy, function, and relevance to disease. Annals of the new York Academy of
Sciences, 1124(1):1–38, 2008.
45. Marcus E Raichle. The brain’s default mode network. Annual review of neuroscience, 38(1):
433–447, 2015.
46. Andrew Westbrook and Todd S Braver. Cognitive effort: A neuroeconomic approach.
Cognitive, Affective, & Behavioral Neuroscience, 15:395–415, 2015.
47. Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. MIT
Press, Cambridge, MA, 2018.
48. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan
Wierstra, and Martin A. Riedmiller. Playing atari with deep reinforcement learning. ArXiv,
abs/1312.5602, 2013.
49. Matteo Gallici, Mattie Fellows, Benjamin Ellis, Bartomeu Pou, Ivan Masmitja,
Jakob Nicolaus Foerster, and Mario Martin. Simplifying deep temporal difference learning,
2025.
2150. Shuo Xie and Zhiyuan Li. Implicit bias of adamw: L inf norm constrained optimization.
ArXiv, abs/2404.04454, 2024.
51. Lucas Prieto, Melih Barsbey, Pedro A. M. Mediano, and Tolga Birdal. Grokking at the edge of
numerical stability. In The Thirteenth International Conference on Learning Representations,
2025.
52. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural
information processing systems, pages 5998–6008, 2017.
53. Meta AI. Llama 3: State-of-the-art open weight language models. Technical report, Meta,
2024. URL https://ai.meta.com/llama/.
54. Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer:
Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024.
55. Noam M. Shazeer. Glu variants improve transformer. ArXiv, abs/2002.05202, 2020.
56. Biao Zhang and Rico Sennrich.
Root mean square layer normalization.
ArXiv,
abs/1910.07467, 2019.
57. Günter Klambauer, Thomas Unterthiner, Andreas Mayr, and Sepp Hochreiter. Self-
normalizing neural networks. In Neural Information Processing Systems, 2017.
58. JAX Developers. jax.nn.initializers.lecun_normal. Google Research, 2025.
URL
https://docs.jax.dev/en/latest/_autosummary/jax.nn.initializers.lecun_
normal.html. Accessed June 22, 2025.
59. Yann LeCun, Léon Bottou, Genevieve B Orr, and Klaus-Robert Müller. Efficient backprop.
In Neural networks: Tricks of the trade, pages 9–50. Springer, 2002.
60. Katie E Everett, Lechao Xiao, Mitchell Wortsman, Alexander A Alemi, Roman Novak,
Peter J Liu, Izzeddin Gur, Jascha Sohl-Dickstein, Leslie Pack Kaelbling, Jaehoon Lee, and
Jeffrey Pennington. Scaling exponents across parameterizations and optimizers. In Forty-first
International Conference on Machine Learning, 2024.
61. Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization, 2017.
62. Rasmus Berg Palm, Ulrich Paquet, and Ole Winther. Recurrent relational networks. In Neural
Information Processing Systems, 2017.
63. Jieyi Long. Large language model guided tree-of-thought. ArXiv, abs/2305.08291, 2023.
64. Yilun Du, Jiayuan Mao, and Josh Tenenbaum. Learning iterative reasoning through energy
diffusion. ArXiv, abs/2406.11179, 2024.
65. Kyubyong Park. Can convolutional neural networks crack sudoku puzzles?
https:
//github.com/Kyubyong/sudoku, 2018.
66. Single-digit techniques. https://hodoku.sourceforge.net/en/tech_singles.php.
Accessed: 2025-06-16.
67. Tom Dillion. Tdoku: A fast sudoku solver and generator. https://t-dillon.github.io/
tdoku/, 2025.
68. Jeffrey Seely, Yuki Imajuku, Tianyu Zhao, Edoardo Cetin, and Llion Jones. Sudoku-bench:
Evaluating creative reasoning with sudoku variants. arXiv preprint arXiv:2505.16135, 2025.
69. Luke Darlow, Ciaran Regan, Sebastian Risi, Jeffrey Seely, and Llion Jones. Continuous
thought machines. arXiv preprint arXiv:2505.05522, 2025.
2270. DiJia Su, Sainbayar Sukhbaatar, Michael Rabbat, Yuandong Tian, and Qinqing Zheng.
Dualformer: Controllable fast and slow thinking by learning with randomized reasoning
traces, 2025.
71. Lucas Lehnert, Sainbayar Sukhbaatar, DiJia Su, Qinqing Zheng, Paul McVay, Michael
Rabbat, and Yuandong Tian. Beyond a*: Better planning with transformers via search
dynamics bootstrapping. In First Conference on Language Modeling, 2024.
72. Mubbasir Kapadia, Francisco Garcia, Cory D. Boatright, and Norman I. Badler. Dynamic
search on the gpu. In 2013 IEEE/RSJ International Conference on Intelligent Robots and
Systems, pages 3332–3337, 2013. doi: 10.1109/IROS.2013.6696830.
73. Isaac Liao and Albert Gu.
Arc-agi without pretraining, 2025.
URL https:
//iliao2345.github.io/blog_posts/arc_agi_without_pretraining/arc_agi_
without_pretraining.html.
74. Lorenzo Posani, Shuqi Wang, Samuel P Muscinelli, Liam Paninski, and Stefano Fusi.
Rarely categorical, always high-dimensional: how the neural code changes along the cortical
hierarchy. bioRxiv, pages 2024–11, 2025.
75. Mattia Rigotti, Omri Barak, Melissa R. Warden, Xiao-Jing Wang, Nathaniel D. Daw, Earl K.
Miller, and Stefano Fusi. The importance of mixed selectivity in complex cognitive tasks.
Nature, 497:585–590, 2013. doi: 10.1038/nature12160.
76. Valerio Mante, David Sussillo, Krishna V. Shenoy, and William T. Newsome. Context-
dependent computation by recurrent dynamics in prefrontal cortex. Nature, 503(7474):78–84,
2013. doi: 10.1038/nature12742.
77. Earl K. Miller and Jonathan D. Cohen. An integrative theory of prefrontal cortex function.
Annual Review of Neuroscience, 24(1):167–202, 2001. doi: 10.1146/annurev.neuro.24.1.167.
78. Wolfgang Maass. Real-time computing without stable states: a new framework for neural
computation based on perturbations. Neural Computation, 14(11):2531–2560, 2002. doi:
10.1162/089976602760407955.
79. Ege Altan, Sara A. Solla, Lee E. Miller, and Eric J. Perreault. Estimating the dimensionality
of the manifold underlying multi-electrode neural recordings. PLoS Computational Biology,
17(11):e1008591, 2021. doi: 10.1371/journal.pcbi.1008591.
80. Vardan Papyan, X. Y. Han, and David L. Donoho. Prevalence of neural collapse during the
terminal phase of deep learning training. Proceedings of the National Academy of Sciences,
117(40):24652–24663, 2020. doi: 10.1073/pnas.2015509117.
81. Cong Fang, Hangfeng He, Qi Long, and Weijie J. Su. Exploring deep neural networks via
layer–peeled model: Minority collapse in imbalanced training. Proceedings of the National
Academy of Sciences, 118(43):e2103091118, 2021. doi: 10.1073/pnas.2103091118.
82. Zhihui Zhu, Tianyu Ding, Jinxin Zhou, Xiao Li, Chong You, Jeremias Sulam, and Qing Qu.
A geometric analysis of neural collapse with unconstrained features. In Advances in Neural
Information Processing Systems, volume 34 of NeurIPS, pages 29820–29834, 2021.
83. Alex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines, 2014.
84. Alex Graves, Greg Wayne, Malcolm Reynolds, Tim Harley, Ivo Danihelka, Agnieszka
Grabska-Barwińska, Sergio Gómez Colmenarejo, Edward Grefenstette, Tiago Ramalho, John
Agapiou, et al. Hybrid computing using a neural network with dynamic external memory.
Nature, 538(7626):471–476, 2016.
2385. Lukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In ICLR, 2016.
86. Jonas Geiping, Sean McLeish, Neel Jain, John Kirchenbauer, Siddharth Singh, Brian R.
Bartoldson, Bhavya Kailkhura, Abhinav Bhatele, and Tom Goldstein. Scaling up test-time
compute with latent reasoning: A recurrent depth approach, 2025.
87. Tiedong Liu and Kian Hsiang Low. Goat: Fine-tuned llama outperforms gpt-4 on arithmetic
tasks. ArXiv, abs/2305.14201, 2023.
88. Alex Graves.
Adaptive computation time for recurrent neural networks.
ArXiv,
abs/1603.08983, 2016.
89. Andrea Banino, Jan Balaguer, and Charles Blundell. Pondernet: Learning to ponder. ArXiv,
abs/2107.05407, 2021.
90. Chris Eliasmith, Terrence C Stewart, Xuan Choo, Trevor Bekolay, Travis DeWolf, Yichuan
Tang, and Daniel Rasmussen. A large-scale model of the functioning brain. science, 338
(6111):1202–1205, 2012.
91. James CR Whittington, Timothy H Muller, Shirley Mark, Guifen Chen, Caswell Barry, Neil
Burgess, and Timothy EJ Behrens. The tolman-eichenbaum machine: unifying space and
relational memory through generalization in the hippocampal formation. Cell, 183(5):1249–
1263, 2020.
92. Lars Buesing, Johannes Bill, Bernhard Nessler, and Wolfgang Maass. Neural dynamics as
sampling: a model for stochastic computation in recurrent networks of spiking neurons. PLoS
computational biology, 7(11):e1002211, 2011.
93. Salah Hihi and Yoshua Bengio. Hierarchical recurrent neural networks for long-term
dependencies. In D. Touretzky, M.C. Mozer, and M. Hasselmo, editors, Advances in Neural
Information Processing Systems, volume 8. MIT Press, 1995.
94. Jan Koutník, Klaus Greff, Faustino J. Gomez, and Jürgen Schmidhuber. A clockwork rnn. In
International Conference on Machine Learning, 2014.
95. Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Lukasz Kaiser.
Universal transformers, 2018. arXiv preprint arXiv:1807.03819.
96. Yiping Wang, Qing Yang, Zhiyuan Zeng, Liliang Ren, Lucas Liu, Baolin Peng, Hao Cheng,
Xuehai He, Kuan Wang, Jianfeng Gao, Weizhu Chen, Shuohang Wang, Simon Shaolei Du,
and Yelong Shen. Reinforcement learning for reasoning in large language models with one
training example, 2025. URL https://arxiv.org/abs/2504.20571.
97. Niklas Muennighoff. s1: Simple test-time scaling. arXiv preprint arXiv:2502.23456, 2025.
98. Liang Wen, Yunke Cai, Fenrui Xiao, Xin He, Qi An, Zhenyu Duan, Yimin Du, Junchen Liu,
Lifu Tang, Xiaowei Lv, Haosheng Zou, Yongchao Deng, Shousheng Jia, and Xiangzheng
Zhang. Light-r1: Curriculum sft, dpo and rl for long cot from scratch and beyond, 2025.
99. Xuefeng Li, Haoyang Zou, and Pengfei Liu. Limr: Less is more for rl scaling, 2025.
100. Tri Dao and Albert Gu. Transformers are ssms: Generalized models and efficient algorithms
through structured state space duality. ArXiv, abs/2405.21060, 2024.
101. Han Guo, Songlin Yang, Tarushii Goel, Eric P Xing, Tri Dao, and Yoon Kim. Log-linear
attention. arXiv preprint arXiv:2506.04761, 2025.
