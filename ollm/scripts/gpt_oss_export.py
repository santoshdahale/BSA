# -*- coding: utf-8 -*-
"""gpt-oss.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ky7OlvW63v0pT84lrAB6f3l4oh3nKvTo
"""

# gps-oss-20B. Export gds_export weights
import json, os
import torch
from transformers import AutoModelForCausalLM, AutoConfig
from safetensors.torch import load_file as load_safetensors
from safetensors.torch import safe_open, save_file
from transformers.utils.quantization_config import Mxfp4Config

# 1. Get original tensors(_blocks, _scales) in torch.uint8, torch.bfloat16.
do = {}
for filename in ["model-00000-of-00002", "model-00001-of-00002", "model-00002-of-00002"]:
    MODEL_DIR = f"/content/drive/MyDrive/temp/gpt-oss-20b/{filename}.safetensors"
    with safe_open(MODEL_DIR, framework="pt") as fin:
        for key in fin.keys():
            t = fin.get_tensor(key)
            do[key] = t

#2. Export
MODEL_ID = "/content/drive/MyDrive/temp/gpt-oss-20b/"
OUT_DIR = "/content/drive/MyDrive/temp/gds_export"
os.makedirs(OUT_DIR, exist_ok=True)
quantization_config = Mxfp4Config(dequantize=False)
state_dict = AutoModelForCausalLM.from_pretrained(
    MODEL_ID,
    quantization_config=quantization_config,
    device_map="cpu",
    torch_dtype="auto",
    low_cpu_mem_usage=True
).state_dict() #converts to bfloat16

manifest = {}
for name, tensor in state_dict.items():
    if not name.startswith(("model.layers", "transformer.h")): continue
    t, packed = tensor.to("cpu").contiguous(), None
    dtype, shape = t.dtype, t.shape

    blocks, scales = do.get(name+"_blocks"), do.get(name+"_scales")
    if blocks is not None and scales is not None:
        t = {"_blocks":blocks, "_scales":scales}
        packed = "mxfp4"

    filename = f"{name.replace('.', '__')}.pt"
    path = os.path.join(OUT_DIR, filename)
    os.makedirs(os.path.dirname(path), exist_ok=True)
    torch.save(t, path)
    manifest[name] = {
        "path": filename,
        "dtype": str(dtype), #torch.bfloat16
        "shape": list(shape),
        "packed": packed
    }

with open(os.path.join(OUT_DIR, "manifest.json"), "w") as f: json.dump(manifest, f, indent=2)
print(f"Exported {len(manifest)} tensors to {OUT_DIR}")
# ./endOf Export gds_export weights

"""[link text](https://)"""

FP4_VALUES = [
    +0.0,
    +0.5,
    +1.0,
    +1.5,
    +2.0,
    +3.0,
    +4.0,
    +6.0,
    -0.0,
    -0.5,
    -1.0,
    -1.5,
    -2.0,
    -3.0,
    -4.0,
    -6.0,
]

def convert_moe_packed_tensors( #copied from transformers/integrations/mxfp4.py
    blocks,
    scales,
    *,
    dtype: torch.dtype = torch.bfloat16,
    rows_per_chunk: int = 32768 * 1024,  # TODO these values are not here by mistake ;)
) -> torch.Tensor:
    """
    Convert the mxfp4 weights again, dequantizing and makes them compatible with the forward
    pass of GPT_OSS.
    """
    import math

    # Check if blocks and scales are on CPU, and move to GPU if so
    #if not blocks.is_cuda and torch.cuda.is_available():
    #    blocks = blocks.cuda()
    #    scales = scales.cuda()

    scales = scales.to(torch.int32) - 127  # TODO that's because 128=2**7

    assert blocks.shape[:-1] == scales.shape, f"{blocks.shape[:-1]=} does not match {scales.shape=}"

    lut = torch.tensor(FP4_VALUES, dtype=dtype, device=blocks.device)

    *prefix_shape, G, B = blocks.shape
    rows_total = math.prod(prefix_shape) * G

    blocks = blocks.reshape(rows_total, B)
    scales = scales.reshape(rows_total, 1)

    out = torch.empty(rows_total, B * 2, dtype=dtype, device=blocks.device)

    for r0 in range(0, rows_total, rows_per_chunk):
        r1 = min(r0 + rows_per_chunk, rows_total)

        blk = blocks[r0:r1]
        exp = scales[r0:r1]

        # nibble indices -> int64
        idx_lo = (blk & 0x0F).to(torch.long)
        idx_hi = (blk >> 4).to(torch.long)

        sub = out[r0:r1]
        sub[:, 0::2] = lut[idx_lo]
        sub[:, 1::2] = lut[idx_hi]

        torch.ldexp(sub, exp, out=sub)
        del idx_lo, idx_hi, blk, exp, sub

    out = out.reshape(*prefix_shape, G, B * 2).view(*prefix_shape, G * B * 2)
    del blocks, scales, lut
    return out.transpose(1, 2).contiguous()

#==========================================
# Check if unpacks correctly
d2 = {}
for filename in ["model-00000-of-00002", "model-00001-of-00002", "model-00002-of-00002"]:
    MODEL_DIR = f"/content/drive/MyDrive/temp/gpt-oss-20b/{filename}.safetensors"
    with safe_open(MODEL_DIR, framework="pt") as fin:
        for key in fin.keys():
            t = fin.get_tensor(key)
            d2[key] = t

for name, o in d.items():
    t1, dtype, shape = o
    if name+"_blocks" in d2 and name+"_scales" in d2:
        t2 = convert_moe_packed_tensors(d2[name+"_blocks"], d2[name+"_scales"])
        print("unpacked:", name, dtype, shape, t1.flatten()[:5], "-- t2:", t2.shape, t2.flatten()[:5])
    else:
        t2 = d2[name]
        print("regular:", name, dtype, shape, t1.flatten()[:5], "-- t2:", t2.shape, t2.flatten()[:5])

# Commented out IPython magic to ensure Python compatibility.
del state_dict
# %reset -f